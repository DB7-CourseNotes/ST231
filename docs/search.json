[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Life Sciences",
    "section": "",
    "text": "Introduction\nThese course notes are self-contained and provided as an Open Educational Resources. In doing so, I will also borrow from OpenIntro Statistics for the Biomedical and Life Sciences, with attribution. As such, these resources have the same license (see the end of this page), except for those elements that are still present from Baldi & Moore (mainly exercises, which are being removed as I go)."
  },
  {
    "objectID": "index.html#alternate-textbooks-for-extra-instruction",
    "href": "index.html#alternate-textbooks-for-extra-instruction",
    "title": "Statistics for Life Sciences",
    "section": "Alternate Textbooks for Extra Instruction",
    "text": "Alternate Textbooks for Extra Instruction\n\nThese course notes structured to coincide with Baldi and Moore’s The Basic Practice of Statistics in the Life Sciences, 4th edition. However, I have made them self-contained in order to provide these notes as an Open Educational Resources."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Statistics for Life Sciences",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCritically appraise published articles in health sciences research.\nUse industry standard tools to apply basic statistical concepts to real-world problems.\nUnderstand the use and application of statistical techniques such as descriptive and inferential statistics."
  },
  {
    "objectID": "index.html#accessing-materials",
    "href": "index.html#accessing-materials",
    "title": "Statistics for Life Sciences",
    "section": "Accessing Materials",
    "text": "Accessing Materials\n\nLectures posted on MyLS\nrdrr.io allows running single R commands to caclulate probabilities\nRStudio\n\nFree, open-source interface to the R programming language.\nA free online cloud version is available - no need to install R on your own computer!\n\nSyzygy Jupyter Notebooks\n\nFree, web-based service for WLU students\nNo need to install R on your own computer!\n\n\nYou can use either RStudio or Syzygy for this course, RStudio has many fantastic bells and whistles that help you produce results and reports, whereas Syzygy has an online interface and makes it easy to use without installing R on your own computer. Note that Syzygy uses “notebooks” rather than the RMarkdown notebooks that RStudio prefers. L1abs will use RStudio.\nFor lectures, I will be using VSCode, which I use because it works well with python and R as well as other languages that I need. I will switch to RStudio for many demonstrations just to show you how it works because this is the program that most people who do statistics will use. I will occasionally demonstrate some concepts using Jupyter notebooks because this is another common way that people do statistics and data science. You will not be tested on the features of Rtudio, VSCode or Jupyter notebooks, but mastery of RStudio will be extremely helpful for all future data analysis tasks beyond this course.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#introduction",
    "href": "L01-Intro_PicturingGraphs.html#introduction",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nDefining “Statistics”\nMy definition: Statistics is the study of variance (or uncertainty).\n\nThe big question: is 1 statistically different from 100?\n\n1 vs. 100 apples? Yes.\n1 vs. 100 atoms in an apple? No.\n\n\n\nMany people think of “statistics” as something along the lines of “methods for dealing with data”. This completely misses out on theoretical statistics, and makes it seem like statistics is a collection of recipes stating “if you have this data, use this method”. I think a better definition of statistics is that it’s the study of variance, whether that means studying the theoretical properties of variance or trying to “explain” variance1 in a particular data set.\nI like to ask the question: “is 1 statistically different from 100”. It may seem like they are obviously different numbers, but we can’t know that without the context. If you’re comparing numbers of apples, then yes, one apple is very different from 100 apples. However, if we’re looking at numbers of atoms per apple, then one and 100 are both imperceptible numbers of atoms and thus we might say the the two apples are basically the same size. The difference in these two examples is the scale, and the variance is a fantastic way to measure the scales of things. In my opinion, the main thing we will learn in this course is how to tell whether two numbers are different given the scale of those two numbers. Another popular definition for statistics is “putting numbers in context”, and by “in context” they mean “relative to their variance”.\nIn my lecture notes, I use bold font for anything that you will be expected to be able to explain or define. You won’t necessarily see a full definition the first time you see a word in bold, but by the midterm/final it is something I expect you to know. A good way to study in this course is to keep a glossary of all of the words I’ve put in bold, with a definition file/note that you update as we learn more about that concept. And, of course, write a description as if you’re teaching someone else!\n\n\n\nWhy study variance?\n\nGive context to different numbers.\n\nThe size of the difference depends on the context.\n\nWe need to know how and why we were wrong.\n\nHow: What is the magnitude/direction of the difference?\nWhy: Are we missing relationships? Bad sampling? Fundamental randomness?\n\n\nVariance is information!\n\nVariance comes from many sources. We might just be doing something wrong and missing out on important feature of our data, we might be collecting the data in a biased or incorrect way, or there might be some fundamental part of the problem that we will never be able to measure perfectly, and so the variance that we calculated may actually be the smallest possible variance for this problem.\nSuppose we’re trying to figure out the heights of undergraduate students. We can calculate the average height, but I don’t expect anyone in the data to actually have this exact height. Exactly how far away from this average do we expect the other heights to be? The measurement of the expected distance to the mean is called the variance.The variance is a measure of the “usual” distance to the mean - we’ll have a formula for this later.\nIf our sample consisted of people walking out of the gym, and out of 50 people sampled we had 20 people who were on the basketball team, we might know a little bit more about our variance. Specifically, we’ll get some students who may or may not be representative of the population as well as some students who are unusually tall. These two groups are fundamentally different, so including both in the same sample will increase our variance. Put another way, the different groups in our data explain some of the variance in our data.Different subgroups within a sample can help explain why the variance is as large as it is.\nThis is why I say that the variance is information! More specifically, the difference between a single value and the mean of the data is due to some unexplained phenomena, and learning more about the problem will reduce the variance.\n\n\n\nDescriptive Versus Inferential Statistics\n\nDescriptive statistics are used to explore the data.\n\nGraphs/figures\nNumbers\n\nInferential statistics relate our data to the population.\n\nMust have a good sample first!\nOur sample has a mean. The population has a mean. How different do we expect them to be?\n\nhow different\n\n\n\n\nIn this course, we will learn about two classes of statistics. Descriptive statistics are the ones that we used to describe the sample that we obtained. This can include things like the mean/median/mode, the variance or the interquartile range, as well as bar charts, histograms box, plots, etc. Descriptive Statistics describe the data that were acquired.\nInferential statistics are numbers that we calculate because we think they have a relationship to the population. For instance, if we calculate the mean of our data and we trust that our sample is good, then we expect this sample mean to be somewhat close to the population mean. Any time in this course I talk about the difference between two things, I will always mean “with reference to a measure of variance”. In this example, we have a sample mean as well as some measure of its variance, and this variance tells us how similar we expect the sample mean to be to the population mean. If we have a small sample variance, it means that we have a lot of information about the population mean. Variance contains information that we haven’t learned yet!2Inferential Statistics attempt to describe the population based on the data acquired. This requires a good sample, and it’s why Statistics is such a useful field of study.\nIn this course, we’re going to start by talking about descriptive statistics and work our way to inferential statistics (with a detour through probability to show why inferential statistics work so well)."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "href": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.2 Descriptive Statistics: Plots and Graphs",
    "text": "1.2 Descriptive Statistics: Plots and Graphs\n\nThe Palmer Penguins Data\nLet me introduce you to a dataset that I’m going to be returning to throughout the semester. This is called the Palmer penguins data, end it contains information on penguins from several islands in Antarctica. In the slides, I use some fancier code to only show some of the data, but I’ll display the full data set with simpler code below:\n\nlibrary(palmerpenguins)\nhead(as.data.frame(penguins))\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\n\nSpecies can be Adelie, Gentoo, or Chinstrap\nSex can be male or female3\nBill Length is measured in millimetres\nBody Mass is measured in grams\nIsland (one of three)\nBill depth (mm)\nFlipper length (mm)\nYear that the penguin was observed.\n\nThese data are really nice for teaching statistics because we can look at what factors contribute to body mass (since it’s easier to measure the length of a beak than it is to weigh a live penguin), or we can try to determine biosex using these measurements (since penguins normally have no external genitalia, but have other morphological differences). We can also fit a model to determine the species of each penguin so that we can get a full picture of what makes each species unique! All of these are examples of statistical analyses that we will cover in this course.\n\n\nTypes of Variables\n\npenguins[, c(\"species\", \"sex\", \"body_mass_g\")] |&gt;\n    head() |&gt;\n    knitr::kable()\n\n\n\n\nspecies\nsex\nbody_mass_g\n\n\n\n\nAdelie\nmale\n3750\n\n\nAdelie\nfemale\n3800\n\n\nAdelie\nfemale\n3250\n\n\nAdelie\nNA\nNA\n\n\nAdelie\nfemale\n3450\n\n\nAdelie\nmale\n3650\n\n\n\n\n\n\nSpecies is categorical\n\nMutually exclusive categories.\n\nSex is binary4\n\nOne or the other\nSpecial case of categorical with 2 possibilities\n\nBody mass is quantitative\n\nIt’s a number (quantity)\n\n\n\nThe type of variable is extremely important for choosing the right summary of the data.\nCategorical variables consist of two or more mutually exclusive categories, that is, each observation has a label and nothing has more than one label. Categorical variables may be ordered (such as “low”, “medium”, and “high”) or unordered (such as names or student numbers; putting names in alphabetical order is not usually meaningful for summarising the data or doing the analysis).Categorical variables are categories. Be careful - some categories are encoded as numbers, such as “group 1” and “group 2”, etc.\nThis distinction of “ordered” versus “unordered” matters to determine what visualization or model you might want to use to compare data within categories. For instance, we might want to see the change in response to a “low” treatment compared to a “medium” or “high” treatment, where we fully expect the response to the treatment to be lower for “low” treatment and higher for “high”. In contrast, suppose we knew the patients’ occupations. We have no expectation that the treatment reponse in the “electrician” group is lower or higher than any other group, because there’s no logical ordering to occupation.5Ordered and unordered categorical variables.\nBinary variables are a special case of categorical variables, which only have two categories. In this case, the ordering is rarely important, and thus we don’t really make the distinction between ordered binary and unordered binary. It’s a single difference either way, so we can just look at the differences within the categories.6Binary variables.\nQuantitative variables are those that are measured with numbers. Unlike “low” to “medium” versus “medium” to “high”, we know how big a step it is from 0 to 1 and then 1 to 2. Unlike categorical variables, quantitative variables have a concept of “in-between”; we have nothing between “medium” and “high”, but there are infinite numbers between 1 and 2.Quantitative variables are numbers.\nThere are, however, cases where there aren’t possible measurements between 1 and 2. These are called “discrete”7 For instance, the number of children that some has is either 0, 1, 2, etc. Discrete variables are somewhere in between categorical and continuous variables (variables that can take be number). Consider the following two examples of discrete variables: the number of children that someone has, and the number of cents in their bank account. In a study, we might want to look at everyone who has no children, everyone who has 1 child, everyone who has 2, etc. However, we would not want to compare everyon who has 0 cents in their bank account, everyone who has 1 cent in their account, 2 cents, 3, cents, etc. In terms of modelling and visualization, we will sometimes treat discrete variables as categorical and sometimes as continuous.8 Discrete variables only allow for some numbers, such as the numbers of children per family.Continuous variables can take any value in a range. For any two possible numbers, there’s another possible number in between them. Alternatively, they can be defined as “any measurement you make could have been more precise.”\nThe distinction between categorical and quantitative isn’t always this obvious, but can be very helpful for choosing the right kind of plot or numerical summary.\n\n\n\nGrey Areas\n\n\n\n\n\n\nStudent Numbers\n\n\n\nA student number looks like a quantitative variable, but it’s actually just a name (category)!\n\n\n\nThere are a couple gray areas when talking about variable types. The distinction between categorical and quantitative isn’t always perfectly clear. For example, student numbers are names, but they are completely made up of numbers. However, you wouldn’t treat these numbers as if one student number comes after another in the same way that you wouldn’t put students in an order based on their names. You technically can do this if you do it alphabetically or order the student numbers in order, but this isn’t a meaningful ordering. It’s not like one student number is larger than another student number, and taking the meaning of student numbers wouldn’t make any sense.\n\n\n\nIndividuals (Subjects)\nAn individual is the thing we are making measurements on. For example, in the Palmer Penguins data set, a penguin is an individual and we measure their flipper length, bill length, the island they were observed on, etc. In other data sets, we may measure the GDP of countries (countries are the individuals) or the cuteness of dogs.Individual: the unit of study. We make measurements on individuals.\n\n\nChallenge: What are the individuals?\nThe following data are from the “Titanic” dataset.\nEach number defines the count of children who survived the Titanic’s sinking.\n\n\n\n\nMale\nFemale\n\n\n\n\n1st class\n5\n1\n\n\n2nd class\n11\n13\n\n\n3rd class\n13\n14\n\n\nCrew\n0\n0\n\n\n\n\nThere are a couple of ways to interpret this.\n\nPeople: We measured whether an individual survived (one measurement per person).\nClass: We have two measurements per class - one for male children, one for female children.\nSex: We have four observations (class 1, 2, 3, or crew).\nFinally, we could say we have one observation per class/sex combination.\n\nThere isn’t one “correct” definition of individual here. However, the way we define individual affects how we might visualize these data!\n\n\n\n\n\n\n\nCO\\(_2\\) Measurements\n\n\n\nIf we measure the C0\\(_2\\) at a given location each month, then do the months count as the individuals? This describes time series data, which we won’t cover in this course. However, visualizations of data that are measured over time can reveal a lot of information!9\n\n\n\n\n\n\n\n\nPaired Observations\n\n\n\n\nSpousal pairs - the pairs are the individuals?\nBefore/after (e.g., weight loss) - people are individuals.\n\nThis is the same idea as spousal pairs!\n\n\n\n\n\nAll of the variables we just talked about are measured on individuals. That is to say, an individual is what you are measuring when collecting data. This can take the form of a single penguin, in which case it’s obvious that this body mass belongs to this penguin; we have measured this body mass on this.\nThere are some gray areas to this as well, though. For example, if we’re measuring carbon dioxide every month, then we’re actually measuring carbon dioxide as the variable and months as the individuals, and we can measure other things on those individuals if needed.\nWe will also encounter paired observations in this course, which are measurements on two things at once. For example, we might be looking at spousal pairs, perhaps measuring the difference in height per pair. The variable we are measuring is the difference, and so we only have one observation per individual, which means that the individuals must be the pairs of people. A slightly more obvious example is something like a weight loss study were observing a change in weight for a certain person, even though we have two observations the individual is still the person who we’re measuring.Paired Observations: a special type of data where the difference (subtraction) of two values is what we are studying.\n\n\n\nExample: What are the Individuals? What are Variables?\n\nmtcars[1:6, c(\"wt\", \"mpg\", \"cyl\", \"am\")]\n\n                     wt  mpg cyl am\nMazda RX4         2.620 21.0   6  1\nMazda RX4 Wag     2.875 21.0   6  1\nDatsun 710        2.320 22.8   4  1\nHornet 4 Drive    3.215 21.4   6  0\nHornet Sportabout 3.440 18.7   8  0\nValiant           3.460 18.1   6  0\n\n\n\n\nIndividuals: Each car (not brand)\nVariables: wt, mpg, cyl, am, etc.\n\nwt: Quantitative; the weight of the car.\nam: Binary (categorical); whether the car is automatic or manual.\ncyl: It’s the number of cylinders, so it can be considered a number. However, there are only three possible values: 4, 6, or 8. We could consider these numbers, but it is likely more useful to think of 4 cylinder cars in one category, 6 cylinder in another category, and 8 cylinder in the last category. This way, when we do analysis, we are just comparing categories rather than see what happens when we add cylinders (“adding cylinders” makes it sound like we might add 1 cylinder to a 4 cylinder car, or like we might make predictions about what would happen with a 2 cylinder car)."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "href": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.3 Graphs/Charts/Plots",
    "text": "1.3 Graphs/Charts/Plots\n\nPie Charts\n\nThe wedges must sum to 1.\n\nIf “Adelie” makes up 44% of the data, it should be 44% of the pie chart.\nAll penguins are either Adelie, Gentoo, or Chinstrap; no penguins have more than one species.\n\nMainly good for emphasizing one wedge\n\nEmphasizing can easily mean misrepresenting, whether accidentally or on purpose!\n\n\nhttps://www.darkhorseanalytics.com/blog/salvaging-the-pie\n\nIn this course, I will not be providing you the code required to make a pie chart. However, you should understand what a pie chart is, what data it works for (categories), and how they relate to bar charts.\n\n\n\nBar Charts: Categories\nBar charts are similar to pie charts, but better in practically every way.\nEach bar represents a category, and the height represents the number of observations in that category.\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(mtcars) +\n    aes(x = factor(am)) +\n    geom_bar() +\n    labs(x = \"Transmission Type\",\n        y = \"Count\")\n\n\n\n\n\nThe code required to create the bar chart can be shown by clicking the “&gt;Code” icon. An explanation of the code is below (but this will not be tested).\n\n\nCode Explanation\n\nThe data set is built into our so we don’t need to load anything to put this data. We do need to load in the ggplot library, though. In my lecture notes, you’ll see lots of code that looks like this, but you will not be tested on your ability to re-create this code. For those interested, here’s a quick breakdown of the functions I used:\n\nThe ggplot() function tells R what data we will be using.\nThe aes() function sets up the plot “aesthetics”, such as what variable goes on the x-axis, what variable goes on the y-axis, what variable is assigned to a colour, what variable determines the shapes of points, etc.\ngeom_bar() actually draws the bar plot using the data set that ggplot() set up and the aesthetics, that aes() set up.\n\nTry running the code without this line and see what happens!10\n\nThe labs() function simply adds labels to the plot to make it look nicer.\n\n\n\nCompare this to the following table:\n\ntable(mtcars$am)\n\n\n 0  1 \n19 13 \n\n\nBar charts are primarily used to compare categories. The most common use of a bar chart is to count the number of observations in each category, and create a bar with a corresponding height. In this example, we see, automatic and manual transmissions, with automatic labelled as zero and manual labelled as one. We can see that approximately 19 cars are automatic and 13 cars are manual. Unlike a pie chart, we can read these numbers off of the plot and it’s easy to compare these two categories.\n\n\nTitanic Example\nThe following example demonstrates why it’s important to be clear about what an “individual” is. Depending on how we define an individual, we might choose to make a different plot!\nIf we’re measuring things about different Sexes (“sexes” are “individuals”), we’re doing a different study than if we consider ourselves to be making measurements about different Classes.\n\n\nBased on this plot, we might be asking how “Sex” varies by “Class”.\n\n\n\n\n\n\n\n\nBased on this plot, we might be asking how “Class” varies by “Sex”.\n\n\n\n\n\n\n\nExercises\nThe following exercises are based on the exercises in the (optional) OpenIntro biostats textbook. They are based on the Functional polymorphisms Associated with human Muscle Size and Strength, or famuss, data set.11\nThe data are available in the oibiostat pacjage in R, which is a package based on the OpenIntro Biostat textbook. Don’t worry if you don’t know what any of that means - it’s not important for this portion of the course.\n\nlibrary(oibiostat)\ndata(famuss)\nhead(famuss)\n\n  ndrm.ch drm.ch    sex age      race height weight actn3.r577x    bmi\n1      40     40 Female  27 Caucasian   65.0    199          CC 33.112\n2      25      0   Male  36 Caucasian   71.7    189          CT 25.845\n3      40      0 Female  24 Caucasian   65.0    134          CT 22.296\n4     125      0 Female  40 Caucasian   68.0    171          CT 25.998\n5      40     20 Female  32 Caucasian   61.0    118          CC 22.293\n6      75      0 Female  24  Hispanic   62.2    120          CT 21.805\n\n\nThe columns are a little difficult to understand, so here’s a brief overview.\n\n\n\nvariable\ndescription\n\n\n\n\nsex\nBiological sex of the patient\n\n\nage\nAge (years)\n\n\nrace\nSelf-reported race\n\n\nheight\nSubject’s height in inches\n\n\nweight\nSubject’s weight in pounds\n\n\nbmi\nSubject’s body mass index (weight divided by height\\(^2\\))\n\n\nactn3.r577x\nI think it’s obvious what this column is\n\n\ndrm.ch\nAgain, this is obvious.\n\n\nndrm.ch\nNo need to explain this one.\n\n\n\nOkay, so some of the columns need some explaining. The study was interested in whether a particular genotype affected the ability of subjects to build muscle. The genotype is at the genome position r557x in the ACTN3 gene, which is a fancy way of saying that the actn3.r577x tells us something about the subject’s genetic code. The genotype can be either TT, TC, CT, or CC, and the researchers believed that these genes determine how easily people build muscle.\nThe way the study was carried out is as follows. The researchers asked each subject to perform a test of strength using each arm. Subjects were then asked to follow a muscle strengthening exercise, but only on their non-dominant side (if they’re right handed, they were asked to strengthen their left arm). After the strengthening, they repeated the test of strength. The columns labelled drm.ch and ndrm.ch are the percentage change in their strength for their dominant arm and their non-dominant arm (the dominant arm was measured to see whether the patients gained or lost strength for reasons other than the prescribed strength training exercise).\n\nBased on the R output, what type of variable is each column?\nWhat counts as an individual in this study?\n\nEach person\nEach pair of people\nEach arm (two per person in the study)\nEach column in the data\n\nExplain why either a bar chart or a pie chart would be appropriate for the actn3.r577x column. Be extremely precise.\n\n\n\nOrdered and Unordered\nWhether the categorical variable is ordered or unordered affects the way we make the plot:\n\nOrdered: put the bars in order\n\nFor example, if we have data from 2020, 2021, and 2022, then we would have the bars in that order!\n\nUnordered: put it in an arbitrary order\n\nAlternative: order according to largest to smallest.\n\n\n\n\nCode\nlibrary(palmerpenguins)\nggplot(penguins) +\n    aes(x = species) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories\")\n\n\n\n\n\n\n\nCode\nlibrary(forcats) # For rearranging \"factors\", aka. categorical variables\nggplot(penguins) +\n    aes(x = fct_infreq(species)) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories, Ordered by Count\")\n\n\n\n\n\nThe bar chart is the de-facto standard for categorical variables, whether binary or otherwise. For quantitative, variables, we need other options.\n\n\nQuantitative Variables\nRecall the distinction between discrete and continuous:\n\nDiscrete (whole numbers)\n\nEx. Number of students in a classroom.\n\nContinuous (could be measured with more precision)\n\nEx. height\n\n\n\n\n\n\n\n\nGrey Area\n\n\n\nWhat type of variable is “dose level”, defined as either no dose, half dose, or full dose? They aren’t whole numbers, but we can’t measure them with greater precision!\n\n\n\nQuantitative variables are split into discrete and continuous variables. Discrete variables are generally represented by whole numbers, for example, the number of students in a given classroom.\nIn contrast, continuous numbers could be anything! I like to think of them as numbers that could’ve been measured with more precision if we had better tools. For example, peoples Heights could be measured to infinite precision if we had perfect tools, whereas we don’t need better tools to measure the number of children in a family more precisely.\nOf course, as with all things, there is a gray area here. Many studies will choose to give their subjects either no dose, a half dose or a full dose. These are obviously numbers and it is very likely that the response for a 0.75 dose is somewhere in between the half dose and the full dose. However, we chose these numbers and thus there are only three possible numbers. No amount of measuring is going to give us something other than a half dose (any deviation in administration of the dose can hopefully be ignored for the purpose of the study). In the definitions we’ve used it is neither a whole number, nor cannot be measured with higher precision. For the purposes of visualization, we might actually want to use a bar chart as if this were a categorical variable. If the dose had more categories and we expected the response to have a smooth trend across different dose levels, then we might use visualizations meant for discrete data. If the dose could have been any number between zero and one then we might use visualization meant for continuous data.\n\n\n\nPlotting Quantitative Variables\nHere are the lengths of sharks:\n 9.4 12.1 12.2 12.3 12.4 12.6 13.2 13.2 13.2 13.2 13.5\n13.6 13.6 13.8 14.3 14.6 14.7 14.9 15.2 15.3 15.7 15.7\n15.8 15.8 16.1 16.2 16.2 16.4 16.4 16.6 16.7 16.8 16.8\n17.6 17.8 17.8 18.2 18.3 18.6 18.7 18.7 19.1 19.7 22.8\n\nEven though we only know the data to one decimal point, we could have measured more precisely!\n\nThis is a continuous variable.\nNote that there are ties in these data, and thus we could make bar charts with bars that are more than one unit tall. However, if we measured more precisely, these ties would go away.\n\nCan’t just draw a bar chart with all sharks that were 9.4, all that were 12.1, …\n\n\nHow many we display this collection of shark lengths? It is clear that there are many different values that we could’ve gotten for the length and so we might not want to use something like a bar chart. Let’s try it anyway.\n\n\n\nQuantitative Variables as a Bar Chart\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\nsharks &lt;-  c(9.4, 12.1, 12.2, 12.3, 12.4, 12.6, 13.2, 13.2, 13.2, 13.2, 13.5,\n13.6, 13.6, 13.8, 14.3, 14.6, 14.7, 14.9, 15.2, 15.3, 15.7, 15.7,\n15.8, 15.8, 16.1, 16.2, 16.2, 16.4, 16.4, 16.6, 16.7, 16.8, 16.8,\n17.6, 17.8, 17.8, 18.2, 18.3, 18.6, 18.7, 18.7, 19.1, 19.7, 22.8)\n\nggplot() + aes(x = sharks) + geom_bar()\n\n\n\n\n\n\nThis plot demonstrates why bar chart isn’t appropriate for these data. We can see that each data point essentially gets its own bar, and so the heights are no longer meaningful. The exception is that these data are rounded to one decimal place, and so some lengths end up in the same bar. Knowing that some of our data are rounded to the same value is not necessarily meaningful for any analyses that we might want to do. Instead, we would like a chart that shows us where most of the data are, and whether or not they are clear patterns in these data.\n\n\n\nHistograms: Put observations into bins\nThe steps in building a histogram:\n\nChoose the bins.\n\ne.g. (0,10], (10,20], (20, 30], etc.\n\nThe notation (a, b] means that “a” is not included in the interval, but “b” is. We have no sharks that have a length of 0, but a shark with a recorded length of exactly 10 would be in the first bin, labelled (0, 10], not the second bin that is labelled (10, 20].\n\n\nCount the number of obs. in each bin.\nDraw a bar chart as if the bins are categories.\n\nBars should touch since there’s nothing in between.\n\n\n\n\nCode\n## Note that I've manually chosen the bin widths and centers.\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\nIn this histogram, the bins are (-1, 1], (1,3], (3,5]…\nNotice how the y-axis is still “Counts” (like a bar chart).\n\nMost of the time we will probably want to use a histogram to display quantitative, continuous data. A histogram is very much like putting continuous numbers into discrete bins, and then showing it as a bar chart. In this example, I chose bins from 1 to 3, then 3 to 5, then 5 to 7, and so on. For the bar on this histogram centred at a shark length of 12 we can see that there were five observations between 11 and 13. Note that the definitions of bins has a round bracket on the left side and the square bracket on the right side, this is to say that the left end point is not included but the right end point is included. This is just to account for cases where X may fall directly on the border between two bins, and we have to choose which bin. The actual bin we choose is arbitrary, kind of like driving in the left or the right. You will not be tested on whether you can remember which endpoint is inclusive.\nFrom the plot, we can see that most of the sharks are around 16 feet in length with sun going down to 10 feet and some as long as around 22 feet. The plot has a nice bell shape.\n\n\n\nHistograms: Bin Width Matters!\nThese histograms are showing the same data!\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=1.5, \n        center = 0.75, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nIn the previous graph, it looked like the distribution of sharks followed a nice bell-shaped curve. However, if we use bins that are 1.5 units wide we get a plot that looks fairly different. It still looks like most sharks are around 16 feet and some go down to 10 and some go as high is 22 or 23, But we see a large bar that covers 12 to 13.5.\nWith histograms the bins that you choose are extremely important. Most software have default values that are generally reasonable, But it’s always always always worth investigating other bins.\nA simple version of the plot can be made as follows, where ggplot chooses the bins automatically. Note that this is rarely desireable, and you should almost always choose the bins yourself.\n\nggplot() + \n    aes(x = sharks) +\n    geom_histogram()\n\n\n\n\nBelow is an app to visualize the difference that the binwidth can make!\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/DensHist\")\n\n\n\n\nDescribing Distributions\nWhen you’re asked to comment on a histogram, always mention the following:\n\nShape: Unimodal/bimodal and skewness\n\nSkewness: put a glob of peanut butter on toast, “skew” it to one side.\n\nCenter: midpoint (mean/median)\n\nMode depends on the bin!\nSkewness shows up in the relation between mean and median: “Mean less (than median) means left (skew).”\n\nSpread: the range/variance/IQR\n\nMore on IQR later!\n\nOutliers: points that don’t fit the shape\n\nMore on outliers when we cover IQR!\n\n\n\nThere are many shapes that a histogram can show. A distribution can be skewed (or “heavy-tailed”), which means that it looks like a bell curve but one side has a longer/thicker tail. We also want to know about several measures of the center of the distribution, as well as how spread out it is. Outliers are also something interesting to note; outliers are something that are not part of the shape (so you wouldn’t consider them when evaluating the skewness of a distribution).\nTry drawing out each of these shapes/patterns!\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis represents a bimodal distribution because it has two peaks (the word “mode” can refer to the category with the most observations, but it can also refer to the top of a peak). This would be described as a bimodal distribution with centres around 190 and 215, ranges around 195 to 205 and 205 to 235, with both peaks being symmetric and without any outliers.\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis is the classic sort of gotcha question that I like to use. This is actually a bar chart that I modified so that the bars have no space in-between - the x-labels are categories, not ranges! It may look somewhat symmetric and unimodal without any outliers, but the x-labels are out of order. These numbers are just numerical encodings of species names - 1 refers to Adelie penguins, 2 refers to Chinstrap, and 3 refers to Gentoo. These numbers were applied alphabetically because there isn’t really a logical way to order these species: they’re unordered categories!\nSo, basically, it does not make sense to talk about shape in a bar graph where the labels could have been put in any order!\n\n\n\nPurely Pedagogical: Stem-and-Leaf plots.\nConsider the data\n12, 43, 12, 32, 53, 66, 78, 25, 36, 12, 26,\n34, 98, 39, 44, 23, 15, 67, 1,  4,  54, 21\n\n\n\n\nStem-and-Leaf\n0  | 14\n10 | 2225\n20 | 1356\n30 | 2469\n40 | 34\n50 | 34\n60 | 67\n70 | 8\n80 |\n90 | 8\n\nSideways Histogram\n\n\n\n\n\n\n\n\nThis visualization technique is shown purely for pedagogical reasons. A stem and leaf plot is like a histogram where the bins are all powers of 10. It is displayed using the stem which is the first digit, and the leaf which is the second digit. For example, the number 78 has a 7 in the tens place (the tens place is the stem) and an 8 in the ones place (and we’re using the ones place as the leaf). In the stem and leaf plot, 78 goes on the stem labelled 70 and it gets a leaf of eight. Going the other way, we can see a stem labelled 90 and a leaf labelled eight which corresponds to the number 98. For the stem labelled zero we have the numbers one and four, for the stem label 10 we have the numbers 12, 12, 12, and 15.\nEssentially, this is just a histogram. Instead of drawing a bar that corresponds to the number of observations in that bin, we are just listing the observations in that bin. Compare the stem and leaf plot to the sideways histogram on the right: in the first bin from 0 to 10 (not including 10) there are two numbers, one and four, and the length of the bin is two. For the stem label 20, we have the numbers 21, 23, 25 and 26, and this is displayed as a bar with length four in the histogram.\nThe main reason for showing this visualization technique is that it can be very useful for tests and quizzes, because it allows you to create a histogram without software. It also allows easy computation of the median since all of the leaves are in order. These are not used in practice because in practice you will have software to create histograms and find the median!12\nNote that the shape of the distribution can be seen from the stem and leaf plot. It is a unimodal distribution that is right skewed and likely does not have any clear outliers (there is one point that appears to be separate from the others, but this might be due to bin choice).\n\n\n\nSummary\n\nIndividuals are what we make measurements on\n\nCan be pairs, dates, or people\n\nVariables are what we measure\n\nCan be derived from other measurements\n\nYou will not be asked to do anything with pie charts in this course.\nBar charts show counts of categories.\n\nCan optionally sum to 1 (like a pie chart).\n\nHistograms are like bar charts for binned data.\n\nBins matter.\nMust interpret shape."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#exercises",
    "href": "L01-Intro_PicturingGraphs.html#exercises",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises\n\nWhich of the following plots would be appropriate for the bmi column?\n\n\n\nBar chart\n\nWhat do you think? Does the plot below give you any useful information about the data?\n\n\n\n\n\n\n\n\n\nPie Chart\n\nNo. I’m not even going to try in this case.\n\n\n\n\nHistogram\n\nCorrect! The plot below tells us where most of the subjects’ BMI lie. We can see that most of the subjects are between 15 and 30, with a few between 30 and 35 and few above 35.\nThis is the whole reason we categorize variables as quantitative or categorical: knowing the type of the variable tells us which plots will give us useful information!\n\n\n\n\n\n\n\n\n\nPrescriptions of opioid pain relievers. Opioid pain relievers are prescribed at a higher rate in the United States than in any other nation, even though abuse of these medications can result in addiction and fatal overdoses. The CDC examined opioid pain reliever prescriptions in each state to find out how variable prescription rates are across the nation. Here are the 2012 state prescription rates, in number of prescriptions per 100 people, listed in increasing order:\n\nopiods &lt;- c(52.0, 57.0, 59.5, 61.6, 62.9, 65.1, 66.5, 67.4, 67.9, 69.6, 70.8, 71.2, 71.7, 72.4, 72.7, 72.8, 73.8, 74.3, 74.3, 74.7, 76.1, 77.3, 77.5, 79.4, 82.0, 82.4, 85.1, 85.6, 85.8, 88.2, 89.2, 89.6, 90.7, 90.8, 93.8, 94.1, 94.8, 96.6, 100.1, 101.8, 107.0, 109.1, 115.8, 118.0, 120.3, 127.8, 128.4, 137.6, 142.8, 142.9)\n\nMake a histogram of the state opioid pain reliever prescription rates using classes of width 10 starting at 50.0 prescriptions per 100 people. e.g. (50, 60]. Do this by hand first, then using R.\nWould you say that the distribution is single-peaked or multiple-peaked? Is it roughly symmetric or skewed?\n\nThe Statistical Abstract of the United States, prepared by the Census Bureau, provides the number of single-organ transplants for the year 2010, by organ. The following two exercises are based on this table:\n\n\n\nDisease\nCount\n\n\n\n\nHeart\n2,333\n\n\nLung\n1,770\n\n\nLiver\n6,291\n\n\nKidney\n16,898\n\n\nPancreas\n350\n\n\nIntestine\n151\n\n\n\n\nThe data on single-organ transplants can be displayed in\n\n\na pie chart but not a bar graph.\na bar graph but not a pie chart.\neither a pie chart or a bar graph.\n\n\nKidney transplants represented what percent of single- organ transplants in 2010?\n\n\nNearly 61%.\nOne-sixth (nearly 17%).\nThis percent cannot be calculated from the information provided in the table.\n\nSee also: OpenIntro Textbook problems relating to visualizations that we have learned, especially 1.30, 1.36, 1.37, 1.39, 1.40, 1.47."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#crowdsourced-questions",
    "href": "L01-Intro_PicturingGraphs.html#crowdsourced-questions",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.5 Crowdsourced Questions",
    "text": "1.5 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nIn a study examining the effects of a new dietary supplement on plant growth, researchers measure the following variables. Which of the variables listed is qualitative?\n\nType of plant (e.g., fern, moss, flowering plant)\nGrowth rate (cm per week)\nColor intensity of leaves (rated on a scale of 1-10)\nNumber of leaves produced after a 2-month period\n\n\n\n\nSolution\n\nThe correct answer is 1. Type of plant. This variable is qualitative because it describes a category or quality of the plant (e.g., fern, moss, flowering plant) rather than quantifying a characteristic. Qualitative data, also known as categorical data, includes descriptions or characteristics that cannot be measured or counted in the traditional sense. Variables 2, 3, and 4 are quantitative because they involve measurements or counts that result in numerical values: growth rate is measured in cm per week (continuous), color intensity of leaves is rated on a numerical scale (ordinal), and the number of leaves is a count (discrete).\n\n\n\nTrue or False: American ZIP codes (e.g. 90210) are quantitative variables\n\n\n\nSolution\n\nFalse. Although quantitative variables are defined as numbers and a zip code is made up of numbers it’s just a category for organizing data. It does not make sense to take the average zip code!"
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#footnotes",
    "href": "L01-Intro_PicturingGraphs.html#footnotes",
    "title": "1  Picturing Distributions with Graphs",
    "section": "",
    "text": "We’ll talk about what “explaining variance” means, along with exact ways to calculate it, in a later lecture.↩︎\nIt is worth noting that there is also variance that isn’t information, and there’s information that we’ll never have access to. Variance is an opportunity to learn, but there’s almost always a limit to how much we can learn.↩︎\nA note on gender/sexuality/biology: penguins, especially Chinstrap and Gentoo penguins, don’t have particularly strong gender roles, and often form same-sex couples. In this course, I will use the term “sex” to mean “biosex”, rather than “gender”, to indicate that we’re looking at morphological differences due to XX and XY chromosomes.↩︎\nPossibly categorical if there are any penguins with chromosomal abnormalities.↩︎\nWe may, however, know that factory workers have more exposure to a pathogen than those who work from home, but we would likely want to measure this directly rather than measuring it by proxy with occupation.↩︎\nWe often encode on of the categories as 0 and the other as 1, but this is usually either clear (0 = no treatment, 1 = treatment) or completely arbitrary (0 = femala, 1 = male) and this arbitrariness is acknowledged. Neither of these cases affect the way we make plots or run analyses that are based on binary variables.↩︎\nNot discreet.↩︎\nThere are methods/visualizations that are specific to discrete variables, but they only apply in very specific circumstances and will not be taught in this course.↩︎\nOf course, by “information”, I mean variance. If we don’t know the date that an observation was observed, we just have a collection of different numbers. If we see that there’s a pattern over time, we know a little more about why each number is different from the others!↩︎\nIt will still create a plot with the correct x and y axes, but won’t draw the bars.↩︎\nThe “FAMUSS” acronym is a bit of a stretch, but just go with it.↩︎\nThat software is R!↩︎"
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#quantitative-variables",
    "href": "L02-Describing_Distributions_Numbers.html#quantitative-variables",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.1 Quantitative Variables",
    "text": "2.1 Quantitative Variables\n\nParameters Versus Statistics\n\nNumerical summaries are called parameters when they describe an entire population, and statistics when they describe a sample.\nSince we typically will not collect data on the entire population, we use (sample) statistics to make statements (with some uncertainty) about what the (population) parameter is.\nCategorical variables are summarized by the count or proportion (percent) of the data in each category.\nQuantitative variables are summarized by the centre and spread of the data\n\n\n\nParameters Versus Statistics: “““Truth”“”\nWe assume there’s some “true” population mean.\n\nThis will change if, say, a new baby is born.\n\nWe cannot know the population parameter!\n\nIf we were able to measure the height of every Canadian at this very instant, we would get one value. We can’t do this though, so we collect a sample instead. We try to collect the sample in such a way that we get value close to the population parameter.\nIn this course, we make a big distinction between population parameter and sample statistic. A sample statistic is something that we calculate from a sample, where is the population? Parameter is the value that we would get if we had the whole population.\nMost of this course is centred around using the sample statistic to get an idea of what the population parameter might be.\n\n\n\nMeasures of Centre\n\n\nThe Centre\nThe “centre” is a strange concept.\n\nWhere are “most” of the data?\nIf we took an individual at random, what’s the “best guess” of their height?\n\nWe want to summarize the data using a few words/numbers.\nI prefer the second description here - if I were to make a prediction, I want my predicted value to be close to the actual value. For example, I might want to design something that fits the average person’s height. I must make a prediction about the height of the people that will use it. My prediction is chosen to minimize how far off I would be, which means that I want to be near the “centre”. ### Centre 1: The Median\nThe median (\\(M\\)) is the midpoint of a distribution: half the observations are smaller and the other half are larger than it. To find the median:Median: The middle number after you’ve put all the values in order.\n\nArrange all observations in order of size, from smallest to largest.\nIf the number of observations (\\(n\\)) is odd, the median \\(M\\) is the centre observation in the ordered list. Find the location of the median by counting \\((n + 1)/2\\) observations up from the smallest observation.\nIf n is even, the median \\(M\\) halfway between the two centre observations in the ordered list. The location of the median is again \\((n + 1)/2\\), counting from the smallest observation in the list.\n\n\nBasically, the median is the middle value. With odd \\(n\\), the middle value is one of the observations, but when \\(n\\) is even we have to go in between two values.\n\n\n1\n\nThe median is 1.\n\n1 2\n\nThe median is 1.5, which occurs at the \\((n+1)/2\\) position.\n\n1 2 3\n\nThe median is 2.\n\n1 2 3 4\n\nThe median is 2.5\n\nContinue the pattern on your own!\n\n\n\nExample 1: Odd \\(n\\)\nThe distribution of needle lengths is how species of pine trees are characterized. The following data are the lengths (in cm) of a sample of 15 needles taken at random from different parts of several Aleppo pine trees (Southern California). What is the median length?\n7.20 7.60  8.50  8.50  8.70  9.00  9.00 9.30\n9.40 9.40 10.20 10.90 11.30 12.10 12.80\n\n\nExample 2: Even \\(n\\)\nWe also have the lengths (in cm) of 18 needles from trees of the noticeably different Torrey pine species. What is the median length for these 18 pine needles? The ordered data are:\n21.20 21.60 21.70 23.10 23.70 24.20 24.20 25.50 26.60\n26.80 28.90 29.00 29.70 29.70 30.20 32.50 33.70 33.70\n\n\nExample 3: Robustness\nSet 1: 1 2 3 4 5 6\nSet 2: 1 1 1 6 6 6\nSet 3: -2000 -1000 3 4 5000 60000000\nAll three have the same median!\nDo the “centres” make sense? Do they provide a good summary?\n\nThese three data sets demonstrate that the median only depends on the middle two numbers when \\(n\\) is odd1. For the first two data sets, the median does seem to describe centre of the data well.\nThe last data set is a little bit different from the others and the median might not be enough information. This will come up again and again: if your data set is “nice” (unimodal and no clear outliers), then summary statistics work well; a simple number can describe a simple data set. A complex data set needs more complicated numbers.2\n\n\n\nThe Mean\nThe mean is defined as:Mean: Add all the numbers, divide by how many numbers you added.\n\\[\n\\bar x = \\frac{1}{n}(x_1 + x_2 + ... + x_n)= \\frac{1}{n}\\sum_{i=1}^nx_i\n\\]\nFor example, the mean of (1, 2, 3, 4) is \\[\n\\frac{1}{4}(1 + 2 + 3 + 4) = \\frac{10}{4} = 2.5\n\\] (this also happens to be the median!)\nThe mean has a few interesting interpretations:\n\nIf we had a metre stick and put a 1kg weight on each of the values, the mean is where the metre stick would balance.\nIf someone pulled one of the observed values out of a hat and we were punished based on how wrong our guess was, the mean would be least overall punishment. This formula for the mean may look a little bit scary, but all it means is that we add up all of the values that we have and divide by the number of values.\n\nCreate a couple examples yourself and find the mean! If you create a list of values, you can use R to check your work as follows:\n\nmy_values &lt;- c(1, 2, 2, 3, 4, 5, 6.3212, 3)\nmean(my_values)\n\n[1] 3.29015\n\n\n\nThe name “my_values” could have been anything that is just letters, numbers, and underscores (no spaces, can’t start with a number). It’s just the name of an object in R.\n\nAn object is just a thing. You can put two things together, do a function of a thing, etc.\n\nmy_values is a vector. That is, it’s a collection of values.\n\nWe create it using the c() function, which means “concatenate”, or “put together”.\n\nThe mean function takes a vector and calculates the mean for you.\n\nTo run this code, go to and create a new R notebook. Insert an “R cell”, and copy/paste the code above into that cell. Either hit the “play” button, or Alt+Enter (Option+Enter on a Mac). Alternatively, you can open up an R script in RStudio if you have it installed on your computer.\nA word of caution: R sometimes calculates the median in a different way than we do in the course. Check your work with R, but do it by hand when asked!\n\n\nThe mean is not robust\nConsider the following sets of data:\nSet 1: 1, 2, 3, 4, 5, 6\nSet 2: 1, 2, 3, 4, 5, 60\n\nBoth have the same median.\nMean of 3.5 and 12.5, respectively.\n\nWhich do you think is the correct measure of the centre?\n\nUnlike the median, which only depends on the middle value, the mean uses information from all of the values. This means that if there’s an outlier or a misrecorded point, the mean will reflect this. The median will not change, though, which is why we call it robust. Sometimes this is what we want and sometimes it is not. Robust: A statistic is robust if a small change to the data, such as adding/removing an outlier, does not affect that statistic.\nA common example of when the mean is not what we want is income. The lowest possible income is zero, but there is no maximum income, so incomes tend to be right skewed. The right skew affects the mean a lot more than that affects the median, and so, in this case the median is a better measure of the most common levels of income.\n\n\n\nMean and Median vs. Skew: Mean less means left\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/MeanLessMeansLeft\")\n\n\nThe app above will be used in class, so do not worry if you’re unable to run it. It demonstrates that when they mean is less than the median it means it’s left skewed. The reason for this is that extra weight in the skewed direction affect the mean more than they affect the median."
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "href": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.2 Measures of Spread",
    "text": "2.2 Measures of Spread\n\nWhich has more variance?\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nAll have the same range (max - min).\n\nRange: The difference between the maximum and minimum.These three data sets all have the same mean and median, but just looking at them shows that they are different collections of numbers. The first set only has two unique values, but those values are relatively far away from each other compared to the other sets. The second set is a more even spread from one to five. The third set has four valued equal to the mean and two values that may be outliers.\nTo me, the first set looks like it’s the most variable because all of the values are very far away from either the mean or the median. The second set has a smaller variance, because there are values closer to the mean. And the last set I expect to have the lowest variance because most values are actually equal to the mean.\nThe formula for the variance, which will be introduced next, matches this intuition. However, many students have different intuitions about which has the most variance - those are valid but harder to quantify.\n\n\nMeasure of Spread: The Variance\nConsider set 1, which has a mean of 3:\n1 1 1 5 5 5\n\nThe distances to the mean are all -2 or 2\n\nIf we found the mean of these distances, we’d get 0! We need to make sure they’re all positive.\n\nPossibility 1: Absolute value. The average absolute distance to the mean is 2.\nPossibility 2: Squared value.\n\nThe average squared distance to the mean is 4\n\nImportant: This is not the actual variance calculation!\n\n\n\nThe variance is the average squared distance to the mean[Variance: The average squared distance to the mean.]\nWe are basically (but not quite) looking at the average deviation from the mean. We want that deviation to be positive and there are several ways to do this. We have settled on squaring the numbers for the same reason we drive on the right side of the road in Canada: it’s just convention. There are benefits to using the absolute distance from the mean, but there are many mathematical advantages to squaring the values first.\n\n\nVariance Formula\n\n\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2\n\\]\nWe use \\(n-1\\) because of math reasons.\n\nThe easiest way to calculate this is to put it in a table:\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n1\n-2\n4\n\n\n2\n1\n-2\n4\n\n\n3\n1\n-2\n4\n\n\n4\n5\n2\n4\n\n\n5\n5\n2\n4\n\n\n6\n5\n2\n4\n\n\n\\(\\sum\\)\n18\n0\n24\n\n\n\nThe mean is 3, and the variance is 24/5 = 4.8.\n\n\nIn the table above, as before, the subscript \\(i\\) is just used to denote different observations. For example \\(x_1\\) is the first observation, \\(x_2\\) is the second observation in our data, and so on (this ordering is arbitrary).\nIn order to calculate the variance, we must first know the mean, and so it’s convenient to put this at the bottom of the table. We then square the deviations from the mean and divide by \\(n-1\\). There are very good technical reasons why we divide by (n-1) that we won’t get into here. Come to my office and chat if you’d like to know more, or just ask ChatGPT!\n\n\n\n\n\n\n\\(n-1\\) in the denominator\n\n\n\nAs a quick explanation for \\(n-1\\), consider the variance of a single observation. It doesn’t vary! There’s not enough information to see how much variance there is. There isn’t enough information in our data. The \\(n-1\\) in the denominator enforces this - we can’t calculate the variance of one observation.\n\n\nNote that the variance can be calculated in R as follows:\n\nmy_values &lt;- c(1, 2, 2, 3, 4, 5, 6.3212, 3)\nvar(my_values)\n\n[1] 3.050982\n\n\n\n\nThe Variance and the Standard Deviation\n\\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x - \\bar x)^2}\n\\]\nThe standard deviation (or sd) is just the square root of the variance.\n\nThis makes it have the same units as the original data.\n\n\nIn addition, if we have two data sets and the variance of one is larger than the other, then the standard deviation is also larger. They’re the same thing, just in different units!\nI will often refer to one when I mean the other. When I’m comparing standard deviations, I may call them variances because the same patterns will be there.\nHere’s the R code:\n\nsd(my_values)\n\n[1] 1.746706\n\nvar(my_values)\n\n[1] 3.050982\n\nsd(my_values)^2 # the sd is the square root of the variance\n\n[1] 3.050982\n\n\n\n\n\nExercise: Variation of the three sets\n\n\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nDraw out bar plots.\nSet up the table and find the variance.\nCompare the standard deviations; make a conclusion.\n\n\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n\\(\\sum\\)\n\n\n\n\n\n\nFill out the table yourself, then try with R.\n\n\n\n\nMeasure of Spread 2: The IQR\nThe IQR is very closely related to the median. But first, we must learn what quartiles are.\nConsider the data:\n1 2 3 4 5 6 7 8\nThe median of these data is 5; 50% of the data are to the left of this point. This is half the data. If, instead, we wanted a quarter of the data, we could find half of the half.\n\n“Quartile”: Split the data into four.[Quartile: One quarter of the data. The first quartile is the first 25%. The second quartile is the median.]\n\nQ1: 25% of the data are to the left.\nQ2: 50% of the data are to the left (the median).\nQ3: 75% of the data are to the left.\n\n\n\n\nFinding Quartiles\n1 2 3 4 5 6 7 8\n\nFind the median\n\nIt’s 4.5. Cool.\n\nQ1 is just half of 50% - we’re finding a median again!\n\nQ1 is the median of everything to the left of the median (don’t include the median when doing this calculation).\nIn this case, 1 2 3 4 are the numbers to the left of the median, and so Q1 is 2.5.\n\nBy a similar argument, Q3 is 6.5. :::notes Q0 is where 0% of the data are to the left. In other words, it’s the minimum value in the data! Similarly, Q4 is the maximum value in the data.\n\n\n\n\n\n\n\nWarning\n\n\n\nThe algorithm we just used for computing the quartiles is not the only one! In R, there are NINE different ways to calculate the quartiles. You should stick to doing this by hand if you want to get the WeBWork answers right.\n\n\n\n\nThe five number summary\nLet’s use the folowing example:\n1, 3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 8, 9, 10, 10, 11, 12\nThe quartiles give an excellent way to summarise data:\n\n\n\nQ0 (min)\nQ1\nQ2 (median)\nQ3\nQ4 (max)\n\n\n\n\n1\n4.5\n7\n8.5\n12\n\n\n\n\nThe five number summary just shows all five of the quartiles. Note that there are five quartiles, because zero is also one of them.\nFor practice, make sure you can calculate the median, and then the median of all the values to the left of it!\n\n\n\nVisualizing the five number summary: the boxplot\n\n\nThe plot on the right shows the body masses for the Palmer Penguins.\n\nThe lowest point is Q0\nThe left of the box is Q1\nThe thick line in the box is Q2 (the median)\nThe right of the box is Q3\nThe highest point is Q4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot and the histogram both demonstrate the right skew of the data, but the boxplot is much more compact!\nTake a moment to compare the two plots and make sure you can explain the skewness. Remember that 25% of the data are in each interval shown in the box plot!\n\n\n\nMeasure 2: The Inter-Quartile Range (IQR)\nThe IQR is defined as: Q3 - Q1.IQR: The distance between the first quartile and the third quartile.\n\nSame units as the original data\nRobust to outliers (unlike the sd)!\n\n\nThis is the second measure of spread that we will learn. The IQR is commonly used when we have highly skewed data or data with outliers. The sd measures the average squared deviation from the mean, whereas the IQR measures the middle 50% of the data.\nNotice how this is not centered on the median. Consider the following data:\n\nmy_values &lt;- c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 10)\nlength(my_values) # number of observations\n\n[1] 20\n\nhist(my_values)\n\n\n\n\nThe median of these should be at position \\((n+1)/2 = (20 + 1)/2 = 10.5\\) (I used the length() function in R to count the number of observations for me). This value is halfway between 3 and 3, meaning it’s 3. Q1 is the median of the first 10 data points (we don’t include the median, but this doesn’t matter here), which is at position 5.5, giving us a value of 2. Q3 is 5.5 positions from the end, which is 4.5. Thus the IQR is \\(4.5 - 2 = 2.5\\).\nFirst, does this make sense to you? Does 2.5 sound like a reasonable width for the middle 50%?\nNow consider that the distribution is clearly skewed to the right. This affects the variance a lot, but the IQR would have been the same no matter what the first 4 or last 4 values were.\n\n\n\nIQR for Outliers\nIn this class, we use a rule of thumb for calculating outliers. Anything that is…\n\nBelow Q1 minus 1.5*IQR, or\nabove Q3 plus 1.5*IQR\n\nis considered an outlier.\n\nThis rule of thumb is not based on any mathematical derivations, it just seems to work in most situations.\nThe idea is that the IQR gives a measure of spread, and the median gives the measure of the centre, so anything too far from the centre is an outlier. We use the spread to figure out how far away from the centre we are willing to accept. This will show up several times in this course. We’ve seen it in this example for the IQR and median because this is simple and easy to interpret.\nMost of the rest of this course will be spent looking at something similar for the mean. We will still use this idea of the centre plus or minus some measure of the spread, but will incorporate information about the sample and assumptions about the population that allow us to make much stronger conclusions beyond simply checking if something is an outlier.\n\n\n\nSummary\n\nThe “centre” is trying to measure the most common value.\n\nOften, this is our best prediction.\n\nThe “spread” is trying to measure the scale, or variation.\n\nGives context to the centre.\n\nThe mean and variance\n\nInterpretations and formulas are important.\n\nThe median and IQR\n\nCalculations, interpretations, five-number-summary, outliers, and boxplots are all important.\n\n\n\nWe saw the same thing a couple of times throughout this lesson. We saw measures of centre that try to describe the middle of a distribution and centres of spread that tell us how spread out the data are. The mean and the sd are intrinsically linked, and the median and IQR are intrinsically linked.\nWe also saw the rule-of-thumb to use IQR for finding outliers by using the median plus-an-minus some number times the spread. You better believe that this idea will show up again later in this course!\nBoxplots are a visual representation of the five number summary. These can be very small while still showing the shape of our data. However, these only work for unimodal data - there isn’t a good way to show a bimodal distribution on a boxplot. Also, it is very easy to plot two boxplots for two different data sets in order to compare the distributions.\nFor assignments and exams, be ready to calculate any of these values and compare the mean/median and sd/IQR. Also be ready to compare the five number summary to a boxplot.\nExercises\n\nSpider Silk. Spider silk is the strongest known material, natural or man-made, on a weight basis. A study examined the mechanical properties of spider silk using 21 female golden orb weavers, Nephila clavipes. Here are data on silk yield stress, which represents the amount of force per unit area needed to reach permanent deformation of the silk strand. The data are expressed in megapascals (MPa):\n\n164.00 173.00 176.10 236.10 251.30 270.50 270.50\n272.40 282.20 288.80 290.70 300.60 327.20 329.00\n332.10 351.70 358.20 362.00 448.90 478.70 740.20\n\nDescribe the shape, centre, and spread of the data using a histogram (code below).\nFind the mean and median yield stress. Compare these two values. Referring to the histogram produced by the code below, what general fact does your comparison illustrate?\nRe-run the code using different values of breaks. What do you see? (Note that this example uses base R rather than ggplot2 because it has simpler code - ggplot2 has more flexibility, but that flexibility isn’t necessary here.)\nUse the boxplot() function to create a boxplot (you do not need the breaks=10 part of the code). Compare this to the histogram. Also comment on any points that stand out (when there are outliers, R shows \\(Q1 - 1.5IQR\\) rather than Q0).\nUse the \\(Q1 - 1.5IQR\\) and \\(Q3 + 1.5 IQR\\) formulas by hand to find the outliers, and verify your calculations with the R plot.\n\n\nsilk_stress &lt;- c(164.00, 173.00, 176.10, 236.10, 251.30, 270.50, 270.50,\n    272.40, 282.20, 288.80, 290.70, 300.60, 327.20, 329.00,\n    332.10, 351.70, 358.20, 362.00, 448.90, 478.70, 740.20)\nhist(silk_stress, breaks = 10)\n\n\n\n\n\nDeep-sea sediments. Phytopigments are markers of the amount of organic matter that settles in sediments on the ocean floor. Phytopigment concentrations in deep-sea sediments collected worldwide showed a very strong right-skew. Of two summary statistics, 0.015 and 0.009 gram per square meter of bottom surface, which one is the mean and which one is the median? Explain your reasoning.\nGlucose levels. People with diabetes must monitor and control their blood glucose level. The goal is to maintain a “fasting plasma glucose” between approximately 90 and 130 milligrams per deciliter (mg/dl). The data tables below give the fasting plasma glucose levels for two groups of diabetics five months after they received either group instruction or individual instruction on glucose control.\n\nI provide the data as vectors in R, but you don’t need R for this question (it’s good practice to do it both ways).\n\ngroup &lt;- c(78.00, 95.00, 96.00, 103.00, 112.00, 134.00, 141.00, 145.00, 147.00,\n    148.00, 153.00, 158.00, 172.00, 172.00, 200.00, 255.00, 271.00, 359.00)\n\nindividual &lt;- c(128.00, 128.00, 158.00, 159.00, 160.00, 163.00, 164.00, 188.00, 195.00,\n    198.00, 220.00, 221.00, 223.00, 226.00, 227.00, 283.00)\n\n\nCalculate the five-number summary for each of the two data sets.\nMake side-by-side boxplots comparing the two groups. What can you say from this graph about the differences between the two diabetes control instruction methods? (Hint, you can create side-by-side boxplot using the code boxplot(variable_1, variable_2).)\nObtain the mean and standard deviation for each sample. Does this information give any clue about the shape of the two distributions?\nAdd to the historgrams a symbol representing the mean of each group and error bars representing one standard deviation above and below the mean. (You can do this by hand.) Compare this graphical summary with the boxplot display you also created.\nUse the 1.5 × IQR rule to identify any suspected outliers. Then look at the raw data to determine if unusually high or low values in either data set actually are outliers.\n\n\nThe Ecological Fallacy. In the data below there are 40 data points, but the data are included in different groups. Consider the following possibilities:\n\nFind the sd of all of the data.\nFind the mean of each group, then find the sd of the means. Before working on this question, what do you expect? Will the sd of the data be larger than, smaller than, or the same as the sd of the means?\n\n\nOnce you’re very certain that you know the answer (you don’t learn anything unless you put effort into, so do the hard thing), uncomment the commented code (i.e., remove the # at the start of the last two lines) and see if you were right!\n\ng1 &lt;- c(1, 2, 3, 4, 5, 6)\ng2 &lt;- c(4, 3, 5, 2, 3, 4)\ng3 &lt;- c(6, 7, 4, 7, 8, 9, 5, 6)\ng4 &lt;- c(7, 4, 5, 7, 3, 6, 7)\ng5 &lt;- c(7, 7, 7, 7, 5, 12)\ng6 &lt;- c(2, 3, 4)\ng7 &lt;- c(4, 3, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0)\nall_g &lt;- c(g1, g2, g3, g4, g5, g6, g7)\nall_means &lt;- c(mean(g1), mean(g2), mean(3), mean(4), mean(5), mean(6), mean(7))\n# sd(all_g)\n# sd(all_means)\n\n\n\nSolution\n\nThe Ecological Fallacy is that the data looks less variable if we only look at the average values in different groups.\nFor example, Statistics Canada releases information of the average household income for people who are 16-24 years old, 25-34, etc. The variance of these averages is NOT related to the actual variance of the values. When we learn more about statistical tests, we learn just how important the variance is - getting it wrong by not acknowledging that we’re working with means is a HUGE problem!"
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#categorical-variables",
    "href": "L02-Describing_Distributions_Numbers.html#categorical-variables",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.3 Categorical Variables",
    "text": "2.3 Categorical Variables\nThese variables are much harder to quantify with a single summary statistic. Instead, we usually give a table of their counts or just draw a plot. There is no concept of “mean” that applies to all categorical variables, and there is especially no concept of “variance”. The one exception is the mode (the most commonly observed category), but on it’s own this gives us very little information about the “centre” or “spread” of the data.\nInstead, we simply show as few numbers as we can while still displaying enough information:\n\nFrequency Tables\n\nOne-Way Table\n\n\n\nfemale   male \n   165    168 \n\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\n\nThe number of penguins in each category.\nFrom what’s given,m we can’t tell how many Chinstrap penguins were female.\n\n\n\nTwo-Way Table\n\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\n\n\n\n\nfemale\n73\n34\n58\n\n\nmale\n73\n34\n61\n\n\n\n\n\n\nThe number of penguins in each combination of categories.\nWe can find that there were 165 female penguins by adding the entries in the row labelled female."
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#crowdsourced-questions",
    "href": "L02-Describing_Distributions_Numbers.html#crowdsourced-questions",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.4 Crowdsourced Questions",
    "text": "2.4 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nGiven this set of numbers: 23, 19, 27, 36, 21, 29, 38, 39, 42, 25, determine the Q0, Q1, Q2, Q3, and Q4. Measure the spread by determining the IQR of this set of numbers.\n\n\n\nSolution\n\nFirst we will re-order the numbers from smallest value to largest value. 19, 21, 23, 25, 27, 29, 36, 38, 39, 42.\n\nQ0 is the lowest value so that will be 19.\nQ4 is the highest value in the set so that will be 42.\nQ2 is the median. Since the number of values (n) is even, the median will be between the middle two numbers, 27 and 29. Making Q2 = 28.\nQ1 is the median between the values between Q0 and Q2 (19, 21, 23, 25, 27). The median in this set is 23, making Q1 = 23.\nQ3 is the median of the second half of the data set, between Q2 and Q4 (29, 36, 38, 39, 42). The median in this set is 38, making Q3 = 38.\n\nNext to determine the IQR, we subtract Q1 from Q3.\nIQR = Q3-Q1 = 38 - 23 = 15\nTherefore, the Interquartile range for this set of data is 15.\n\n\n\nThe test scores for the ST231 midterm are as follows: 64, 77, 79, 88, 99, 81, 72, 77, 70 and 89. Given this information, are there any outliers?\n\nYes, 64 is an outlier\nYes, 99 is an outlier\nYes, both 64 and 99 are outliers\nNo, this data set does not contain any outliers\n\n\n\n\nSolution\n\nAnswer: d\nFirstly, put data in numerical order: 64, 70, 72, 77, 77, 79, 81, 88, 89, 99\nSecondly, find the five-number summary: Q0 (min) = 64, Q1 (25%) = 72, Q2 (median) = 78 ((77+79)/2) , Q3 (75%) = 88, Q4 (max) = 99\nThirdly, determine the interquartile range: IQR = Q3-Q1 = 88-72 = 16\nFourthly, use the outliers formula, below Q1-1.5(IQR) OR above Q3+1.5(IQR) : lower bound = 72-1.5(16) = 48 ; upper bound = 88+1.5(16) = 112\nFifthly, make a conclusion: since no values fall below 48 or above 112, there are no outliers for this data set."
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#footnotes",
    "href": "L02-Describing_Distributions_Numbers.html#footnotes",
    "title": "2  Describing Distributions with Numbers",
    "section": "",
    "text": "The median only depends on the middle number (singular) when \\(n\\) is even.↩︎\nIf your data requires complex numbers (\\(\\sqrt{-1}\\)), then you have a very interesting data set!↩︎"
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#relationships",
    "href": "L03-Scatterplots_Correlation.html#relationships",
    "title": "3  Scatterplots and Correlation",
    "section": "3.1 Relationships",
    "text": "3.1 Relationships\n\nExplanatory and Response Variables\n\nResponse: Responds to the explanatory variable.\n\nAlso called dependent variable.\n\nExplanatory: Explains the response variable.\n\nAlso called independent variable.\n\n\nKnowledge about explanatory tells us about the response.\n\nWe are not assuming the explanatory causes the response. We will not be covering causality in this course.\nWe are discovering tendencies, not rules.\n\n\nI just want to make this very clear: we are not looking for a causation. Instead, we’re just looking at whether or not to variables are related, and we think that measurements of one will be enough to tell us about measurements of the other. For example, if we think one variable is easy to measure and another is harder to measure, then we might want to set the easy to measure variable as the explanatory variable and see if it “explains” the harder to measure variable. This has nothing to do with the easy to measure variable causing the hard to measure one.\n\n\n\nExamples\n\nBlood alcohol content affects reflex time. – Some individuals may be more or less affected.\nSmoking cigarettes is associated with increased risk of lung cancer, and mortality. – Some heavy smokers may live to age 90\nAs height increases, weight tends to increase.\n\nHeight does cause weight, but there are other explanations.\n\n\n\nIn these examples, we carefully use words like “affects”, “associated with”, and “tends to”. For all of these examples we would expect a relationship of some sort, but the causality is not necessarily obvious.\nWe obviously expect the blood alcohol contact to affect reflex time. We expect this to be a causal relationship.\nIn the mid-1900s, it was hypothesized by cigarette companies that, rather than cigarettes causing cancer, people who were at increased risk of lung cancer with the sorts of people who also tended to smoke. Finding a relationship was not enough to convince people that it was cigarettes causing lung cancer. Even though we know that there’s a relationship between cigarettes and lung cancer, the techniques we learn in this course are not enough to conclude causality.\nHeight and weight are an example of how are the knowledge of one variable tells us about the other, without there being any causal relationship. We expect that taller people will have more mass, but there are also other reasons why somebody might have more mass that or not captured by their height."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#scatterplots",
    "href": "L03-Scatterplots_Correlation.html#scatterplots",
    "title": "3  Scatterplots and Correlation",
    "section": "3.2 Scatterplots",
    "text": "3.2 Scatterplots\n\nExample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\nChinstrap\nDream\n50.0\n19.5\n196\n3900\nmale\n2007\n\n\nChinstrap\nDream\n51.3\n19.2\n193\n3650\nmale\n2007\n\n\nChinstrap\nDream\n52.7\n19.8\n197\n3725\nmale\n2007\n\n\n\n\n\n\nOne penguin has a flipper length of 193.\n\nThat same penguin has a body mass of 3650.\nBoth measurements were made on the same individual.\n\nIndividual: thing we are making measurements on.\n\n\nEach value on the x-axis is paired with a value on the y-axis.\n\n\n\n\n\n\n\n\n\n\nIn the plot above, each individual has multiple measurements recorded on them. Because of this, we can plot each pair as a point in the plot. Note that we need to know which x-value is associated with each y-value in order to make the plot!\n\n\n\nWhat to look for\n\nOverall pattern\n\nLinear, curved, etc.\nDirection (increasing/positive, decreasing/negative)\nConstant variability\n\nDeviations from the pattern\n\nE.g., linear only in a small range\n\nOutliers\n\nAs before, discuss outliers separately from the pattern.\n\n\n\nIn general for this course were looking for a linear pattern. There are other models out there that fit nonlinear patterns, but we do not cover them in this course. There’s one way for things to be linear, and there are an infinite number of ways for things to be nonlinear. However, there are many common ways to account for non-linearity while still using a linear model.\nRegardless of whether something is linear or has some sort of curve, we are very interested in how strong of a pattern there is. For a linear model this means we want the points to be very close to the line, whereas for non-linear models we want the pattern to be very clear. We generally want patterns to pass the “facial impact test”, were the pattern is so obvious that it might as well be slapping you in the face (this is not an official test).\nAs with describing the shape of histograms, we treat outliers as something that are not part of the shape. We can have a clear linear pattern that happens to have an outlier.\n\n\n\nPenguins!\n\n\nWhat pattern is this?\n\n\n\n\n\n\n\n\n\nThe plot above shows a clear linear pattern. There is still some variation above and below the lines, but the pattern is still clear. It kinda looks like there may be two clusters; there’s a space between the two groups in the center of the X axis.\n\n\n\nAdding a Categorical Variable\n\n\nEach point has an \\(x\\) coordinate, \\(y\\) coordinate, and some other information. We can encode that information with a colour! Again, we have to have a categorical variable measured on the same individual as the x and y values.\n\n\n\n\n\n\n\n\n\nFrom this plot, we can see that the three species in these data all have a similar relationship, but still it might be worth separating out the groups and seeing what happens!\nBonus question: the first plot of body mass versus flipper length only showed one species. Can you tell which species?\n\n\n\nThe Importance of Plotting: Anscombe’s Quartet\n\n\n\n\n\nvariable\nmean\nsd\n\n\n\n\nx1\n9.000000\n3.316625\n\n\nx2\n9.000000\n3.316625\n\n\nx3\n9.000000\n3.316625\n\n\nx4\n9.000000\n3.316625\n\n\ny1\n7.500909\n2.031568\n\n\ny2\n7.500909\n2.031657\n\n\ny3\n7.500000\n2.030424\n\n\ny4\n7.500909\n2.030578\n\n\n\n\n\n\nIn this course we’re introducing plots before we talk about numerical summaries of two variables for a very good reason. The table above shows summary statistics from a well-known data set called Anscombes quartet. Up to the first two decimal places, all of the variables in the data have the same mean and standard deviation. If this were all of the information you had, you might expect the plots of y1 versus x1, y2 versus x2, y3 versus x3, and y4 versus x4 to look similar.\nInstead, they look like this:\n\n\n\n\n\n\n\nClearly, there’s a very different pattern in each plot.\n\nThe first plot looks relatively linear with a little bit of random variation. For this data set a linear model does seem appropriate.\nThe plot at the top right she was a very clear pattern that is not linear, so we may be able to fit a model that accounts for this non-linearity.\nThe plot at the bottom left is almost a perfect line, but with an outlier. This outlier makes it so that the line that I have added to the plot doesn’t actually go through the perfect pattern that we can see if that outlier weren’t there.\nThe bottom right plot is a mess. If it weren’t for the outlier, the X values would all be identical! In this case, a scatterplot would not be appropriate. If I saw this while analysing my data, I would have assumed that X was supposed to be either constant (e.g., all X values should have been 8) or categorical. In both cases, a scatterplot would not be appropriate.\n\nDespite all of these wildly different shapes, all of these data sets have the same summary statistics.\nPlot. Your. Data.\n\n\n\nSummarizing Plots\n\nEach data point has an \\(x\\) and a \\(y\\). We plot \\(y\\) against \\(x\\).\n\n\\(y\\) is the response, \\(x\\) is the explanatory variable.\n\nWe’re looking to see if it’s linear. Linear models are something we know how to deal with!\n\nDeviations from linearity are noteworthy.\nOutliers are noteworthy.\n\nWe can incorporate more information in a scatterplot, especially categorical variables."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#summary-statistics-for-two-continuous-variables",
    "href": "L03-Scatterplots_Correlation.html#summary-statistics-for-two-continuous-variables",
    "title": "3  Scatterplots and Correlation",
    "section": "3.3 Summary Statistics for Two Continuous Variables",
    "text": "3.3 Summary Statistics for Two Continuous Variables\n\nMeasuring Strength of Linearity\n\n\nFrom plots, we can sorta see that one looks more linear than another.\nIt would be splendid if we could have a way to quantify this…\n\n\n\n\n\n\n\n\n\nFrom this point on, we’re focusing on linear relationships. The plots above both demonstrate the same linear relationship, but with different “strength”s. Let’s measure that!\n\n\n\nThe correlation coefficient \\(r\\)\nRecall the formula for the variance: \\[\ns_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)(x_i - \\bar x)\n\\]\nThe correlation coefficient is defined as: \\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\] where \\(s_x\\) is the s.d. of \\(x\\) and \\(s_y\\) is the s.d. of \\(y\\).\nIt’s like a variance for two variables at once!\n\nThis explanation might not stick for those of you who aren’t a fan of formulas, but I think this demonstrates an important aspect of the correlation coefficient. The formula for the standard deviation includes \\((x_i - \\bar x)(x_i - \\bar x)\\). If we replaced one of those with \\(y\\), we’d get \\((x_i - \\bar x)(y_i - \\bar y)\\), which is one step closer to the correlation coefficient. In other words, the correlation is a measure of how two (quantitative) variables vary together! Correlation is an extension of variance!\nLet’s try another approach. \\(x\\) has variance. \\(y\\) has variance. They also have variance with each other. This is measured by the correlation!\nIf neither of these explanations make sense, don’t worry! We’ll see plenty of correlations and get an intuition for how correlations are different with different data.\n\n\n\nThe range of \\(r\\)\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\n\\(s_x\\) and \\(s_y\\) are positive\n\\(s_x &gt; \\sum_{i=1}^n(x_i - \\bar x)\\), similar for \\(s_y\\)\n\nThe correlation coefficient cannot be larger than 1\n\n\\(x_i - \\bar x\\) can be negative (same with \\((y_i-\\bar y)\\)).\n\nTogether, this means that the correlation coefficient can be anything from -1 to 1, with 0 representing no correlation and -1 and 1 representing perfect correlation.\n\nThe fact that the correlation can be negative is important. A correlation coefficient of -1 looks like a perfect downward slope. It’s still a strong relationship. In other words, the relationship is stronger if \\(r\\) is further away from 01.\n\n\n\nInterpreting correlation\n\n1 and -1 are perfect correlation.\n0.8 is a strong correlation (depending on context)\n\nPhysics: 0.8 is very very weak.\nSocial science: 0.8 is very very strong.\n\n\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Apps/ScatterCorr\")\n\n\nThe app above shows data that start uncorrelated, then are slowly transformed into perfect correlation. If you have R installed on your computer it should run just fine (you may need to run install.packages(\"shiny\") for the shiny package, and possibly install.packages(\"ggplot2\") if you haven’t already).\nFor more examples (and more info on the correlation coefficient in general), see the OpenIntro Textbook!\n\n\n\nComments on the correlation\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\nThe order of \\(x\\) and \\(y\\) can be switched\n\n2 times 3 is the same as 3 times 2.\n\nSince we’re subtracting the mean and dividing by the s.d., the units don’t matter!\n\nSwitching from kg to lbs has no effect on the correlation.\n\n\\(r&gt;0\\) means the line goes up. \\(r &lt; 0\\) means the line goes down.\nQuantitative only\nLinear only\nNot robust to outliers.\n\n\nLet’s explore some of these ideas with code!\n\nplot(y1 ~ x1, data = anscombe)\n\n\n\n\nIt looks relatively linear. Take a moment to think of how correlated these two variables are, and assign it a value between 0 and 1. This is how you would guess the correlation coefficient\nOn exams, you will be expected to differentiate between “not correlated” (about 0), “slightly correlated” (0.2 to 0.4), “very correlated” (0.6 to 0.8), and “near perfect correlation (almost exactly 1)”, or the negatives of these values; you won’t need to guess whether the correlation is 0.55 or 0.6.\nIn R, we calculate the \\(r\\) with the cor() function.\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nDoes this number make sense to you? It seems fairly high to me, but with small amounts of data it’s not that surprising. Think of it this way: if you removed a quarter of the data at random, would you still be able to see the pattern? If so, then it’s probably “very correlated”!\nThe first point states that the order doesn’t matter:\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nThe units don’t matter:\n\ncor(anscombe$y1*5 + 1, anscombe$x1)\n\n[1] 0.8164205\n\n\nHowever, it does matter if we do a non-linear transformation, such as squaring the values. The correlation is a measure of linear association, so making things non-linear will affect it.\n\nplot(y1^2 ~ x1, data = anscombe)\n\n\n\ncor(anscombe$x1, anscombe$y1^2)\n\n[1] 0.7992029\n\n\nFor these data, squaring didn’t have much of an effect (as we can see in the plot), but we still saw a change in \\(r\\)! Notice that a unit change had absolutely no effect on \\(r\\). In general, we either expect things to be exactly the same or they can be completely different; very few things are “almost equal” in the general case (they may be almost equal with one set of data, but that means nothing for completely different sets of data).\n\n\n\n\\(r\\) measures linear correlation\n\n\nEnzymatic activity is known to be affected by temperature. A study examined the activity rate (in micromoles per second, μmol/s) of the digestive enzyme acid phosphatase in vitro at varying temperatures (measured in kelvins, K). The findings are displayed in the following table.\n\nDescribe the relationship\nExplain why it doesn’t make sense to describe this as “positively associated” or “negatively associated”.\nIs this a strong or a weak relationship? Explain.\n\n\n\n\n\n\n\n\nSolution\n\n\nThe relationship increases with an upward curve from temperatures of 300K to 340K, when it turns downward sharply and decreases to 355K.\nThe association is different for different X values. This is not a linear relationship, which means we have to do extra work to make sure that we cover all the non-linearities.\nThis is a very strong relationship. The pattern clearly passes the facial impact test that we discussed before. It is far from a linear relationship, but it’s clearly noticable.\n\n\n\n\n\n\nAgain, always plot your data!!!\n\n\nAll of the plots in the Anscombe quartet have the same correlation coefficient.\n\\(r\\) is a measure of linear association - if it’s not linear, \\(r\\) can’t be interpreted!!!\n\n\n\n\n\n\n\n\n\nIt’s important to note that \\(r\\) can always be calculated for numeric data. If we had student numbers as well as a categorical variable that used 0 to represent black, 1 to represent asian, etc., then we could technically calculate the correlation coefficient. This would be utterly meaningless!!!!!\n\n\n\nExample: Penguins\n\n\n\n\n\n\nThis is an example of something called Simpson’s Paradox: If we don’t account for the sub-groups, we get the opposite affect! As we can see in the plot, if we have all the groups together than it looks like a negative correlation (plot on the left), but once we separate groups each individual group has a positive correlation (plots on the right). In general, the conclusion that incorporates the most information is probably closest to the truth.\n\n\n\nCorrelation Summary\n\n\\(r\\) is a measure of linear association\n\nI’ve said it plenty, I’ll say it again: \\(r\\) does not apply to non-linear patterns!\nAlways plot your data before calculating \\(r\\).\n\n\\(r\\) is like a measure of how two variables vary together.\n\nFormula is similar to the variance formula!\n\n\\(r\\) is a number between -1 and 1, with 0 meaning no correlation and 1 or -1 meaning perfect correlation.\n\nA negative \\(r\\) means a negative relationship (i.e. a line that goes down).\n\nEverything on the “Comments” slide is fair game for test questions.\n\n\nExercises:\n\nThe following code will draw a plot and calculate the correlation coefficient. Currently, it’s doing this for the column mpg (response) versus the column wt (“weight”, explanatory) in the mtcars data which is built in to R.\n\nRe-run the code, but replace wt with disp (engine displacement), hp (horsepower), drat (rear axle ratio, although I couldn’t explain this further), and qsec (quarter mile time, in seconds). Comment on the apparent pattern and the magnitude of the correlation.\nChange wt tocyl, the number of cylinders. What do you notice about the plot, and how does this affect your interpretation of the correlation between mpg and cyl? Explain why cyl might be better incorporated as a categorical variable, even though it is indeed numeric.\nRepeat part (b) for am, which is “0” for automatic transmission and “1” for manual transmission.\n\n\n\nplot(mpg ~ wt, data = mtcars)\n\n\n\ncor(mtcars$mpg, mtcars$wt)\n\n[1] -0.8676594\n\n\n\nThe following figure comes from the article “Shared neural representations and temporal segmentation of political content predict ideological similarity” by De Brujin et al., published in 2023 (link to aricle here). The star on the plot indicates that they have found a statistically significant relationship (more on this next week). Is this a strong correlation?\n\n\n\nThe following figure comes from the article “Effect on Blood Pressure of Daily Lemon Ingestion and Walking” by Kato et al., published in 2013 (link to article here). Comment on the shape of this relationship. Recall how we described a “strong” shape as a shape that remains even if some of the data points were removed.\n\n\nExercises from OpenIntro Biotatistics textbook\nQuestions 1.35, 1.36, 1.37.\nFor further R practice and case studies, see the labs page for the OpenIntro textbook."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#crowdsourced-questions",
    "href": "L03-Scatterplots_Correlation.html#crowdsourced-questions",
    "title": "3  Scatterplots and Correlation",
    "section": "3.4 Crowdsourced Questions",
    "text": "3.4 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nConsider a study investigating the relationship between the amount of time students spend studying for an exam (in hours) and their resulting exam scores (percentage). The study gathers data from 100 students, measures the two variables for each student, and plots these data points on a scatterplot. Which of the following statements best aligns with the principles of interpreting scatterplots and understanding relationships between variables as discussed in the course notes?\n\nIf the scatterplot shows a clear upward trend, it proves that spending more time studying causes students to achieve higher exam scores.\nThe study identifies the amount of time spent studying as the response variable and the exam score as the explanatory variable, predicting that higher exam scores explain why students spend more time studying.\nThe scatterplot can help visualize the relationship between the two variables, but further analysis is required to determine if one variable causes changes in the other.\nA linear pattern in the scatterplot indicates that every additional hour of study leads to a uniform increase in exam scores for all students.\n\n\n\n\nSolution\n\nC is the correct answer. This option correctly acknowledges that while scatterplots are instrumental in visualizing the relationship between two variables (the amount of time spent studying and exam scores in this case), they alone cannot confirm causality. This aligns with the course’s emphasis on the distinction between correlation and causation and the role of scatterplots in identifying patterns rather than proving causation. Options A, B, and D either incorrectly assume causation from correlation, misidentify explanatory and response variables, or oversimplify the interpretation of a linear pattern, respectively.\n\n\n\nIn an agricultural study, researchers are investigating the impact of different amounts of fertilizer (in kilograms) applied to tomato plants on the yield of tomatoes (in kilograms). Which of the following correctly identifies the explanatory and response variables?\n\nExplanatory variable: Yield of tomatoes; Response variable: Amount of fertilizer\nExplanatory variable: Amount of fertilizer; Response variable: Yield of tomatoes\nBoth the amount of fertilizer and the yield of tomatoes are explanatory variables since they influence each other.\nBoth the amount of fertilizer and the yield of tomatoes are response variables due to external factors like sunlight and water.\n\n\n\n\nSolution\n\nB is the correct answer. The amount of fertilizer is the explanatory variable because it is the variable being manipulated or controlled by the researchers to observe its effect on the yield of tomatoes, which is the response variable.\n\n\n\nA study plots the average daily temperature against the total daily sales of ice cream over a summer in a coastal city. The scatterplot shows a clear upward trend. What does this trend indicate about the relationship between these two variables?\n\nAs the average daily temperature increases, the total daily sales of ice cream decrease.\nThere is no relationship between the average daily temperature and the total daily sales of ice cream.\nAs the average daily temperature increases, the total daily sales of ice cream also increase.\nThe increase in average daily temperature causes an increase in the total daily sales of ice cream.\n\n\n\n\nSolution\n\nC is the correct answer. The upward trend in the scatterplot indicates that there is a positive relationship between the average daily temperature and the total daily sales of ice cream—meaning, as one increases, so does the other. However, it is important to note that this does not imply causation (which option D incorrectly suggests).\n\n\n\nIn a study examining the relationship between hours spent on physical activity per week and overall quality of sleep, a correlation coefficient of -0.3 is found. Which of the following statements best interprets this finding?\n\nThere is a strong negative relationship between hours spent on physical activity and quality of sleep.\nThere is a weak negative relationship between hours spent on physical activity and quality of sleep, suggesting that as physical activity increases, quality of sleep slightly decreases.\nThere is a weak positive relationship between hours spent on physical activity and quality of sleep.\nThe correlation coefficient indicates that increased physical activity causes poorer quality of sleep.\n\n\n\n\nSolution\n\nB is the correct answer. A correlation coefficient of -0.3 indicates a weak negative relationship between the two variables. This means that as one variable increases (hours spent on physical activity), the other variable (quality of sleep) slightly decreases. However, it’s important to remember that correlation does not imply causation, making option D incorrect."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#footnotes",
    "href": "L03-Scatterplots_Correlation.html#footnotes",
    "title": "3  Scatterplots and Correlation",
    "section": "",
    "text": "It is not true that a “larger” \\(r\\) means stronger relationship!↩︎"
  },
  {
    "objectID": "L04-Regression.html",
    "href": "L04-Regression.html",
    "title": "4  Regression",
    "section": "",
    "text": "5 Exercises\n1 Suppose \\(r = 0.3\\). What are the possible values for the slope? a. \\(b &gt; 0\\) b. \\(b \\ge 0\\) c. The slope could still be any value."
  },
  {
    "objectID": "L04-Regression.html#introduction",
    "href": "L04-Regression.html#introduction",
    "title": "4  Regression",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThese notes are based on Chapter 6.1 to 6.3 in OpenIntro Biostats.\nIn linear modelling, we have a collection of pairs \\(x_i\\) and \\(y_i\\). We think that there’s some sort of relationship between \\(x\\) and \\(y\\), and we think that a line is an adequate way to characterize that relationship1.\nJust like we assume that there’s a “true” population mean, there is also a “true” slope and intercept for the line that characterizes the relationship between \\(x\\) and \\(y\\). In the plot below, the green line represents the “true” relationship between \\(x\\) and \\(y\\), and the data are random values above and below that line2.\n\n\n\n\n\nIn high school, you may have learned a line as \\(y = mx + b\\). In statistics, we often use latin letters (a, b, c, d, …) for estimates and greek letters (\\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and other lower case versions of letters you’ve seen on frat/sorority houses) for population parameters3. The population line is labelled:\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\n\n\\(\\alpha\\) is the intercept.\n\\(\\beta\\) is the slope.\n\nA 1 unit increase in \\(x\\) corresponds to a \\(\\beta\\) increase in \\(y\\).\n\n\\(\\epsilon_i\\) is random noise (\\(N(0,\\sigma)\\), although you’re not expected to understand this notation yet).\n\nAgain, we think of \\(x\\) as being fixed. The random noise is above and below the line, not side to side.\n\nThe formula implies that \\(y_i\\) is centered at \\(\\alpha + \\beta x_i\\) but randomly varies above and below the line with variance \\(\\sigma^2\\).\n\nThe word “regression” means to go backward. I like to think that we are “going backward” to the population numbers from the sample values4. Any situation where you are estimating a population parameter is technically a regression, but this terminology is not useful for this class.5\nTo regress, we estimate the parameters using sample statistics. \\(a\\) is the estimate for \\(\\alpha\\), \\(b\\) for \\(\\beta\\), and \\(e\\) for \\(\\epsilon\\). In order to do find these sample statistics, we minimize the squared error between the line and the data:\n\\[e_i^2 = (y_i - a - b x_i)^2\\]\nIn other words, we find \\(a\\) (for \\(\\alpha\\)) and \\(b\\) (for \\(\\beta\\)) that make the sum of the squared errors \\(e_i\\) as small as possible. We use the squared errors for the same reason we use squared deviations in the forumla for the variance: so that positive and negative values do not cancel out6.\nThe estimates \\(a\\) and \\(b\\) are as follows:\n\\[\\begin{align*}\nb &= rs_y/s_x\\\\\na &= \\bar y - b\\bar x\n\\end{align*}\\]\nThese are called the least squares estimates7. The equation for \\(b\\) is especially important!\nIn R, these can be calculated as follows. The mtcars data set is a collection of measurements made on various cars. In this example, we’ll regress the fuel efficiency (in miles per gallon, or mpg) against the weight of the car.\n\n## Load a built-in data set\ndata(mtcars) \n\n## Define which variables are x and y.\n## This isn't necessary, but helps with teaching\nx &lt;- mtcars$wt\ny &lt;- mtcars$mpg\n\n## Calculate the estimates by hand\nb &lt;- cor(x, y) * sd(y) / sd(x)\na &lt;- mean(y) - b * mean(x)\n\n## Print the estimates \nc(a, b)\n\n[1] 37.285126 -5.344472\n\n## Use the built-in functions\nsummary(lm(y ~ x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nx            -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom this line, we can make predictions about new points by simply plugging in the \\(x\\) value. For example, let’s say we wanted to guess the mpg of a car that weighs 3,000 lbs. In the data, the units for weight are 1000 lbs, so this means plugging a value of wt=3 into the data.\n\na + b * 3\n\n[1] 21.25171\n\n\nSo we would guess that a 3 ton car would have a fuel efficiency of 21.25 miles per gallon. Let’s look at this on a plot:\n\nplot(y ~ x)\npoints(3, a + b * 3, col = \"red\", pch = 16)\n\n\n\n\nIt looks like this is somewhere around where we would expect.\nIf we repeat this for every possible \\(x\\) value, we get the regression line below:\n\nplot(y ~ x)\npoints(3, a + b*3, col = \"red\", pch = 16)\n## abline adds a line with slope b and intercept a to a plot.\nabline(a = a, b = b, col = \"red\")\n\n\n\n\nWe cal also see the values of \\(e\\), the residuals.\n\ne &lt;- y - (a + b*x) ## Observed minus expected\nplot(e ~ x, main = \"Plot of the Residuals\")\n## abline can also draw a line with slope 0 (horizontal)\nabline(h = 0, col = \"grey\")"
  },
  {
    "objectID": "L04-Regression.html#regression-facts",
    "href": "L04-Regression.html#regression-facts",
    "title": "4  Regression",
    "section": "4.2 Regression Facts",
    "text": "4.2 Regression Facts\nHere are some facts about the least squares regression line:\n\nThe point \\((\\bar x, \\bar y)\\) is always on the line.\n\nLeast squares regression can be seen as putting a line through \\((\\bar x, \\bar y)\\) and rotating it until the squared error is the smallest.\n\n\\(s_y\\ge 0\\) and \\(s_x\\ge 0\\), so whenever \\(r &gt; 0\\), we know that \\(b &gt; 0\\).\n\nThe slope has the same sign as the correlation. Otherwise, the slope could be pretty much any number, regardless of the correlation.\nIf \\(r = 0\\), then \\(b = 0\\), and vice versa.\nOther than the sign and the special case of \\(r=0\\), there is no way to tell the value of \\(r\\) if all you know is \\(b\\).\n\nFor \\(r\\), the distinction between \\(y\\) and \\(x\\) doesn’t matter.\n\nFor the regression line, it absolutely matters!\n\nThe sum of the errors is 0.\n\n\n## The prediction at mean(x) is equal to mean(y)\n## In other words, (mean(x), mean(y)) is a point on the line\na + b * mean(x)\n\n[1] 20.09062\n\nmean(y)\n\n[1] 20.09062\n\n## Correlation doesn't care about order\ncor(x, y)\n\n[1] -0.8676594\n\ncor(y, x)\n\n[1] -0.8676594\n\n## Theoretically 0, but computers aren't perfectly precise\n## Note: e-14 refers to 10^-14, or 14 zeroes before the first digit\n    # So, pretty close to 0.\nsum(e) \n\n[1] 1.065814e-14"
  },
  {
    "objectID": "L04-Regression.html#percent-of-variation-explained",
    "href": "L04-Regression.html#percent-of-variation-explained",
    "title": "4  Regression",
    "section": "4.3 Percent of Variation Explained",
    "text": "4.3 Percent of Variation Explained\nBecause of some mathematical magic, \\(r^2\\), the squared value of \\(r\\), can be interpreted as:\n\nThe percent of variation in \\(y\\) that can be “explained” by the linear model.8\n\nThe value of \\(r^2\\) can be calculated as: \\[\nR^2 = r^2 \\approx \\frac{\\text{Variance of the predicted }y\\text{-values}}{\\text{Variance of the observed }y\\text{-values}}\n\\]\nI’ll explain this in steps. The first plot below shows just the values in \\(y\\). This collection of values has a own mean and variance.\nThe second plot shows the change in variance that the line “explains”. Instead of deviations above and below the mean, the variance can now be characterized as the deviations above and below the regression line. This variance will always be lower than the variance of \\(y\\) without incorporating \\(x\\)9.\nThe third plot shows where this variance went. The line itself has variance; there is deviation in the line above and below the mean of \\(y\\). This is the variance that gets explained by incorporating \\(x\\)! If you consider one of the points in \\(y\\), say \\(y_1\\), the distance between \\(y_1\\) and \\(\\bar y\\) can be split up into the difference between \\(\\bar y\\) and the regression line plus the distance between the regression line and \\(y_1\\).\n\n\n\n\n\nThe rest of the variance is left unexplianed. No regression will ever be perfect unless we are studing a very very simple .\nTo see this a different way, consider what happens when \\(r = 0\\)10. This will just be a horizontal line, and none of the variance is explained. On the other had, if \\(r = 1\\) then all of the points will be exactly on the line. All of the variance in \\(y\\) has been explained by the regression against \\(x\\) - there’s no variance left to be explained!11\nNotice how the R output includes"
  },
  {
    "objectID": "L04-Regression.html#extensions-and-cautions",
    "href": "L04-Regression.html#extensions-and-cautions",
    "title": "4  Regression",
    "section": "4.4 Extensions and Cautions",
    "text": "4.4 Extensions and Cautions\n\nPrediction\nFor a new \\(x\\) value, \\[y = a + bx\\] is the predicted value of \\(y\\). That is, if we have an \\(x\\) value, we can plug it into the equation and find out what value of \\(y\\) we would expect.\nNote: There is still variance around this prediction! Our “expected” value will never be exactly equal to the truth - The value of \\(y\\) at a given value of \\(x\\) follows a normal distribution12, and the probability of a single point is 0!\n\n\nExtrapolation\nExtrapolation is what happens when prediction goes wrong. In particular, it’s what happens when we try to make a prediction at an \\(x\\) value where we don’t have any data. Usually this means we’re predicting an \\(x\\) value far above or far below the range of our data, but it can also happen if there’s a gap in the middle of our data.\nIn the plot below, the black dots are the original data, and we’re trying to predict a new value at \\(x = 25\\). The red line is the true model that I generated the data from. The black line represents a linear model. This model fits the original data quite well13, but predictions are completely inappropriate for values outside the data.\n\n\n\n\n\n\n\nLurking Variables\nThe black line in the plot below represents a regression where all of the data was lumped together. As we can see, this line does not seem to fit the data well. There is a hidden relationship in the the data - the green points and the red points should be considered separately14.\n\n\n\n\n\nA more serious consequence of a lurking variable has shown up before in the Palmer penguins data. In that example, the lurking variable actually reversed the correlation - if we lumped the groups together we got a negative correlation (and therefore negative slope), but if we looked at the groups individually we got positive associations in all of the groups! This is called Simpson’s Paradox, and basically means that we have to be very careful about interpreting correlations!"
  },
  {
    "objectID": "L04-Regression.html#crowdsourced-questions",
    "href": "L04-Regression.html#crowdsourced-questions",
    "title": "4  Regression",
    "section": "5.1 Crowdsourced Questions",
    "text": "5.1 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nSuppose you have collected data on the number of hours studied and the corresponding exam scores for 10 students. using simple linear regression, determine the relationship between the number of hours studied and exam scores.\n\n\n\n\nhours studied (x)\nexam Score (y)\n\n\n\n\n3\n60\n\n\n4\n65\n\n\n6\n75\n\n\n5\n70\n\n\n7\n85\n\n\n8\n80\n\n\n9\n90\n\n\n10\n95\n\n\n12\n100\n\n\n11\n98\n\n\n\n\nWhat are the means of x and y?\nWhat are the standard deviations of x and y?\nWhat is the correlation between x and y?\nUse your answers above to calculate a regression line. Validate with R.\n\n\nhours_studied &lt;- c(3, 4, 6, 5, 7, 8, 9, 10, 12, 11)\nexam_score &lt;- c(60, 65, 75, 70, 85, 80, 90, 95, 100, 98)\n\n\n\nSolution\n\nTry the calculations by hand!\n\n# a)\nmean(hours_studied)\n\n[1] 7.5\n\nmean(exam_score)\n\n[1] 81.8\n\n# b)\nsd(hours_studied)\n\n[1] 3.02765\n\nsd(exam_score)\n\n[1] 14.1091\n\n# c)\ncor(hours_studied, exam_score)\n\n[1] 0.9832055\n\n# d)\n# Regression slope b = r * s_y / s_x\nb &lt;- cor(hours_studied, exam_score) * sd(exam_score) / sd(hours_studied)\nb\n\n[1] 4.581818\n\n# Regression intercept a = ybar - xbar * b\na &lt;- mean(exam_score) - mean(hours_studied) * b\na\n\n[1] 47.43636\n\n# Check with R\nlm(exam_score ~ hours_studied)\n\n\nCall:\nlm(formula = exam_score ~ hours_studied)\n\nCoefficients:\n  (Intercept)  hours_studied  \n       47.436          4.582"
  },
  {
    "objectID": "L04-Regression.html#footnotes",
    "href": "L04-Regression.html#footnotes",
    "title": "4  Regression",
    "section": "",
    "text": "Very few things are actually linear, but lines are fantastic approximations to many things.↩︎\nWe assume that \\(x\\) is fixed, but \\(y\\) has random noise. In other words, \\(x\\) is not a random variable but \\(y\\) is.↩︎\nBecause we think it makes us sound smarter.↩︎\nActually, the word comes from “regressing to the mean”, which comes from how children are closer to average height than their parents - they go back toward the mean. This is not important.↩︎\nI get very annoyed by the term “Regression Analysis” because there are practically infinite different types of regression. Linear regression is just one of them, albeit a a very very very popular one.↩︎\nAlso, because the calculus works out so much better.↩︎\nThere are other ways to estimate these parameters, but they’re outside the scope of this course. For example, we could use the absolute value instead of the squared value. This works just as well for the calculation, but there are a lot of mathematical reasons why the square is nice. Mainly, it has a nice derivative and allows for easy equation manipulation.↩︎\nUsually \\(r^2\\) is labelled \\(R^2\\) for historical reasons. Capitalization matters in math; it’s just coincidence that both lower case and upper case mean the same thing here.↩︎\nExcept when \\(r=0\\), can you explain why?↩︎\nTherefore the slope will also be 0.↩︎\nStatistics is still just the study of variance.↩︎\nOur prediction is just us guessing the mean value of \\(y\\) at different values of \\(x\\).↩︎\nEven though it’s not the true relationship, it’s a reasonable approximation.↩︎\nPossibly as a blocking variable.↩︎"
  },
  {
    "objectID": "L05-Probability.html#defining-probability-with-dice",
    "href": "L05-Probability.html#defining-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.1 Defining Probability with Dice",
    "text": "5.1 Defining Probability with Dice\nI find that the easiest way is to build this up by examples. Let’s start with rolling a dice.3 Let’s say you rolled the dice, and you got a 3. This is called a simple event. The collection of all simple events is called the Sample Space, which in this case is \\(\\mathcal S\\)={1,2,3,4,5,6}.\nNow, suppose that you’re about to roll a dice. You might be curious about whether it’s a 1, 2, 3, 4, 5, or a 6, but you might only care about the event that the outcome is even. Since there are multiple simple events that make up this event, it is called a compound event.\nThere’s no way for you to know what’s going to happen, but you know all of the possibilities and you know how likely they all are.4 This is called a Probability Model: The sample space along with the probability of all of the events.\nI said “the probability of all events”, but this is more complicated than it may seem and requires some explanation. For something like rolling a dice, you only need to know the probability of each simple event. Compound events, like the probability that the outcome is even, can be determined from these simple events.\nSuppose you’re playing a game where, if the outcome of the dice is less than a certain cutoff value you get to roll again (e.g., your character has a special ability that allows re-rolling of dice, but the re-roll condition depends on the situation). You know the probability of all of the simple events, but you need to know the cutoff value to actually compute any probabilities. Without the cutoff value, you cannot define the probability model.\nFor a dice, the probability model is simply: Each number has a probability of 1 in 6. But what does this mean? There are two perspectives on what a “probability” is: The Frequentist approach and the Bayesian approach. In this class we’re only going to learn the Frequentist definition of probability, but if you’re interested in learning more I’m happy to talk.5\nProbability (Frequentist Definition): The long run frequency of observing an event. In other words, it’s the number of times an event is observed divided by the number of trials after doing a near-infinite number of trials.\nFor the dice, if we rolled the dice 60 times, we would expect 10 of those rolls to be a 1, 10 of them to be a 2, etc. Due to randomness, we won’t get exactly that, but this is what we would expect. If we rolled 600 dice, we would expect 100 to be 1, etc. As we roll more dice, we get closer to the proportion of 1/6. The plot below this demonstrates this - it is the number of times that a dice was 1 divided by the number of trials, with the number of trials being increased. Notice how it takes a while for the “empirical” probability to reach the theoretical probably; as the number of trials approaches infinity, the proportion of rolls that showed a 1 will approach 1/6."
  },
  {
    "objectID": "L05-Probability.html#calculating-probability-with-dice",
    "href": "L05-Probability.html#calculating-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.2 Calculating Probability with Dice",
    "text": "5.2 Calculating Probability with Dice\nFirst, let’s introduce some notation. I will use P(x) to mean “The probability of x”. In some cases, the context will be clear, such as:\n\n“The probability of rolling a 1” = P(1)\n“The probability of rolling an even number” = P(even)\n“The probability of not rolling a 1” = 1 - P(1)6\n\nFor this section, we’ll assume that P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=1/6.7\n\n“Or”\nWhat’s the probability that we roll an even number? The even numbers are 2, 4, and 6, so what we’re really asking is “What’s the probability that we roll a 2, 4, or a 6?” In this case, the probability is P(2 or 4 or 6) = P(2)+P(4)+P(6) = 1/6 + 1/6 + 1/6 = 3/6 = 0.5.\nWe also could have figured out this probability by noting that half of the values are even, so a probability of 0.5 makes sense. It’s a good thing when our intuition matches our answer, as we’ll see next.\nLet’s consider the probability that the dice is even^ (which we will denote P(even)) or it’s strictly larger than 3 (denoted P(&gt;3)). This means the dice is either 2, 4, or 6, or it’s 4, 5, 6. Since there are 4 different numbers (2, 4, 5, and 6) that would match the criteria, the probability is 4/6. Let’s use our “or” rules to verify this!\nThe probability that the dice is even is 1/2. The probabilty that the dice is larger than 3 is also 1/2. So, obviously, P(even or &gt;3) = P(even) + P(&gt;3) = 1/2 + 1/2 = 1.\nWait.\nThat can’t be right.\nI think you may be able to see what went wrong. The P(even) = P(2) + P(4) + P(6), and P(&gt;3) = P(4) + P(5) + P(6). When we did P(even) + P(&gt;3), we added P(4) and P(6) twice! To get the right answer, we need to fix this. Since we added them twice, we must subtract them once. This brings us to…\n\n\nThe Addition Rule for “or”\nFor any two events A and B,8 the Addition Rule states:\n\\[\\begin{align}P(A\\; or\\; B) = P(A) + P(B) - P(A\\; and\\; B).\\end{align}\\]\nFirst, note that the probability of both events is P(Even and &gt;3) = P(4) + P(6), since 4 and 6 are both even and larger than 3.\n\\[\\begin{align*}\nP(Even\\; or\\; &gt;3) & = P(Even) + P(&gt;3) - P(Even\\; and\\; &gt;3)\\\\\n& = [P(2) + P(4) + P(6)] + [P(4) + P(5) + P(6)] - [P(4) + P(6)]\\\\\n& =  P(2) + P(4) + P(5) + P(6)\\\\\n& = 1/6 + 1/6 + 1/6 + 1/6\\\\\n& = 4/6\n\\end{align*}\\] \n\n\n“and”: Part 1\nThe word “and” came up in the addition rule, and so I should give a good definition of “and”. When we talk about events A and B, P(A and B) refers to the probability that they both happen together. It’s most helpful to see this as a Venn diagram:\n\nP(A) is the area of the circle labelled A, P(B) is the area of the circle labelled B, and P(A and B) is the area of the overlap between these two circles. P(A or B) is the total shaded area, including the yellow-green, green, and dark green.\nYou can see the Addition Rule at work here. If you add the area of A (which includes P(A and B)) to the area of B (which also includes P(A and B)), you’ve added P(A and B) twice!\nThere are two formulas for P(A and B). The first one is found by rearranging the formula for P(A or B):\n\\[\\begin{align*}\n\\small P(A\\; or\\; B) &\\small = P(A) + P(B) - P(A \\;and\\; B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(A) + P(B) - P(A \\;or\\; B)\n\\end{align*}\\]\nWhen in doubt, just remember P(A_B) = P(A) + P(B) - P(A_B), then put “or” in one blank and “and” in the other.\nThis formula won’t always get you to the solution, though. There will be many times where neither “and” nor “or” will be obvious, and we’ll need to do some more work to get them. We have special formulas for “and”, so we’ll usually try to figure out the “and” and then use it to figure out the “or”9.\n\n\n“given”: Conditional Probabilities\nA condition is something that must happen before you can proceed. A conditional probability is a probability that requires something else to happen, and usually involves a more complicated setup.\nLet’s look at another scenario. Let’s say I told you that the number on the dice was larger than 3. What’s the probability that the number on the dice is a 4? Intuiutively, it’s 1/3, since there are 3 possible numbers. Our notation fails us here, P(4) denotes the probability that the dice is a 4, which we already determined was 1/6. We can’t use P(4) for two things, so we need to add some notation.\nIn this case, the solution is to use a vertical bar, “|”, which is pronounced “given”. We write “P(dice is 4 | dice is greater than 3) = 1/3”,10 which is read as “The probability that the dice is 4, given that the dice is larger than 3.”\nA very important thing happened here: when we used a conditional probability (“given that”), we restricted the sample space. When we “condition” on an event, it means that we’re only looking at cases where that event happened. “The probability that the dice is 4, given that the dice is larger than 3” is another way of saying that we’re only considering events where the dice roll is greater than 3; we don’t care about 1, 2, or 3.\nWe defined “probability” as the total number of events divided by the total number of trials. For conditional probabilities, this means that we’re only looking at some of the trials.\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nThis formula is incorrect: “The number of ways that a dice can be 4” depends on the condition. For instance, the number of ways that a dice can be 2 is 0 since we’re told it’s larger than 3. We are actually looking at the number of ways that the dice can be both 4 and greater than 3. Let’s incorporate this information:\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;\\;and\\;&gt;3}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nFor any two events A and B, conditional probabilities are defined as follows:11\n\\[\\begin{align*}\n\\small P(A | B) = \\frac{P(A\\;and\\; B)}{P(B)}\n\\end{align*}\\]\nThe equation above is a definition. It’s not the result of something else, it’s the way we define conditional probability. Rearranging it, though, gives us an important result.\n\n\n“and” Part 2: The Multiplication Rule\nFor any two events A and B, the Multiplication Rule states:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) = P(A|B)P(B)\n\\end{align*}\\]\nNote that P(B|A) = P(A and B)/P(A), so the multiplication rule can be extended:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) &\\small = P(A|B)P(B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(B|A)P(A)\n\\end{align*}\\]\nIn other words, you can write it either way as long as the event that comes after the “|” also appears on it’s own.\nLet’s use this to answer the following question: What’s the probability that a dice is larger than 3 and even? By intuition, this should be 2/6 since there are two cases where both are true, but let’s verify with math!\nFirst, recall that P(&gt;3) and P(even) are both 1/2.\nP(&gt;3 | even) means that we’re look at the number of dice rolls that are larger than 3, but we’re only considering even dice rolls. We have 3 total dice rolls that are even, and 2 of those are larger than 3, so this probability is 2/3. Using the multiplication rule, P(&gt;3 and even) = P(&gt;3 | even)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\nThe other way works out the same. Given that the roll is larger than 3, there are 2 even rolls, which means that P(even | &gt;3) = 2/3. P(&gt;3 and even) = P(even | &gt;3)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\n\n\nSpecial Cases: Independent or Disjoint\n\n\nDisjoint, a.k.a. Mutually Exclusive\nDisjoint events, also called mutually exclusive events, are events that cannot occur together. For example, the event that you roll a 4 and it’s also a 3. This simply does not work, so the probability is 0.\nMore formally, A and B are disjoint if P(A and B) = 0.\n\n\nThe Addition Rule for Disjoint Events\nIf A and B are disjoint, then P(A or B) = P(A) + P(B).12\nThis is actually why we were able to say P(even) = P(2 or 4 or 6) = P(2) + P(4) + P(6) = 3/6: the events “2”, “4”, and “6” are disjoint.\n\n\nIndependent\nTwo events are independent if the knowledge of one event tells you nothing about the other.13 For instance, if I flip two different coins and tell you that the first one was Heads, you still only have a 50/50 chance of guessing the second one.\nNotice the phrasing in the previous sentence: “If I tell you that the first one is heads…” That is, I’m restricting the sample space. Independence is all about conditional probabilities!\nFormally, A and B are independent if P(A | B) = P(A).\nThis adds further insight into conditional probabilities: P(A|B) is how likely A is, given that you know B happened. Knowledge of B changes your guess of the likelihood of A. If it doesn’t change your guess, then they are independent.14\nThe following app demonstrates this concept:\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/indep\")\n\nAnother lesson to take from the app above: Independence doesn’t look special. You can’t just tell that things are independent by looking at them.\n\n\nThe Multiplication Rule for Independent Events\nAny time I see a conditional probability, I immediately write down the formula. For dependence, we are saying:\nP(A|B) = P(A and B)/P(B)\nwhich is the same as\nP(A and B) = P(A|B)P(B)15\nIf two events are independent, then P(A|B)=P(A), therefore:\nP(A and B) \\(\\stackrel{indep}{=}\\) P(A)*P(B)16\nGet this tattood backwards on your forehead so you see it every time you look at yourself in the mirror: P(A and B) is ONLY equal to P(A)*P(B) when A and B are independent!!! Some textbooks start with this rule then move to the general rule, but far too many students start using P(A and B) = P(A)P(B) as if it’s always true. My entire thesis is based on whether you can say two things are independent, so it’s kind of a sore spot for me. DO NOT MIX THIS UP.\nFor example, are the events “even” and “&gt;3” independent? If you know that the dice roll is &gt;3, then there’s a 2/3 chance that it’s even. That is, P(even|&gt;3) = 2/3 \\(\\ne\\) 1/2 = P(even), so it’s not independent.\nAlternatively, we can calcuate P(even and &gt;3) = 2/3, but P(even)*P(&gt;3) = (1/2)*(1/2) = 1/4. Since 2/3 \\(\\ne\\) 1/4, these events are not independent.\n\n\nDisjoint means Dependent\nIndependence can be defined as “if you know that one event happened, you have no knowledge of the other event.” Disjoint can be defined as “if you know one event happened, you know for sure that the other one did not happen.” If two events are disjoint, they must be dependent. In fact, knowledge of one event means that you for sure know about the other - the exact opposite of independence!\n… except when one event is impossible. For instance, P(even and 7) = 0 since there are no dice rolls that are both even and 7, but this is also equal to P(even)*P(7) = 0 since there are no dice rolls that are 7."
  },
  {
    "objectID": "L05-Probability.html#word-problems",
    "href": "L05-Probability.html#word-problems",
    "title": "5  Probability Background",
    "section": "5.3 Word Problems",
    "text": "5.3 Word Problems\nQuestion 10.6 from the textbook:17\nThe National Survey on Drug Use and Health reports that 18.1% of all adults in the United States had a mental illness in 2014. Among adults with a substance use disorder, 39.1% had a mental illness. By comparison, only 16.2% of adults without a substance use disorder had a mental illness. The report also states that 3.3% of American adults had both a mental illness and a substance use disorder. Use the notation MI and SUD for mental illness and substance abuse disorder, respectively.\n\nExpress the four percents cited here as probabilities for a randomly selected American adult. Use proper probability notation.\nObtain the probability P(SUD|MI). Write a sentence reporting this probability in context.\n\nSolutions:\n\nThere are a couple of probabilities:\n\n“18.1% of all adults in the United States had a mental illness”: P(MI) = 18.1\n“Among adults with a substance use disorder, 39.1% had a mental illness.” The part that says “among adults with SUD” means that we’re only looking at people with SUD; we’re restricting the sample space. This is a condition, so our answer must be P(_ | SUD) = __. The blanks can be filled in as P(MI|SUD) = 0.391.\n“16.2% of adults without a substance use disorder had a mental illness.” The part that says “adults without a SUD” is also restricting the sample space, so our probability statement will be P(__ | no SUD) = __. The blanks are filled in as P(MI | no SUD) = 0.162.18\n“3.3% of American adults had both a MI and a SUD”. This clearly states and, so we are looking at P(MI and SUD) = 0.033\n\n\nPart b. is going to take a few steps. Let’s write down all the formulas that might help. Firstly. there’s no “or”, so that probably won’t do it.\n\nWant: P(SUD|MI)\n\nP(SUD|MI) = P(SUD and MI)/P(MI), so we need P(SUD and MI) and P(MI).\n\nHave:\n\nP(MI) = 0.181\nP(MI | SUD) = 0.391\nP(MI | no SUD) = 0.162\nP(MI and SUD) = 0.033\n\n\nBoth P(SUD and MI) and P(MI) are given in the question, so our answer is simply:\n\nP(SUD | MI) = P(SUD and MI)/P(MI) = 0.033/0.181 = 0.1823\n\nTherefore 18.23% of people with mental illness have substance abuse disorder.\nCompare this value to P(MI | SUD) = 0.391. In general, there is no easy relationship between P(A | B) and P(B | A). If you know what P(A | B) is, you can’t really guess at what P(B | A) is; you need a lot more information!"
  },
  {
    "objectID": "L05-Probability.html#two-way-tables",
    "href": "L05-Probability.html#two-way-tables",
    "title": "5  Probability Background",
    "section": "5.4 Two-Way Tables",
    "text": "5.4 Two-Way Tables\nI rigorously collected the following data19 on programming language usage for different disciplines using the most appropriate sampling methods.\n\n\n\n\nStats\nMath\nComp Sci\nTotal\n\n\n\n\nR\n90\n30\n40\n160\n\n\nPython\n10\n60\n100\n170\n\n\nMatLab\n15\n60\n15\n90\n\n\nJulia\n10\n10\n1\n21\n\n\nTotal\n125\n160\n156\n431\n\n\n\nFrom this table, we can calculate marginal and conditional probabilities.\nMarginal probabilities are calculated from the margins, which means that we ignore one of the variables. For example, P(Math) = 160/431 and P(Julia) = 21/431. Both of these proportions are based on the margins - they don’t take the other variable into account.\nConditional probabilities are the same idea as we saw earlier. Again, we are restricting our sample space by conditioning on another variable. For example, P(R | Stats) = 90/125, whereas P(Stats | R) = 90/160. The conditioning event determines which row/column we use. When we condition on Stats, we only look at the column labelled stats - we do not consider any of the other numbers. This is why P(R | Stats) has a numerator of 125, rather than 431.\nYou should be familiar with the following calculations:\n\nP(Stats) = 125/431\nP(Stats | Julia) = 10/21\nP(Matlab | Comp Sci) = ???20\nP(Stats and Julia) = 10/431\nP(Matlab and Stats) = 15/431\nP(Stats or Julia) = P(Stats) + P(Julia) - P(Stats and Julia) = 136/431\nP(Matlab or Stats) = 200\nP(Stats or R) = ???\nP(Stats or Math) = ??\n\nTwo-way tables can also be created in R using the table() function:\n\ndata(mtcars) # It's a very useful dataset\n\ncbind(mtcars$am, mtcars$cyl) # cbind BINDs Columns together\n\n      [,1] [,2]\n [1,]    1    6\n [2,]    1    6\n [3,]    1    4\n [4,]    0    6\n [5,]    0    8\n [6,]    0    6\n [7,]    0    8\n [8,]    0    4\n [9,]    0    4\n[10,]    0    6\n[11,]    0    6\n[12,]    0    8\n[13,]    0    8\n[14,]    0    8\n[15,]    0    8\n[16,]    0    8\n[17,]    0    8\n[18,]    1    4\n[19,]    1    4\n[20,]    1    4\n[21,]    0    4\n[22,]    0    8\n[23,]    0    8\n[24,]    0    8\n[25,]    0    8\n[26,]    1    4\n[27,]    1    4\n[28,]    1    4\n[29,]    1    8\n[30,]    1    6\n[31,]    1    8\n[32,]    1    4\n\ntable(mtcars$am, mtcars$cyl)\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nThe table above is telling us that there were 3 cars that were automatic (0) and had 4 cylinders.\n\n## Note: TRUE == 1, so the sum of a logical vector is the number of TRUEs\n## The \"&\" operator only returns true if BOTH conditions are true, i.e.\n## if mtcars$cyl == 4 AND mtcars$am == 0\nsum(mtcars$cyl == 4 & mtcars$am == 0)\n\n[1] 3"
  },
  {
    "objectID": "L05-Probability.html#self-study-questions",
    "href": "L05-Probability.html#self-study-questions",
    "title": "5  Probability Background",
    "section": "5.5 Self-Study Questions",
    "text": "5.5 Self-Study Questions\n\nExplain why P(A) + P(not A) must be 1.\nIf P(A) = 0.2, P(B) = 0.35,\n\nand P(A or B) = 0.75, find P(A and B).\nand P(A and B) = 0.15, find P(A or B).\nexplain why P(A and B) can only be as large as 0.2.\nexplain why P(A or B) must be at least 0.35.\n\nFor a 6-sided dice, show that the events “even” and “odd” are not independent.\nFor a 6-sided dice, show that the events “even” and “&gt;4” are independent.\nConsider flipping one coin and rolling one dice.\n\nList out all possible events (e.g., H1 for heads and 1, T4 for tails and a 4 on the dice).\nBased on your sample space, argue that P(T1) = 1/12.\nAre the events “coin is tails” and “dice is 1” independent? Give an intuitive and a mathematical reason.\n\nConsider a loaded dice, where the probability of 1, 2, 3, 4, and 5 are all 1/8.\n\nExplain why P(6) must be 3/8.\nWhat is P(even)?\nAre the events “even” and “&lt;3” independent?\n\n\nSolutions to Two-Way Table exercises: 3. 15/156; 8. 195/431; 9. 285/431"
  },
  {
    "objectID": "L05-Probability.html#crowdsourced-questions",
    "href": "L05-Probability.html#crowdsourced-questions",
    "title": "5  Probability Background",
    "section": "5.6 Crowdsourced Questions",
    "text": "5.6 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nWhich statement about disjoint and independent probabilities is true?\n\nEvents with disjoint probabilities never occur simultaneously.\nEvents with independent probabilities are always disjoint.\nIf two events are independent, you know the probability of one based on the other.\nEvents are always independent if they have disjoint probabilities.\n\n\n\n\nSolution\n\n\nEvents with disjoint probabilities never occur simultaneously.\n\nDisjoint events are mutually exclusive, meaning that they cannot occur at the same time as each other. A good example of disjoint events is a coin toss or the roll of a die. On a coin, there are two sides, heads or tails, and when flipped, only one side can be landed on. The same is true with a dice, once rolled, it can only land on one of its six faces. Therefore, when events are disjoint, the probabilities can’t occur together.\nIndependent events on the other hand, are events where one occurrence doesn’t affect the occurrence of another. If we take the roll of a dice for example, and roll the same dice 5 times, the number it lands on can be different every time. Therefore, they are independent because landing on a number on the first roll has no effect on what number will be rolled on the second and so on. *****"
  },
  {
    "objectID": "L05-Probability.html#footnotes",
    "href": "L05-Probability.html#footnotes",
    "title": "5  Probability Background",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThe singular form “die” is dieing out; the dictionary lists “dice” as singular noun, and the singular “dice” is clearer for new English speakers.↩︎\nAssuming there’s nothing unusual about your dice.↩︎\nMost of my work uses the Bayesian definition.↩︎\nThis is called a complement.↩︎\nThe sum of all probabilities must be 1.↩︎\nFor example, A = “Even”, B = “&gt;3”.↩︎\nThis lecture has some of the weirdest sentences.↩︎\nP(4 | &gt;3) just looks too confusing, so I added some words.↩︎\nTo remember this, I like to imagine the vertical bar falling on the the B and pushing it into the denominator.↩︎\nNot an important point: This is a rule, not a result. The General Addition Rule is a result of this rule, not the other way around.↩︎\nThe opposite of independence is dependence.↩︎\nJust like in correlations, dependence does not imply causation!↩︎\nThis equation is always true.↩︎\nThe “\\(indep\\)” over the equals sign is there to specify that this is only true if events are independent.↩︎\nBaldi, B. and DS. Moore. 2018. The Practice of Statistics in the Life Sciences. 4th Edition, W.H. Freeman and Company.↩︎\nThis is a great place to mention: There’s absolutely no reason why P(A|B) + P(A| not B) should add to 1.↩︎\nSource: I made it up.↩︎\nAnswer is at the end.↩︎"
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#introduction",
    "href": "L07-Binomial_Probabilities.html#introduction",
    "title": "6  Binomial Probabilities",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nProbability models are ways of laying out all possible events as well as the probability of each event. For things like coins and dice, everything has the same probability and things work out nicely. In Two-Way tables, we have all the probabilities laid out in front of us. The Binomial distribution is our first foray into a formulaic approach to probabilities.\n\nWith Coins\nIf we flip two coins, the outcomes are {HH, HT, TH, TT} and each of these are equally likely. Instead of looking at each event, what is the probability that there are 0 heads? 1 head? 2 heads?\nFor 0 and 2 heads, there is only 1 possibility, so it must be 1/4 for each. For 1 head, there are 2 possibilities, each with probability 1/4, so the answer is 2*1/4.3\nLet’s flip three coins. The outcomes are {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}, so each outcome has a probability of 1/8. Another way to come to this number is to look at the probability of heads: For HHH, the probability is 0.5*0.5*0.5 since there’s a 50% chance of heads and each coin flip is independent.4\n\n\n\n# Heads\nOutcomes\nProbability\n\n\n\n\n0\nTTT\n1/8\n\n\n1\nTTH, THT, HTT\n1/8+1/8+1/8 = 3/8\n\n\n2\nHHT, HTH, THH\n1/8+1/8+1/8 = 3/8\n\n\n3\nHHH\n1/8\n\n\n\nAlright, let’s do 4 coins. How many ways are there to get, say, 2 heads out of four flips? You can bet that a smart mathemetician has figured out a way to do this without writing them all out again! This is called combinatorics, and includes a lot of things that are not relevant right now. We’ll focus on the choose function. For three coins, “3 choose 1” means “out of 3 options, choose 1 of them”. Sometimes this is shortened to “3C1”. As we saw in the table above, there’s 1 way to choose nothing (no heads), 3 ways to choose 1 thing, 3 ways to choose 2 things, and 1 way to choose 3 things.5 In R:\n\n# This code will **not** be tested on exams or assignments.\n# It's just here to do calculations for us.\nchoose(n = 3, k = 0) # 3C0\n\n[1] 1\n\nchoose(n = 3, k = 1) # 3C1\n\n[1] 3\n\nchoose(n = 3, k = 2) # 3C2\n\n[1] 3\n\nchoose(n = 3, k = 3) # 3C3\n\n[1] 1\n\nchoose(n = 4, k = 2) # 4C2\n\n[1] 6\n\n\nSo for 4 coins, there is 4C2 = 6 ways to get two heads.6 What’s the probability of each of these 6 outcomes? Since there’s a 0.5 chance of heads and a 0.5 chance of tails, there’s a 0.5*0.5*0.5*0.5 = 0.5\\(^4\\) = 0.0625 chance. That means that there’s a 6*0.0625 = 0.375 chance of getting two heads out of four flips.7\nJust to be complete, let’s do this again for 5 coins. We’re already at the point where we need the choose function because there are too many outcomes to write out by hand. Let’s calculate some probabilities with R:\n\n# Again, this code is not on the tests or assignments.\n\n## Probability of 4 heads out of 5 flips:\nchoose(5, 4) * 0.5^5\n\n[1] 0.15625\n\n## Probability of 3 heads out of 5 flips:\nchoose(5, 3) * 0.5^5\n\n[1] 0.3125\n\n\nFor completeness, let’s make sure these all add up to 1:\n\n## I really hope this adds to 1\n(choose(5, 0) * 0.5^5) +\n  (choose(5, 1) * 0.5^5) +\n  (choose(5, 2) * 0.5^5) +\n  (choose(5, 3) * 0.5^5) +\n  (choose(5, 4) * 0.5^5) +\n  (choose(5, 5) * 0.5^5)\n\n[1] 1\n\n\n\n\nWith Dice\nIf I roll two dice, what’s the probability that exactly 1 of them is a 3? One way this can happen is if the first dice is a 3 and the second one is not a 3.\nThe probability that the first dice is a 3 is clearly 1/6. The probability that the second dice is not a 3 is 5/6. Finding the probability that something doesn’t happen can be simplified: either something happens or it doesn’t. The probability has to add to 1, so P(happens) + P(doesn’t happen) = 1. Rearranging this equation, P(doesn’t happen) = 1 - P(happens). So P(not 3) = 1 - P(3). This will come up often in this lecture, so take a moment to explain it to your grandma8.\nFrom this, and knowing that the two dice rolls are independent, we see that the probability is (1/6)*(1 - 1/6).\nWe can also have exactly one 3 if the first dice is not 3 but the second dice is 3. This has the same probability as the other way around: (1 - 1/6)*(1/6).\nNotice how this is the second of two options. Again, we get to use the choose function:\n\n## Probability of exactly one 3 in two dice rolls\nchoose(2, 1) * (1/6) * (1-1/6)\n\n[1] 0.2777778\n\n\nIf we roll 18 dice, what’s the probability that exactly four of them show a 5? Regardless of the order, we have four dice that are 5 and 14 dice that are not 5. The probability of any one of the outcomes is (1/6)\\(^4\\)*(1 - 1/6)\\(^{14}\\) = 0.000060098. That’s pretty unlikely for this exact dice combination of dice rolls! But how many ways are there for this to happen? There are 18C4 = 3060, which is a lot, so there are a lot of opportunities for things with small probabilities. The probability of exactly four 5s out of 18 rolls is 18C4*(1/6)4*(1-1/6)14 = 0.1840. Even though an individual dice roll is unlikely, there are a lot of dice rolls that meet our criteria of four 5s out of 14 rolls!"
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#binomial-probabilities",
    "href": "L07-Binomial_Probabilities.html#binomial-probabilities",
    "title": "6  Binomial Probabilities",
    "section": "6.2 Binomial Probabilities",
    "text": "6.2 Binomial Probabilities\nIn general, if we have \\(n\\) trials and the probability of the event of interest, a.k.a. success, is \\(p\\), then\n\\[\\begin{align*}\nP(x\\text{ successes in }n\\text{ trials}) = nCx*p^x*(1-p)^{n-x}\n\\end{align*}\\]\nFor the dice example, “x successes in n trials” can be interpreted as “four 6s in 18 trials”, where \\(x=4\\), \\(n=18\\), and the “14 rolls that are not four” comes from \\(n-x=14\\).\n\nConfusing (but Useful) Notation\nIn the statement above, we used \\(x\\) to refer to the number of heads. I like this. Let’s keep doing this.\nFor Binomial probabilities, we use the notation:\n\\[\\begin{align*}\nX \\sim B(n,p)\n\\end{align*}\\]\nwhich is read as “the random variable X is distributed as Binomial with n trials and probability of success p”.9 The “\\(\\sim\\)” just means “is distributed as”, which tells us where the probabilities are distributed. This is why \\(nCx*p^x*(1-p)^{n-x}\\) is called the probability distribution function, or pdf.10\nA random variable is just a variable that has a probability distribution,11 such as the number of heads out of 5 flips. Before flipping these coins we have no idea how many heads there will be, but we know the probability of each number. We always use upper case letters for random variables. Once we actually have a value (say, 1 heads), we use lower case. We often use the notation \\(P(X = x)\\) to refer to “the probability that the random variable \\(X\\) will have the specific value of \\(x\\)”. In other words, \\(X\\) is the unknown that could be anything, \\(x\\) is the specific probability that we’re interested in.\nI just want to talk about “distributions” a little bit more. A distribution tells you where the probabilities are. For coins, 50% of the probability is in Heads, 50% is in in tails. When we talk about “is the dice a 3?”, one-sixth of the probability is distributed to the 3 and five-sixths are distributed elsewhere.\nTo summarise, saying that \\(X \\sim B(n,p)\\), or that \\(X\\) is distributed as a Binomial random variable with \\(n\\) trials and probability of success \\(p\\), is the exact same as saying that \\(P(x\\text{ successes in }n\\text{ trials})\\) can be found using the equation \\(nCx*p^x*(1-p)^{n-x}\\). This is what it means to have a probability distribution function.\nTo see all of these probabilities at once, we can plot this as a graph. To reduce coding, let’s look at \\(X\\sim B(3, 0.4)\\):\n\n# We're still not going to have this on tests!\nx &lt;- c(0, 1, 2, 3) # X values\ny &lt;- c(\n    choose(3, 0) * (0.4)^0 * (1 - 0.4)^3,\n    choose(3, 1) * (0.4)^1 * (1 - 0.4)^2,\n    choose(3, 2) * (0.4)^2 * (1 - 0.4)^1,\n    choose(3, 3) * (0.4)^3 * (1 - 0.4)^0\n)\n\n#plot(x,y) # This will plot them, but it looks kinda bad\n## Since X can only be 0, 1, 2, or 3, let's us a bar plot!\nbarplot(y, names = x,\n  main = \"pdf of B(3, 0.4)\", xlab = \"x\",\n  ylab = \"3Cx * p^x * (1-p)^(3-x)\")\n\n\n\n\nNotice how 1 is the most likely value, with 2 being much less likely. This makes sense - if the probability of heads is less than 0.5, we expect that more of the coin flips will be tails! If the probability of “heads” were 0.5, then we would expect 1 and 2 to be equally likely.\n\n\nExamples\n\nSuppose I have a coin that is weighted so that Heads comes up 80% of the time. What is the probability that I get 8 heads in 10 flips?\n\n\nchoose(10, 8) * (0.8)^8 * (1-0.8)^2\n\n[1] 0.3019899\n\n\n\nWhat’s the probability that I get anything other than 10 flips?\n\nTry it yourself!\n\nWhat’s the probability that I get more than 8 heads in 10 flips?\n\nSince the events “9 heads” and “10 heads” are disjoint, we can calculate these individually and add them together.\n\n\n\nchoose(10, 9) * (0.8)^9 * (1-0.8)^1 +\n  choose(10, 10) * (0.8)^10 * (1-0.8)^0\n\n[1] 0.3758096\n\n\n\n\nIn R\nTyping out the whole formula is getting boring. Surely R, a statistical programming language, has a way to do it for me, right? Of course!\n\nchoose(10, 8) * (0.8)^8 * (1 - 0.8)^2\n\n[1] 0.3019899\n\ndbinom(x = 8, size = 10, prob = 0.8) # This will be useful!\n\n[1] 0.3019899\n\n\nThe dbinom() function has exactly the arguments that you would expect. Lower case x is the specific value, size is the number of coin flips, prob is the probability of success. The d stands for “density”, which for our purposes is the same as “distribution”.\nAs a special note, R will take a vector for x. We can find multiple probabilities at once:\n\ndbinom(x = c(8, 9, 10), size = 10, prob = 0.8)\n\n[1] 0.3019899 0.2684355 0.1073742\n\n\nThis allows us to easily plot the pdf:\n\nx &lt;- 0:10 # a vector of the numbers from 0 to 10\n\n## note: x is the name of the object AND the argument,\n## hence why I wrote \"x = x\"\ny &lt;- dbinom(x = x, size = 10, prob = 0.8)\n\nbarplot(height = y, names = x)\n\n\n\n\nThis bar plot shows us a couple of things. We can see that eight heads is the most likely outcome with 30% of all sets of 10 flips having 8 heads, which makes sense because the probability of heads is 80%. Since we’re only flipping 10 times, we can’t have values above 10. Because of this, our plot is left skewed!"
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "href": "L07-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "title": "6  Binomial Probabilities",
    "section": "6.3 Cumulative Binomial Probabilities",
    "text": "6.3 Cumulative Binomial Probabilities\nA cumulative probability is the probability of observing up to \\(x\\) successes in \\(n\\) trials. In other words, this is \\(P(X \\le x)\\): the probability that the random variable \\(X\\) is smaller than or equal to some specific number \\(x\\). This is referred to as the Cumulative Distribution Function, or cdf. It really matters whether it’s \\(P(X\\le x)\\) or \\(P(X&lt; x)\\)!\nWhat’s the probability that we get at most 4 heads in 10 flips? That’s the same as the probability of 0 heads plus the probability of 1 heads plus the probability of 2 heads plus…\n\n## Note: R evaluates the arguments *in order*\n## It expects the arguments in the order of \"x, size, prob\",\n## so it assumes the first argument is x, the second is size,\n## and the third is prob.\ndbinom(x = 0, size = 10, prob = 0.5) +\n  dbinom(1, 10, 0.5) +\n  dbinom(2, 10, 0.5) +\n  dbinom(3, 10, 0.5) +\n  dbinom(4, 10, 0.5)\n\n[1] 0.3769531\n\n\nWhat about the probability of at most 40 heads in 100 flips? Do I have to type all that out?\nNope! We can use the pbinom() function. First, let’s verify it with what we’ve already calculated:\n\npbinom(q = 4, size = 10, prob = 0.5)\n\n[1] 0.3769531\n\n\nNow, let’s find the probability of at most 40 heads in 100 flips:\n\npbinom(40, 100, 0.5)\n\n[1] 0.02844397\n\n\nIt’s surprisingly small! Let’s look at the pdf to see why:\n\nx &lt;- 30:70 # The pdf is REALLY small outside this range\n\n## I'm going to colour the bars where x &lt;= 40\n## Start with a bunch of white bars by REPeating the colour\n## white for as many x values as we have\nmycols &lt;- rep(\"white\", length(x))\n## Next, we change the colour where x &lt;= 40\nmycols[x &lt;= 40] &lt;- \"red\"\n\n## Calculate the distribution function\ny &lt;- dbinom(x, 100, 0.5)\nbarplot(height = y, names = x, col = mycols)\n\n\n\n\nMake sure this number makes sense to you! It’s always good to check the results against your intuition - can you see why this number is so close to 1 (use the plot above to inform your answer).\n\nExamples\nWhat’s the probability of at least 40 heads in 100 flips? Be careful here: it matters whether I ask “at least” or “more than”. The cdf always calculates “less than or equal to”12, and the complement of this is “strictly greater than”.13 If I’m looking for “strictly greater than”, I need to be careful what I use!\nIn this case, P(X \\(\\ge\\) 40) = P(X &gt; 39) = 1 - P(X \\(\\le\\) 39) = 1 - pbinom(39, 100, 0.5)\n\n1 - pbinom(q = 39, size = 100, prob = 0.5)\n\n[1] 0.9823999"
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "href": "L07-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "title": "6  Binomial Probabilities",
    "section": "6.4 Properties of the Binomial Distribution",
    "text": "6.4 Properties of the Binomial Distribution\nI define “math” as the process of making up rules just to see what happens. The Binomial distribution isn’t just some abstract entity that we discovered - it’s a set of rules we created that seem to logically fit some situations.14 So first: what are the rules?\n\nBinomial Assumptions\nI’m going to motivate these assumptions first. If you’re the type that just wants to memorize, you can skip to the end of this section.\nWe’ve been talking about flipping coins and rolling dice, which helped motivate this distribution. We wouldn’t be teaching you this distribution if it only applied to dice and coins, so when can we apply it?\nConsider flipping a “sticky” coin twice. It starts with a 50/50 chance of being heads, but the next flip has a 75% chance of being the same as the first.15 So if the first flip was heads, there’s a 75% chance that the second flip will be heads. If the first flip was tails, there’s a 75% chance that the second flip will be tails.\nLet’s first just calculate the probability of each outcome. The probability that the first flip is heads and the second flip is tails can be found using the Multiplication Rule, which states that P(A and B) = P(A)P(B|A). So P(HH) = P(first is H)P(second is H given that the first was H) = 0.5*0.75 = 0.375. Similarly, P(HT) = 0.125, P(TT) = 0.375, and P(TH) = 0.125.16\nLet’s compare these probabilities with the ones we calculated earlier. The probability of 0 heads with the fair coin was 1/4, and this value was calculated with the binomial distribution. With the sticky coin, the probability of 0 heads is 0.375, which does not come from the binomial distribution.\nFormally, the Binomial distribution assumes that each trial is independent and the probability of success is the same in each trial. While I didn’t touch on this, the only random thing should be the number of successes, not the number of trials. Finally, recall that, with the dice, I converted things to “3” or “not 3”; the Binomial distribution only works when each individual trial can only be one thing or another. More succinctly, the assumptions for the Binomial Distribution are:\n\nThere are n trials, and this number is known ahead of time.\nEach trial is either a “success” or a “failure”.\nEach trial is independent of the other trials.\nThe probability of success is the same for all trials.\n\n\n\nSide note: “probability of success is the same”\nAs an example, consider studying, say, the proportion of questions that a student got right on a multiple choice test. Each student has a different probability of getting each question correct. However, if we want to say something about the proportion of questions that a random student gets right on a test. In this sense, the fourth assumption is not violated.\nAs an alternative, consider a test where the students go one-by-one17 and can see the previous student’s solutions. In this case, the probability of success changes as you have more trials. This is where the problem lies - the students are still coming in a random order, but the probability of success changes.\nAs another alternative, suppose some students are cheating. They’re more likely to get the right answers together, so they’re answers are dependent on each other; knowing one cheater’s answer gives you a better guess at another cheater’s answer.\nIn summary, a different probability of success is only an issue if the researcher would be able to know this ahead of time. If the probability of success is different but we have a simple random sample with independent trials, there is no issue with this assumption.\n\n\nBinomial Mean and Variance\nNow that we know the assumptions, we can see what comes out of these assumptions. First, we can find the average value. It makes perfect sense that the average number of heads in 10 flips should be 5. There’s a 50/50 chance of heads, so you’d expect half of the flips to be heads! Formally, \\(\\mu = np\\).18 That is, the theoretical average is just the number of trials times the probability of success.\nWhat about the variance? It’s not as obvious. I’m going to try and give my own intuitive argument, but most teachers and textbooks simply skip this and have you memorize the answer. If this is your style, you can skip to the end of this section.\nIn the past, I have defined “variance” as something like “the average amount that you would be wrong if you always guessed the mean value.” Consider flipping one coin. If this coin is rigged and always comes up heads, the mean number of heads is 1 and you would always be right when you guess the mean. Intuitively, the variance here is 0. The same happens if the coin is rigged to always come up tails - the mean number of heads is 0, and the number of heads never varies so the variance is 0.\nWhat happens between 0 and 1? If the coin was heads 80% of the time, then your guess would be right 80% of the time. The actual value of the coin varies, but not too much. If the coin was heads 20% of the time, you’d still be right 80% of the time by guessing 0 heads each time. You’d be wrong most often if the coin had a 50% chance of being heads.\nSo we’ve established this: At \\(p=0\\) and \\(p=1\\), the variance is 0. The maximum value is at 0.5, and the variance should be the same if you’re 0.2 above 0.5 or 0.2 below (it’s symmetric around 0.5). The following plot, then, seems reasonable:\n\n\n\n\n\nThere’s a lot of math behind this, but the variance for Bin(1,p)19 turns out to be p(1-p). You can see that it would be symmetric around 0.5 and would be 0 whenever p=0 or p=1.\nIn general, the variance of a B(n,p) distribution is \\(\\sigma^2\\) = np(1-p)."
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#conclusion",
    "href": "L07-Binomial_Probabilities.html#conclusion",
    "title": "6  Binomial Probabilities",
    "section": "6.5 Conclusion",
    "text": "6.5 Conclusion\nIf you have a known number of repeated trials that are independent and are either a “success” or “falure”, then the Binomial distribution is your friend. Once these assumptions are met, you can calculate the probability of any number of successes using the pdf, you know what the mean number of successes in \\(n\\) trials will be, and you know the variance!20\nAs a rule, if you see the phrase “Not enough information for a valid answer” as an option in a multiple choice question, double check that the assumptions are all met. If the observations are not independent, you need to know all of the conditional probabilities in order to calculate the answer, which you probably don’t have, so you’re missing information. If the probability of success changes from trial to trial, you need to know how it changes."
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#self-test-questions",
    "href": "L07-Binomial_Probabilities.html#self-test-questions",
    "title": "6  Binomial Probabilities",
    "section": "6.6 Self-Test Questions",
    "text": "6.6 Self-Test Questions\n\nTake a moment to explain what the dbinom() and pbinom() R functions do.\nWhat happens when you put x = 0.5 into dbinom(x, 10, 0.5)? Interpret this in terms of flipping coins.\nI debated whether to include a section on “shape”, but decided to let you figure it out for yourself. I’ve already given you the code to plot the pdf. For each of the values of n (size) and p (prob), plot the pdf and describe the shape. Note that x should (almost) always be x &lt;- 0:n.\n\nBin(50, 0.5)\nBin(50, 0.8)\nBin(50, 0.2)\nBin(50, 0.02)\nBin(4, 0.25)\n\nFor each of the assumptions, give an example of a situation that violates only one of them, not the others."
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#crowdsourced-questions",
    "href": "L07-Binomial_Probabilities.html#crowdsourced-questions",
    "title": "6  Binomial Probabilities",
    "section": "6.7 Crowdsourced Questions",
    "text": "6.7 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA biased coin has a probability of 0.7 of landing on heads when flipped. If the coin is flipped 10 times, what is the probability of getting exactly 8 heads?\n\n\n\nSolution\n\nThe probability of getting exactly “k” successes in “n” trials, each with a probability success “p”, can be calculated using the binomial probability formula \\[\nP(X=K) = (n/k) * p^k * (1-p)^(n-k)\n\\] Where (nk) is the binomial coefficient = (n!/k!(n-k)!)\nIn this case, n = 10, p = 0.7, k = 8\nCalculating the binomial coefficient:\n\n\\(P(X=8) = (10/8) * 0.7^8 * (1-0.7)^(10-8)\\)\n\\((10/8)=(10!/8!(10-8)!) = 45\\)\n\\(P(X = 8) = 45 * 0.7^8 * (0.3)^2\\)\n\\(P(X = 8) ≈ 0.233\\)\n\nTherefore, the probability of getting exactly 8 heads when flipping the biased coin 10 times is approximately 0.233.\nIn R:\n\ndbinom(8, size = 10, prob = 0.7)\n\n[1] 0.2334744\n\n\n\n\n\nIn a hockey game, a player has a 20% chance of scoring a goal with each shot attempted.\n\nIf the player takes 5 shots during a game, what is the probability that exactly 2 of them result in goals?\nWhat is the probability that the player scores at least 3 goals out of 8 shots attempted in a game?\nWhat is the probability that the player scores at most 1 goal out of 2 shots attempted in a game?\n\n\n\n\nSolution\n\n\n“EXACTLY” 2 goals out of 5 shots ⇒ We want to solve for P(X = 2):\n\nIn R:\n\ndbinom(x = 2, size = 5, prob = 0.20)\n\n[1] 0.2048\n\n\nTherefore, the probability that exactly 2 of the 5 shots taken during a game is approximately 20.5%.\n\n“AT LEAST” 3 goals out of 8 shots ⇒ We want to solve for P(X \\(\\ge\\) 3) = 1 - P(X &lt; 2):\n\nIn R:\n\n# The easy way: P(X &gt;= q) = 1 - P(X &lt; q) -- the equal sign is important!\n1 - pbinom(q = 2, size = 8, prob = 0.20)\n\n[1] 0.2030822\n\n# Alternatively: 1 - P(X = 0) - P(X = 1) - P(X = 2)\n1 - (dbinom(0, 8, 0.20) + dbinom(1, 8, 0.20) + dbinom(2, 8, 0.20))\n\n[1] 0.2030822\n\n\nTherefore, the probability that the player scores at least 3 goals out of 8 shots attempted in a game is approximately 20.3%.\n\n“AT MOST” 1 goal out of 2 shots ⇒ We want to solve for P(X ≤ 1):\n\nIn R:\n\npbinom(q = 1, size = 2, prob = 0.20)\n\n[1] 0.96\n\ndbinom(0, 2, 0.20) + dbinom(1, 2, 0.20)\n\n[1] 0.96\n\n\nTherefore, the probability that the player scores at most 1 goal out of 2 shots attempted in a game is 96.0%."
  },
  {
    "objectID": "L07-Binomial_Probabilities.html#footnotes",
    "href": "L07-Binomial_Probabilities.html#footnotes",
    "title": "6  Binomial Probabilities",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nSince these events are disjoint, we can simply add them.↩︎\nFor coin flips, this is obvious. It won’t always be!↩︎\nAnd 0 ways to choose 4 things.↩︎\nVerify this by writing out all of the possibilities.↩︎\nDon’t be afraid to re-read this paragraph several times, there’s a lot of math here.↩︎\nCall your grandma - she’ll appreciate it!↩︎\nNotice how we’re using upper case for the random variable, and lower case for the actual values. This is important for future stats classes, but just something you’ll see me do for now.↩︎\nI usually use lower case so you don’t confuse it with the PDF file extension.↩︎\nThere’s a much more correct, much more technical definition, but it’s outside the scope of this course.↩︎\nP(X \\(\\le\\) x)↩︎\nP(X \\(\\le\\) x) = 1 - P(X &gt; x)↩︎\nThe philosophy of math is extremely interesting. Most philosophers seem believe that we do discover math, rather than create it. I also believe this, but probability distributions are in a grey area for this part of philosophy. When all this is over we should grab a drink and discuss this.↩︎\nIf an engineer could make this coin for me I’d be infinitely grateful.↩︎\nAlways make sure the numbers that I give you add to 1 - I will try and trick you with this!↩︎\nin a random order↩︎\nWhy \\(\\mu\\) and not \\(\\bar x\\)? Because this is a theoretical result. You can think of this as being the “true” population.↩︎\nWhen n=1, this is also called the Bernoulli distribution, but this is not important right now.↩︎\nStatistics is the study of variance.↩︎"
  },
  {
    "objectID": "L08-Normal_Distributions.html#introduction",
    "href": "L08-Normal_Distributions.html#introduction",
    "title": "7  The Normal Distributions",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIn this lecture we are looking at continuous distributions. Continuous distributions have an odd quirk. If a variable has a continuous distribution, then \\(P(X = x) = 0\\). That is, the probability of any specific value is infinitely small.\nThink of it this way: suppose that human heights go from 54 cm to 272 cm. For now, suppose all of these heights are equally likely. If we record heights to the nearest centimeter, there are 219 possible heights, so the probability that you are one of those heights is 1/219. If we round to the nearest mm, there are 21,900 different heights. As we get a more and more accurate measuring instrument, the probability of any given height goes to 0. It’s not that these heights are impossible, it’s that you’re probably not going to ever guess my exact height when we measure it with infinite accuracy.\nSo what do we do? How could we possibly calculate probabilities? Well, we measure ranges! You can’t guess my height exactly, but we can talk about the probability that my height is between 170 and 180 cm, or even the probability that my height would be 170, assuming we round to the nearest centimeter.\n\nSome Facts about Distributions\nBefore we begin, the following properties are true of any distribution, regardless of whether they are discrete or continuous.\n\nAll probabilities must be between 0 and 1.\nAll probabilities together must make 1.\n\nFor discrete, adding them all should get you to 1.\nFor continuous, the area under the density curve must be 1.1\n\nIf two events are disjoint, you must be able to add their probabilities.\n\nIt’s weird, but we have to define this as a rule first before we can calculate probabilities.\n\n\nThe first point should be obvious, and you won’t ever need to check whether the third point is true.\nThe second point is the important one: The total probability for all events must be 1. For continuous distributions like the normal distribution, that means that the area under the curve is 1.2"
  },
  {
    "objectID": "L08-Normal_Distributions.html#the-normal-distribution",
    "href": "L08-Normal_Distributions.html#the-normal-distribution",
    "title": "7  The Normal Distributions",
    "section": "7.2 The Normal Distribution",
    "text": "7.2 The Normal Distribution\nThe normal distribution is a way to define the probability of something using a function, but the function is complicated.3 Instead, we’ll jump right into how to use it and let software deal with the function.\nIn the introduction, I used the example of people’s heights. I made the assumption that all heights were equally likely, but this is just a bonkers thing to say. Instead, some heights are more likely than other heights. Of course, this doesn’t mean that, say, 170 cm is very unlikely, but 175 is likely, then 176 is unlikely, then 177 is very unlikely, then 178 is suddenly really likely again; most people have heights close to the average and heights further from the average are less likely. This is exactly what the normal distribution is for! Here’s what the normal distribution looks like:\n\nmu &lt;- 162.3\nsig &lt;- 7.11\nxseq &lt;- seq(mu-3*sig, mu + 3*sig, length.out = 300)\nyseq &lt;- dnorm(x = xseq, mean = mu, sd = sig) \n\nplot(xseq, yseq, type = \"l\",\n  main = \"Heights of Canadian Women\",\n  xlab = \"Height (cm)\", ylab = \"Prob. Density\")\n\n\n\n\nThe plot above has the highest point occurs at exactly 162.3, which is the best number I could find for the actual average height of Canadian women. This is denoted \\(\\mu\\). The width of the curve is a little trickier - how did I choose to make it go from 145 to 180? I could easily have stretched it out or squeezed it inwards in both directions. The width is defined by the standard deviation, which is denoted \\(\\sigma\\). Because of the way the normal distribution is defined, there’s nothing else we can change about it - knowing \\(\\mu\\) and \\(\\sigma\\) are enough to draw the entire curve.\n\n## Requires the \"shiny\" library\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/normShape\")\n\nIf we have a variable \\(X\\) that follows a normal distribution, we use the notation \\(X \\sim N(\\mu, \\sigma)\\).\n\n\n\n\n\n\nWarning\n\n\n\nSome textbooks use the notation \\(X \\sim N(\\mu, \\sigma^2)\\), i.e. they use \\(\\sigma^2\\) rather than \\(\\sigma\\). It’s like driving on the left or the right side of the road - both are fine, but we have to choose one and stick with it.\n\n\nThe idea that “most things are close to the center, and fewer things further away” can be very powerful. This applies to:\n\nHuman heights\nIncome for a given job position\nChange in stock price from day to day\n\nOn average the change is 0, but it does change. Small changes are much more likely than large ones, but large ones do happen.\nObviously, extreme events happen sometimes, and major changes can happen.\n\nIQ scores\nBirth weight\nHow much the prediction of a model differs from the truth"
  },
  {
    "objectID": "L08-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "href": "L08-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "title": "7  The Normal Distributions",
    "section": "7.3 Calculating Normal Probabilities - Part 1",
    "text": "7.3 Calculating Normal Probabilities - Part 1\nThe height and width of the normal distribution are determined by the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)), and only the mean and standard deviation. The mean just moves the curve left and right, the standard deviation squeezes or stretches it.\nTo highlight this, we introduce something called the Empirical Rule, a.k.a. the 68-95-99.5 Rule. No matter what the mean of the distribution is, 68% of the probability is within 1 standard deviation of the mean. To say this another way, let’s extend our notation slightly. If \\(X\\sim N(\\mu,\\sigma)\\), we can say that:\n\\[\\begin{align*}\nP(\\mu - \\sigma \\le X \\le \\mu + \\sigma) \\approx 0.68\n\\end{align*}\\]\nTo phrase this in another way, if we were to draw random numbers from the normal distribution, 68% of them would be between 1sd below the mean and 1sd above the mean.\n\nset.seed(-4) # Ensure the same random numbers every time\n\n## generate 10000 random N(0,1) values\nx &lt;- rnorm(n = 10000, mean = 0, sd = 1) \n\n## You won't need to know how to write this code:\nsum(x &gt; -1 & x &lt; 1) # x is larger than -1 AND less than 1\n\n[1] 6766\n\n\nSo out of 10,000 random numbers from a N(0,1) distribution, 6,766 (67.66%) of them were above -1 but below 1. If we change the mean and sd, we still get the same results:\n\n## Mean is 4, sd is 30, so mean - 1sd = 4 - 30\n## Change the mean and sd for yourself to see what happens!\nmu &lt;- 4\nsigma &lt;- 30\nx2 &lt;- rnorm(n = 10000, mean = mu, sd = sigma)\nsum(x2 &gt; (mu - sigma) & x2 &lt; (mu + sigma)) # Not exactly 68%, but approximate!\n\n[1] 6845\n\n\nAs you can guess from the name “68-95-99.7 Rule”, 68% being within one sd is only part of the story. The 95 refers to 95% being within 2sd of the mean, and the 99.7 refers to 99.7% being within 3sd of the mean.\n\nsum(x &gt; -2 & x &lt; 2) # within 2sd of the mean\n\n[1] 9523\n\nsum(x &gt; -3 & x &lt; 3) # within 3\n\n[1] 9970\n\n## Try this with x2 as well!\n\nSome variant of the following image appears in countless textbooks:\n\n\n\n\n\nAs a small side note, the image above uses the word “data”. By this, it means that if this is the population, then 68% of all the data that it were possible to collect would be within one standard deviation of the mean. As we saw in the simulated data above, this number is almost never going to be perfect.\n\nTrickier calculations\nIf 68% of the data is between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\), then there’s still 32% of the distribution outside this range. The normal distribution is symmetric, so this 32% gets split exactly in half and 16% of the distribution is below \\(\\mu - \\sigma\\), and 16% is above \\(\\mu + \\sigma\\).\nBased on this calculation, we can say that 84% of any normal distribution is below \\(\\mu + \\sigma\\), and 84% is above \\(\\mu - \\sigma\\). Before we move on, draw out some normal distributions to convince yourself that 97.5% of any normal distribution is less than \\(\\mu + 2\\sigma\\).4\nYou should try the following calculations yourself, all of which can be done with basic arithmetic and the 68-95-99.7 Rule:\n\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu-\\sigma\\).\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu+\\sigma\\).\nAbove \\(\\mu + 2\\sigma\\) and below \\(\\mu + 3\\sigma\\)\nAbove \\(\\mu - 3\\sigma\\) and below 0."
  },
  {
    "objectID": "L08-Normal_Distributions.html#the-standard-normal-distribution",
    "href": "L08-Normal_Distributions.html#the-standard-normal-distribution",
    "title": "7  The Normal Distributions",
    "section": "7.4 The Standard Normal Distribution",
    "text": "7.4 The Standard Normal Distribution\nWe use a special letter (Z, pronounced “zed” because we’re Canadian) to denote a standard normal distribution. In particular, \\(Z\\sim N(0, 1)\\) is a normal distribution with mean 0 and standard deviation 1. Many many many many textbooks have a table in the back of them that gives probabilities for the standard normal distribution, and they call them \\(Z\\) tables.\nAll normal distributions have the exact same shape. In order to change the mean and sd, we can simply re-write the numbers on the axes. If we want to shift the whole curve to the left by 2 units, we can re-label the numbers on the x axis. If we change the sd, the plot might get “taller” or “shorter”, but if we zoom in on the plot we can make it look exactly the same!5\n\nStandardizing a Normal Distribution\nBecause they all look the same, we might as well work with just one of them! Suppose \\(X\\sim N(\\mu,\\sigma)\\). If we shift the whole curve to the left, then the mean shifts as well and the mean is 0. In other words, \\(X-\\mu \\sim N(0,\\sigma)\\). Now that the mean is at 0, \\(\\mu + 1\\sigma\\) is simply \\(\\sigma\\), \\(\\mu-3\\sigma\\) is \\(-3\\sigma\\), and so on. If we divide all of the numbers by \\(\\sigma\\), then \\(\\sigma\\) is simply 1, \\(-3\\sigma\\) is simply -3, and so on. To formalize this, if \\(x\\sim N(\\mu,\\sigma)\\), then\n\\[\\begin{align*}\n\\frac{X-\\mu}{\\sigma} = Z \\sim N(0, 1)\n\\end{align*}\\]\nThis is called standardizing a normal distribution. The resultant value is called the z-score.\nFor example, suppose a woman is 155.19 cm tall. If the true mean height of Canadian women is 162.3 and the standard deviation is 7.11, then this particular woman is exactly one standard deviation below the mean. This is the z-score, a.k.a. the standardized value; this woman’s z-score is -1.\nNow consider a woman who is 161.22 cm tall. Her z-score would be -0.152,6 meaning that she is 0.152 standard deviations below the mean.\nLet’s return to the 155.19 cm tall woman. If you take a woman at random from the population, what is the probability that the randomly chosen woman be be shorter than 155.19 cm? Based on the 68-95-99.7 rule, 68% of women are within one standard deviation of the mean, which is a range from 155.19 to 169.41. Since 68% of the women are betwen these two numbers, 16% of them are shorter than 155.19 (it is also true that 16% are taller than 169.41, but this was not required for the question).\nNow, what’s the probability that a randomly chosen woman is, say, shorter than 160 cm? This doesn’t fit nicely in the empirical rule, so we need another way to calculate probabilities. However, it’s worth stopping and trying to make a guess! The empirical rule tells us that 16% of women are below 155.19 cm, and we also know that 50% of women are shorter than the average of 162.3 cm (since the normal distribution is symmetric), so we expect that the answer is somewhere between 16% and 50%, probably closer to 50% since 160 cm is closer to 162.3 cm than it is to 155.19 cm."
  },
  {
    "objectID": "L08-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "href": "L08-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "title": "7  The Normal Distributions",
    "section": "7.5 Calculating Normal Probabilities - Part 2",
    "text": "7.5 Calculating Normal Probabilities - Part 2\nIn general, we use the cumulative distribution function (CDF, or cdf) to calculate probabilities. As with the cumulative probability tables we saw in the probability lectures, the cumulative probability calculates the area to the left of a particular point.7 Questions about the normal distribution generally come in three flavours:\n\nFind \\(P(X \\le a)\\)\nFind \\(P(X \\ge b)\\)\nFind \\(P(c \\le X\\le d)\\)\n\nThe cdf is defined as \\(P(X\\le x)\\), which allows us to answer questions like 1. For the standard normal distribution, a table of Z probabilities can be found at the back of the textbook. I’ve added a file that demonstrates how to use the Z-table in the Lecture Materials. This is something that is crucial to know for closed-book tests since you will need to caclulate probabilities somehow, but we can’t let you have a computer to run R! Before moving on, read “Intro to Ztable.pdf”.\nIn that file, there are some practice problems. Below, you’ll find a selection of solutions using R. For your own practice, try and calculate them with the Z-table (with some good drawings) and verify your answer with R.\n\n## 1. Find the probability of a z-value less than 1.11.\npnorm(1.11)\n\n[1] 0.8665005\n\n## 2. Find the probability of a z-value greater than 1.11\n1 - pnorm(1.11)\n\n[1] 0.1334995\n\n## 3. Find the probability of a z-value greater than -2.01 but less than 1.\npnorm(1) - pnorm(-2.01)\n\n[1] 0.8191292\n\n## 4. Verify the empirical rule: 68-95-99.7\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\npnorm(2) - pnorm(-2)\n\n[1] 0.9544997\n\npnorm(3) - pnorm(-3)\n\n[1] 0.9973002\n\n\nFor questions like \\(P(X\\ge x)\\), we can simply use the fact that \\(P(X \\ge x) = 1 - P(X&lt;x)\\). Since this is a continuous distribution and \\(P(X = x)=0\\), we also know that \\(P(X\\le x) = P(X&lt;x)\\) and we can just use the cdf. The last one is a little bit trickier.\nTo calculate the probability that a randomly chosen value will be within a given range, there are a few steps. Let’s use the same example as the textbook: If \\(X\\sim N(-2, 1)\\) find \\(P(-2.5\\le X\\le -1)\\). If we want to use the cdf, we need to re-write this in terms of \\(P(X\\le x)\\).\nHere’s how we do it. If we only find \\(P(X\\le-1)\\), then we have taken too much of the distribution. Everything to the left of -2.5 was something that should not have been included. So why don’t we just remove it? By this logic, we can find \\(P(-2.5\\le X \\le -1) = P(X\\le -1) - P(X \\le -2.5)\\). This is shown graphically below:\n\nReturning to the heights example, the probability of a randomly chosen woman being less than 160 cm can be calculated as: \\[\n\\frac{x - \\mu}{\\sigma} = \\frac{160 - 162.3}{7.11} = -0.323488045\n\\] We can now look up -0.323 with the pnorm function:\n\npnorm(-0.323488045)\n\n[1] 0.3731628\n\n\nNote that R will do the standardization for you if you ask it politely.\n\npnorm(q = 160, mean = 162.3, sd = 7.11)\n\n[1] 0.3731628\n\n\nI have created a shiny app that lets you explore these calculations8. Feel free to use this to answer the questions in this lecture, and then double check the answers with pnorm.\n\n## install.packages(\"shiny\") # Run this if you get an error about \"package not found\"\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\nExamples\n\n\nEx1: P(X &lt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value less than 0?\n\nSolution 1: Standardize and Z-table. I’ll split this up into steps:\n\nStandardize: \\((x-\\mu)/\\sigma = (0 - 4)/2 = -2\\).\nFind -2 on the Z table: -2=-2.00, so this will be in the row labelled -2.0 and the column labelled 0.00,9 which is 0.0228.\nConclude: 2.28% of the N(4,2) distribution is below 0.\n\nSolution 2: Empircal rule.\n\nBefore calculating a normal probability, try and estimate how many standard deviations away from the mean the value is. In this case, 0 is 2 standard deviations from 4. The 68-95-99.7 rule states that 95% of the distribution is outside the range from \\(\\mu - 2\\sigma\\) to \\(\\mu + 2\\sigma\\), so 5% is outside of this range. This means that 2.5% is on either side, which means that 2.5% is below 0.\n\n\n\nA short version of Solution 2: By the 68-95-99.7 Rule, 95% is between 0 and 8. Therefore, 2.5% must be less than 0.\nAs you can see, the 68-95-99.7 rule is approximate. However, I highly recommend doing many practice problems with it. On a multiple choice question, if you can figure out the answer with the Empirical Rule than you might be able to guess the correct answer much quicker. You won’t get the exact answer, but if there’s only one answer that’s close to your guess, then that’s probably it.10\nSolution 3: R.\n\n## Standardize:\npnorm((0 - 4)/2)\n\n[1] 0.02275013\n\n## Same answer, without standardizing:\npnorm(q = 0, mean = 4, sd = 2)\n\n[1] 0.02275013\n\n\n\nIf \\(X\\sim N(1234, 56)\\), what’s the probability of a number smaller than 1432.\n\nBefore we begin: What do we expect the number to be? The mean is 1234, which is smaller than 1432. Is it a little smaller, or is it a lot smaller? Compared to the standard deviation, it’s a lot smaller. By the empirical rule, the vast majority of the distribution is below \\(\\mu + 3\\sigma\\), which is approximately 1400.11 We should expect an answer close to 1, since the area under the normal distribution is 1.\nSolution 1: \\((x-\\mu)/\\sigma = (1432-1234)/56 = 3.54\\), which is not on the Z table. When this happens (and we don’t have access to technology), we simply say the answer is 1.12\nSolution 2: The value we’re interested in isn’t 1, 2, or 3 standard deviations from the mean, so the Empirical Rule doesn’t apply. However, we can guess that our probability will be close to 1 since it’s larger than 3 standard deviations away.\nSolution 3: R.\n\npnorm(1432, mean = 1234, sd = 56)\n\n[1] 0.9997967\n\n\nIdeally, you would only ever use intuition from the Empirical rule, or use R. The Z-table is super convenient for written, in-person exams. It’s also nice for situations where you don’t have a computer with R available.\n\n\nP(X &gt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value greater than 0?\n\nBefore we start: Use the empirical rule! 0 is 2sd below the mean, so the answer should be close to 97.5%\nWith the Z table: \\((x-\\mu)/\\sigma = -2\\), and we’ve already found this on the table as 0.0228. Since we’re looking at the right tail, our answer is 1 - 0.0228 = 0.9772.\nWith R:\n\n1 - pnorm(0, mean = 4, sd = 2)\n\n[1] 0.9772499\n\n\n\nSuppose \\(X\\sim N(23, 23)\\). What’s the probability of a value larger than 23?\n\nBefore we start: The normal distribution is perfectly symmetric, which we have learned means that the mean is equal to the median. The median marks the point where 50% of the distribution is smaller. So before doing any work, we know that the answer must be 50%\n\npnorm(23, 23, 23)\n\n[1] 0.5\n\n\n\n\nP(a &lt; X &lt; b)\n\nIf \\(X\\sim N(0, 1)\\), what’s the probability of a value between -1.52 and -0.5?\n\nSolution 1: We have a standard normal value, so we can look these values up directly. P(Z &lt; -1.52) = 0.064313 and P(Z &lt; -0.5) = 0.3085.14 We want the area between these two values. P(Z &lt; -0.5) contains everything from negative infinity to -0.5, but we only want values from -1.52 to -0.5. To fix this, we remove everything from negative infinity to -1.52. Our answer is 0.3085 - 0.0643 = 0.2442.\nSolution 2: R.\n\npnorm(-0.5) - pnorm(-1.52)\n\n[1] 0.2442821\n\n\nI have made a shiny app for you to visualize this:\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\n\\(X \\sim N(2,3)\\), find \\(P(-1 &lt; X &lt; 5)\\)\n\nBefore we begin: This is the empirical rule for 1sd. The answer is 68%.\nWith a Z table: We calculate the z-score individually, then subtract the probabilities in a way that makes sense.15 \\(P(X &lt; -1) = P((X-\\mu)/\\sigma &lt; (-1 - \\mu)/\\sigma) = P(Z &lt; (-1 - 2)/3) = P(Z &lt; -1) = 0.1587\\). Similarly, \\(P(X &lt; 5) = P(Z &lt; 1) = 0.8413\\). The answer is 0.8413 - 0.1587 = 0.6826, which is very close to what we got with the Empirical Rule.\nWith R:\n\npnorm(5, mean = 2, sd = 3) - pnorm(-1, mean = 2, sd = 3)\n\n[1] 0.6826895\n\n\n\n\nGoing Backwards\nWhat’s the first quartile of an N(2,3) distribution? It’s the point at which 25% of the distribution is smaller. In other words, P(X &lt; Q1) = 0.25. How do we find Q1?\nWe can look up 0.25 as a probability. That is, as a value in the body of the Z table. This will give us the corresponding z-score.16 Unfortunately, 0.25 isn’t in the table. The closest values are 0.2514 (which is a Z score of -0.67) and 0.2483 (Z score of -0.68). On a test situation, -0.67 and -0.68 would both be valid answers, as would -0.675.\nIn R, the “q” family of functions are the reverse lookup functions. That is, You tell them the probability, and they return the z-score.\n\nqnorm(0.25, mean = 0, sd = 1)\n\n[1] -0.6744898\n\n\nHowever, we’re not done yet! We found the quartile for a standard normal distribution. We have to go backwards in the standardization formula. In essence, we have found Z and we need to find X.\n\\[\\begin{align*}\n\\frac{x - \\mu}{\\sigma} = z \\Leftrightarrow x = z\\sigma + \\mu\n\\end{align*}\\]\nTo finish this question, we say that the first quartile of a N(2, 3) distribution is -0.67*3 + 2 = -0.01.17\nIn R:\n\nqnorm(0.25, mean = 2, sd = 3)\n\n[1] -0.02346925"
  },
  {
    "objectID": "L08-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "href": "L08-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "title": "7  The Normal Distributions",
    "section": "7.6 Problems: Verifying the Empirical Rule",
    "text": "7.6 Problems: Verifying the Empirical Rule\n\n68-95-99.7"
  },
  {
    "objectID": "L08-Normal_Distributions.html#problems-z-scores",
    "href": "L08-Normal_Distributions.html#problems-z-scores",
    "title": "7  The Normal Distributions",
    "section": "7.7 Problems: Z-scores",
    "text": "7.7 Problems: Z-scores\n\n\n\\(P(z \\le 2.25)\\)\n\\(P(z \\le -2.25)\\)\n\\(P(z \\ge 2.25)\\)\n\\(P(z \\ge -2.25)\\)\n\n\n\n\\(P(-2 \\le z \\le 2)\\)\n\n\\(P(Z \\le 2\\; and\\; Z \\ge -2)\\)\n\n\\(P(2 \\le z \\le -2)\\)\n\\(P(0 \\le z \\le 2)\\)\n\\(P(-2 \\le z \\le 0)\\)\n\\(P(Z \\ge 2\\; or\\; Z \\le -2.5)\\)\n\n\n\n\\(P(Z \\le z) = 0.5\\)\n\\(P(Z \\ge z) = 0.4238\\)\n\nWhat is \\(z\\)?"
  },
  {
    "objectID": "L08-Normal_Distributions.html#problems-standardizing",
    "href": "L08-Normal_Distributions.html#problems-standardizing",
    "title": "7  The Normal Distributions",
    "section": "7.8 Problems: Standardizing",
    "text": "7.8 Problems: Standardizing\n\nThe birthweights of cute widdle babies born at full-term is \\(N(3350, 440)\\).\n\nLow birthweight babies are those with a weight less than 2500. Probability of this?\nHigh birthweight is above 4200. Probability?\nProbability of either low or high?\n\n\nA paper claimed that their control group was normal with a mean of 7 headaches per month, and the treatment group had a mean of 3.\nThe paper later claims that there’s only a 10% chance of seeing fewer than 3 headaches in the control group.\nThe paper never provided the sd. What is it?"
  },
  {
    "objectID": "L08-Normal_Distributions.html#participation",
    "href": "L08-Normal_Distributions.html#participation",
    "title": "7  The Normal Distributions",
    "section": "7.9 Participation",
    "text": "7.9 Participation\n\n\n\\(P(Z &lt; 1.5)\\)\n\\(P(Z &gt; -1.5)\\)\n\\(P(Z &lt; 1.2 or Z &gt; 1.3)\\)\n\\(X\\sim N(0,2)\\), find \\(P(X &lt; 2)\\)\n\\(X\\sim N(\\mu, 5)\\) and \\(P(X \\le 2) = 0.25\\), find \\(\\mu\\)\n\\(X \\sim N(2, 4)\\). Find the IQR."
  },
  {
    "objectID": "L08-Normal_Distributions.html#summary",
    "href": "L08-Normal_Distributions.html#summary",
    "title": "7  The Normal Distributions",
    "section": "7.10 Summary",
    "text": "7.10 Summary\n\nMost values are close to the mean, with fewer values as you get further away.\nThe mean and sd are sufficient to draw the whole curve.\nProbabilities are areas. The area of a single point is 0.18\n68% is within one sd of the mean, 95% within 2 sd, and 99.7% within 3 sd\n\n\\(P(\\mu - 1\\sigma \\le X \\le \\mu + 1\\sigma) \\approx 0.68\\).\n\\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma) \\approx 0.95\\).\n\\(P(\\mu - 3\\sigma \\le X \\le \\mu + 3\\sigma) \\approx 0.997\\).\n\nFor standard normal, the values on the x axis are z-score.\nThe cdf, P(X &lt;= x), is used to calculate areas.\n\nThe table can be found in the back of the textbook for standard normal. To standardize, use the formula \\((x-\\mu)/\\sigma\\).\npnorm(x, mean = 0, sd = 1) gives the standard normal cdf. If mean and sd are not specified, pnorm() assumes you want standard normal.\n\n\\(P(a \\le X \\le b) = P(X \\le b) - P(X \\le a)\\)\n\nEmpirical rule: pnorm(1) - pnorm(-1); pnorm(2) - pnorm(-2); …\n\nYou need a lot of practice with these kinds of problems. Do not check the answers prematurely.\n*norm functions:\n\nrnorm(n, mean, sd) generates random numbers\ndnorm(x, mean, sd) gives the height of the curve at the point x. This is not a probability.\npnorm(q, mean, sd) = \\(P(X \\le q)\\).\nqnorm(p, mean, sd) finds \\(q\\) such that \\(P(X \\le q) = p\\).\n\nIt is the backwards version (inverse function) of pnorm().\npnorm(qnorm(0.5)) returns 0.5, qnorm(pnorm(2)) returns 2."
  },
  {
    "objectID": "L08-Normal_Distributions.html#self-study-questions",
    "href": "L08-Normal_Distributions.html#self-study-questions",
    "title": "7  The Normal Distributions",
    "section": "7.11 Self-Study Questions",
    "text": "7.11 Self-Study Questions\n\nFor each of the probability statements, draw the normal distribution and add shading for the probability. For example, P(Z &gt; 1) should be a normal distribution with everything under the curve and larger than 1 shaded in. This is a very good way to help internalize the fact that all probabilities are areas.\nIn P(Z &lt; 1.32) = 0.9066, what do 1.32 and 0.9066 represent? Where are they on the Z table. If I were to give you one and not the other, could you find the missing number?\nWrite down all of the probability statements on a separate piece of paper. Solve them without looking at these notes. More practice, more better.\nPicture two normal distributions: one looks taller, and one looks wider. Which one has the larger standard deviation?\nExplain why the standard deviation does not affect the shape of the normal distribution. Now, explain why it does affect the shape.19"
  },
  {
    "objectID": "L08-Normal_Distributions.html#more-questions",
    "href": "L08-Normal_Distributions.html#more-questions",
    "title": "7  The Normal Distributions",
    "section": "7.12 More Questions",
    "text": "7.12 More Questions\nIf you have not calculated at least 50 or 60 different normal probabilities by the midterm, you have probably not done enough practice.\nFor each of these questions, start by trying to use the empirical rule, then use the Z table, then confirm your answer with R. Answers with R are shown below, but you should only check these once you’re confident with your own answer.20\n\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 1?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 2?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 3?\n\\(X \\sim N(-2, 500)\\), find the 75% quantile (aka Q3).\n\\(X \\sim N(3.14, 15.9)\\), what proporion of values are between 2.71 and 8.28?\nSuppose 25% of a normal distribution is below 0, and the mean of this distribution is 1. What’s the standard deviation?21\nWhat to Expect claims that the average baby weighs about 7.5 lbs, with a “normal”22 range of 5.8 to 10 lbs. If the “normal” range is defined as the middle 95%, what is the standard deviation of birth weights?\nYou’re asked to estimate the number of M&M’s in family-sized bags. You’re pretty sure that they are normally distributed and you think the mean is 600. How do you go about guessing the sd? One way is to say that you think it’s “unlikely” that there are more than 650 M&Ms in any given bag.23\n\nIf “unlikely” = 10%, that is, only 10% of the bags have more than 650 M&M’s, what is the sd?\nIf “unlikely” = 5%, what is the sd?\n\nIn the population of Canadian women, what’s the probability that a randomly selected woman is further than 1.7 standard deviations from the mean?\nThere’s a peculiar model that applies to certain kinds of data. If you have \\(\\mu = \\sigma\\), then the normal distribution has certain nice properties.24 Suppose \\(X\\sim N(\\theta, \\theta)\\), where \\(\\theta\\) is just a stand-in for the mean and variance. If \\(P(X &lt; 8) = 0.2\\), what is \\(\\theta\\)?25\n\nI’m going to say it again before you check the answers: Pre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge. You should spend time struggling to convince yourself that you did it right. On an exam, you won’t have the answers so you’ll feel that struggle. Practice the exam struggle now, then you’ll be more confident in your answer on exams.\nHave you ever had that feeling that you knew the material because you could do all of the practice problems, but when you get the exam you forgot everything? That’s because you checked the answers before struggling. You taught yourself to anticipate the answers of those particular questions, rather than teaching yourself the material. The struggling is where you learn. It’s the same as exercise: no pain no gain.\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Questions 1, 2, and 3\n## ~~~~~~~~~~~~~~~~~~~~~~~\n1 - pnorm(c(1,2,3), mean = 0, sd = 2)\n\n[1] 0.3085375 0.1586553 0.0668072\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q4\n## ~~~~~~~~~~~~~~~~~~~~~~~\nqnorm(0.75, mean = -2, sd = 500)\n\n[1] 335.2449\n\nqnorm(0.75)*500 - 2 # Alternative, using standard normal\n\n[1] 335.2449\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q5.\n## ~~~~~~~~~~~~~~~~~~~~~~~\npnorm(8.28, 3.14, 15.9) - pnorm(3.14, 3.14, 15.9)\n\n[1] 0.1267548\n\n## Alternative version, with standard normal\na &lt;- (3.14 - 3.14)/15.9\nb &lt;- (8.28 - 3.14)/15.9\npnorm(b) - pnorm(a) # P(a &lt; z &lt; b) = P(Z &lt; b) - P(Z &lt; a)\n\n[1] 0.1267548\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q6: x = z*sigma + mu =&gt; sigma = (x-mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(0 - 1)/qnorm(0.25)\n\n[1] 1.482602\n\n## Verify that 0 is the first quartile\nqnorm(0.25, mean = 1, sd = (0 - 1)/qnorm(0.25)) # Good!\n\n[1] 0\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q7: empirical rule: 5.8 = mu - 2*sigma, so sigma = (7.5 - 5.8)/2\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(7.5 - 5.8)/2\n\n[1] 0.85\n\n## However, if 10 = mu + 2*Sigma,\n(10 - 7.5)/2\n\n[1] 1.25\n\n## The normal distribution doesn't work because this is a *skewed distribution*\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q8.a) sigma = (x - mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(650 - 600)/qnorm(0.9)\n\n[1] 39.01521\n\n## Verify:\npnorm(650, 600, 39.01521)\n\n[1] 0.9\n\n## Q8b:\n(650 - 600)/qnorm(0.95)\n\n[1] 30.39784\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q9: The \"Canadian Women\" part is irrelevant.\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## The area WITHIN the range is:\npnorm(1.7) - pnorm(-1.7)\n\n[1] 0.9108691\n\n## So the area outside this range is:\n1 - (pnorm(1.7) - pnorm(-1.7))\n\n[1] 0.08913093\n\n## Why is the \"Canadian Women\" part irrelevant?\n## The lower bound will be mu - 1.7*sd = 150.213. When we\n## standardize this, we get z = (x-mu)/sd = 1.7, so we'd use\n## 1.7 in the standard normal distribution\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q10\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## P(X &lt; 8) = 0.2, so let z = qnorm(0.2)\n## z = (x - mu)/sigma = (8 - theta)/theta\n## and therefore theta = 8/(z + 1)\n8/(qnorm(0.2) + 1)\n\n[1] 50.51182"
  },
  {
    "objectID": "L08-Normal_Distributions.html#crowdsourced-questions",
    "href": "L08-Normal_Distributions.html#crowdsourced-questions",
    "title": "7  The Normal Distributions",
    "section": "7.13 Crowdsourced Questions",
    "text": "7.13 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA group of researchers studying Kane toads found that the length of the toads follow a normal distribution. They have a mean length of 20 cm and a standard deviation of 2 cm. Using the 68-95-99.7 Rule, answer the following questions about the toad lengths:\n\nWhat percentage of Kane toads have a length between 18-22 cm?\nWhat percent of the toads have a length between 14-26 cm?\nWhat Percent of Kane toads have a length between 16-24 cm?\n\n\n\n\nSolution\n\n\nWith a mean length of 20 cm and a standard deviation of 2 cm, the toads with a length between 18-22 cm fall within one standard deviation. Approximately 68% of the toads have a length in this range.\n99.7% of the toads have a length that fall within 3 standard deviations, therefore, falling in the range of 14-26 cm.\nToads with a length of 16-24 cm, fall into 2 standard deviations, meaning 95% of the toads fall into this range.\n\n\n\n\nLet X represent the heights, in metres (m), of male basketball players. It is known that X follows a normal distribution with a mean (μ) of 2.05 m and a standard deviation (σ) of 0.1 m. Find the probability that a randomly selected male basketball player is at least 2.2 m tall.\n\n\n\nSolution\n\nWe want to find the probability of a randomly selected male basketball player’s height to be at least 2.2 m tall, which means we are trying to find P(X &gt; 2.2)\nIn order to find the probability, we are going to use the z-score formula: \\(Z = \\frac{X - \\mu}{\\sigma} = (2.2 - 2.05) / 0.1 = 1.5\\)\nFind the probability using the z table:\n\nP(Z &gt; 1.5) = 1 - P(Z &lt; 1.5)\n\nP(Z &gt; 1.5) = 1 - 0.9332\nP(Z &gt; 1.5) = 0.0668\n\n\nTherefore, there is a 6.68% (or 0.0668) chance that a randomly selected male basketball player is at least 2.2 m tall.\nIn R, this can be calculated without rounding:\n\n1 - pnorm(1.5)\n\n[1] 0.0668072\n\n\nWe can also let R do the standardization for us!\n\n1 - pnorm(2.2, mean = 2.05, sd = 0.1)\n\n[1] 0.0668072\n\n\n\n\n\nA group of students’ test scores in calculus are normally distributed with a mean of 75 and a standard deviation of 10. using the empirical rule, calculate the range of scores that which will fall within 68%, 95%, and 99.7% of the mean.\n\n\n\nSolution\n\n68% of scores fall between 75−10 and 75+10, which is 65 to 85.\n95% of scores fall between 75−2(10) and 75+2(10), which is 55 to 95.\n99.7% of scores fall between 75−3(10) and 75+3(10), which is 45 to 105."
  },
  {
    "objectID": "L08-Normal_Distributions.html#footnotes",
    "href": "L08-Normal_Distributions.html#footnotes",
    "title": "7  The Normal Distributions",
    "section": "",
    "text": "This is done with integrals, but we won’t actually do this in this course.↩︎\nFor continuous distributions, “probability” and “area under the curve” are synonyms.↩︎\n\\((2\\pi\\sigma^2)^{-1/2}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right)\\)↩︎\nI generally keep a running tally of the number of normal distributions I draw on the board. Last time I did this, I was almost at 100. The moral: you should be drawing a lot of normal distributions!!!↩︎\nThis is also the reason why the empirical rule works! If you change the labels on the plot, \\(\\mu+\\sigma\\) stays in the same place so you can calculate the same probability.↩︎\nThe negative is important!↩︎\nRecall that \\(P(X=x) = 0\\) in continuous distributions, so we look at ranges.↩︎\nIf you’re curious, yes I’ve made a lot of Shiny apps. You can find them all here: https://github.com/DB7-CourseNotes/TeachingApps↩︎\nThe rows are the digits before and after the decimal, the column is the second digit after the decimal.↩︎\nYou need to trust your ability to use the Empirical Rule, though.↩︎\nQuick maths - we’re just trying to get an okay guess, not the exact answer right now.↩︎\nIf the z-score were -3.54, we’d say the probability is 0.↩︎\nRow labelled -1.5, column labelled 0.02.↩︎\nVerify this!↩︎\nP(X &lt; 5) - P(X &lt; -1)↩︎\nRecall: the body of the Z table are probabilities, the margins are z-scores.↩︎\n-0.68*3 + 2 and -0.675*3 + 2 would also be acceptable.↩︎\nP(X=x) = 0↩︎\nHint: Use the app with “Sticky Axes” checked and unchecked.↩︎\nPre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge.↩︎\nHint: Find the z-score for Q1, then fill out the standardization formula with the values you have.↩︎\nNormal as in “usual”, not as in the normal distribution.↩︎\nThis is actually a very useful way to think about distributions, especially in Bayesian statistics.↩︎\nSorry, the details are far beyond the scope of this course.↩︎\nThis is one of the hardest questions you will encounter.↩︎"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "href": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "title": "8  Sampling Distributions",
    "section": "8.1 Prelude: Populations and Samples",
    "text": "8.1 Prelude: Populations and Samples\nThe main idea in the rest of the course is this: We can use a sample to say something about the population. Before we dive into that idea, let’s make a distinction.\n\nStatistic: A number that we calculate from data.\nPopulation parameter: The value of a statistic if it were calculated for the whole population.\nSample Statistic: The value of a statistic if it were calculated for a single sample.\n\nFor example, we find the mean by taking all of the values and adding them up, then dividing by the number of things we added. For heights of Canadians, the population parameter is the value we would get if we found every Canadians’ height and added them up, then divided by the population of Canada. We obviously can’t do this, but it’s useful to think about. The sample mean is the mean we get when we just have a sample. Since we can only get a sample, it would be super cool if we could use that sample mean to talk about what values of the population mean were reasonable guesses.\nIn the height example, the population was all Canadians. This isn’t always how we define the population! For example, if we wanted to know the average length of pregnancy, we’d be looking at a population of all people who get pregnant at some point in their lives."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#introduction",
    "href": "L10-Sampling_Distributions.html#introduction",
    "title": "8  Sampling Distributions",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nYou take a sample. You find the sample mean. Is this mean exactly equal to the population mean?3 Probably not.\nWait, did I just say probably not? How probably? We’ve done a few lectures on probability, so we can probably describe the distribution somehow. What is the probability that the sample mean is within one standard deviation of the population mean? Two standard deviations?\nBecause of random sampling error,4 every sample is going to have a different mean. We expect most of the sample means to be close to the population mean, with fewer samples resulting in sample means that are further away. In other words, the sample mean should be close to the population mean, but due to sampling error there will be a little bit of a difference.\nThe variation within our sample should be similar to the variation within the population5, and the variance in the population tells us the variance in the sample means. Variation is not something to be afraid of, and sampling errors are not sampling mistakes; we can harness the variability within a sample to draw conclusions about the population!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "href": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "title": "8  Sampling Distributions",
    "section": "8.3 Sampling distribution of the sample mean",
    "text": "8.3 Sampling distribution of the sample mean\nBecause the value of a sample mean is random (since we took a random sample), there’s a probability distribution that describes it. I could just jump to the answer, but it’s best if I build up to it.\nThe app below6 will take a random sample from the population (in this case, normal), then find the mean and add it to a histogram. As you collect more means, the histogram gets more and more data. This simulates taking many many different samples.\n\nlibrary(ggplot2) # if this fails, run install.packages(\"ggplot2\")\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/samplingDist\")\n\nPlay around yourself! Start with \\(n\\) equal to 2 or 3. The sample shows the individual values, but it also shows the sample mean. Notice how the mean is usually closer to the population mean than any of the individual sample values.\nNow, take another sample! Again, the sample mean is closer to the population mean than most of the sampled values. Take more samples. Take 1000 more samples. Notice how the distribution of sample means is bell-shaped, but slightly skinnier than the population.\nRepeat what you did above, but use n = 25 or so. The histogram of sample means is even skinnier now! It’s still centered on the population mean, though!\nThese histograms are approximations to the sampling distribution of the sample mean. If you take an infinite number of samples and calculate the mean for each different sample, you’ll get a distribution of all possible sample means. This is what a sampling distribution is. I’m going to repeat that, since this is often a very difficult topic: the population distribution shows you the probability distribution for all possible individuals, while a sampling distribution shows you the probability distribution for all possible sample means. Each sample has a different mean, the sampling distribution describes many many samples."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#normal-populations",
    "href": "L10-Sampling_Distributions.html#normal-populations",
    "title": "8  Sampling Distributions",
    "section": "8.4 Normal Populations",
    "text": "8.4 Normal Populations\nIf the population is normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then there is some relatively straightforward math7 to show that:\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\nThat is, the distribution of all possible sample means8 is normal with the same mean as the population, but with a smaller standard deviation. Go back to the app and see this for yourself.\n\nExample\nSuppose the population of heights of Canadian women is N(162.3, 7.11).9 We’re going to try and build up some intuition for why the distribution of all means has a smaller variance than the distribution of the population.\n\nThe probability that a randomly chosen woman is taller than 170 cm is \\(P(X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11) = 0.139. So there’s about a 14% chance of finding a woman taller than 170 cm.\n(This is just for example - this part is not often important.10) If we take a sample of n=2 women, what’s the probability that both of them are taller than 170cm? If it’s a truly random sample, then the heights of the two women should be independent and we can just multiply their probabilities.11 This means that there’s approximately \\(0.14*0.14 = 0.0193 = 1.93\\%\\) chance of this. Obviously, if one woman taller than 170 is unlikely, then both women taller than 170 is very unlikely.\nIf we take a sample of \\(n=2\\) women, what’s the probability that their average height is larger than 170? From above, we know that the distribution of the sample mean is \\(N(162.3, 7.11/\\sqrt{2})\\), so we can calculate this probability as \\(P(\\bar X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11/sqrt(2)) = 0.06. This is somewhere in between just one of them being taller than 170cm and both of them being taller than 170.\n\nWhen we took a sample of 2 women, one might have been taller than 170 but one might have been shorter, so the average ends up being less than 170. The sample mean is less variable than the individual values, so it’s less likely to be further away.12\n\n\n\n\n\n\nSummary\n\n\n\nIf you take two values from a normal distribution, the average of those two values is probably closer to the true mean than either of the individual values. If you found the average of 100 observations from a normal distribution, the mean of all of those means is probably even closer to the true mean. If you took 200 observations, the mean of means will be even closer!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "href": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "title": "8  Sampling Distributions",
    "section": "8.5 Non-Normal Populations with Large Sample Size",
    "text": "8.5 Non-Normal Populations with Large Sample Size\nIn the previous example, we saw that a normal population distribution will result in a distribution for all possible sample means that is also normal, but with a smaller variance. If the population isn’t normal, but you have a large enough sample size, the sampling distribution is still normal. It’s kind of amazing, but it seems to work in practice!\nThe app below13 will help you understand this relationship. I use an “Exponential distribution” for the population, but this isn’t a distribution you really need to worry about. All you need to know is that the population clearly isn’t normal.\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/nLarge\")\n\nRegardless of “lambda”14, as n increase, the sampling distribution becomes closer and closer to the normal distribution. By around n=30 or 40,15 they’re basically the same!16\nAgain,\n\\[\n\\text{If }X\\sim N(\\mu, \\sigma)\\text{ and n is ``large'', then }\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\n\\]\nwhere 60 is definitely “large”, 50 is probably “large”, 30 is debatably “large” (depending on what textbook you read), and anything less than 30 is definitely small. I will not test you on the grey areas here.\nThis result has a very special name:\n\n\n\n\n\n\nThe Central Limit Theorem\n\n\n\nGiven a simple random sample of size \\(n\\) (where \\(n\\) is “large”) from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean will follow a \\(N(\\mu, \\sigma/\\sqrt{n})\\) distribution.\n\n\nFor a perfectly normal population, this is true for any \\(n\\). For a population that just a little bit not normal, \\(n\\) must be moderately large. For a very not normal population (e.g. Binomial with \\(p\\) far from 0.5), we need \\(n\\) even larger. Still, as long as the sd of the population is finite, the sampling distribution will be normal for sufficiently large \\(n\\)!\n\nExamples\n\n\n\n\n\n\nThe angle of big toe deformation in 38 patients.\n\nThere’s an outlier, but the sampling distribution would still be normal even for relatively small \\(n\\).\n\nThe number of servings of fruit per day for 74 adolescent girls.\n\nThe distribution is clearly (right or left???) skewed17. This makes sense - the number of fruits can only be as low as 0 and there may be many people who don’t eat a lot of fruit, but there will be a few eating many fruits per day!\nThe skewness of the data implies skewness in the population (assuming this is a good sample). No worries, though, the sampling distribution will still be normal! We just might need a larger sample size in future studies.\n\nThe lengths of 56 perch from a Swedish lake.\n\nThis is clearly a bimodal distribution, indicating that there might be two subgroups in these data.\nThe sampling distribution will still be normal (unimodal), but the mean of this sampling distribution will probably be somewhere in between the two peaks. In other words, it won’t be describing either of the apparent subgroups! No amount of beautiful theorems will ever fix errors in sampling.\nIn this case, we would want to find out why there are two subgroups before trying to say anything about the population distributions. If we actually have two types of fish, it’s better to study them separately!\n\n\n\n\nNon-Normal Population with Small Sample Size\nThis is modelled with the \\(t\\)-distribution, which will be covered later."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "href": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "title": "8  Sampling Distributions",
    "section": "8.6 Very Non-Normal: The Binomial Distribution",
    "text": "8.6 Very Non-Normal: The Binomial Distribution\nHere’s some mild deja-vu:\nYou roll a dice. You find the sample proportion of heads, denoted \\(\\hat p\\).18 Is this proportion exactly equal to the population proportion? Probably not.\nWait, did I just say probably not? How probably? What is the probability that the sample proportion is within one standard deviation of the population proportion? Yeah, we’re back on this again.\n\nAside: The normal approximation to Binomial\nMost textbooks provide the rule: if both np and n(1-p) are larger than 1019, then the normal distribution is a good approximation to the binomial distribution. I prefer to let you see whether these rules make sense. The app below lets you change n and p, and shows a \\(B(n, p)\\) and an \\(N(np, \\sqrt{np(1-p)})\\)20 distribution.\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Apps/normBinom\")\n\nSet n = 20 and find p such that np &lt; 10. Also find p such that n(1-p) &lt; 10. What is the shape of the Binomial distribution in these cases? What do you notice about the normal distribution? Why do both np and n(1-p) need to be greater than 10?21\n\n\nBinomial? More like Binormial!\nIt turns out that, with large \\(n\\) the sampling distribution of \\(p\\) also follows a normal distribution!22 Even though the population distribution isn’t even continuous,23 the normal distribution approximates it well when there are lots of samples.\nFor each sample, the actual proportion that you calculate is variable. You might get 3 heads out of 10 flips one time, then 8 heads out of 10 flips the next. On average, though, you’ll get 5 heads out of 10 flips. Formally, the mean of the sampling distribution of the sample proportion is \\(p\\).24\nThe variance is a little trickier. In the Binomial lecture notes, I said that the variance increases as n increases. However, when we calculate the proportion, we take the number of successes divided by n. According to some math that is not important for this course, this leads to a variance of the sampling distribution of the sample proportion of p(1-p)/n, which means that the standard deviation25 of the sampling distribution is \\(\\sqrt{p(1-p)/n}\\).\nTo recap: The variance of a Binomial distribution is \\(np(1-p)\\). If we take repeated samples from that Binomial distribution and calculate the proportion of sucesses, the variance will be \\(p(1-p)/n\\).26\n\n\nExample\nSuppose I’m rolling a dice 5 times. The probability of exactly 1 one is defined by the Binomial distribution: dbinom(1, size = 5, prob = 1/6) = 0.4.27 The number of ones we expect after rolling many many dice is \\(np = 5/6 =0.8333\\). The variance in the number of ones in 5 rolls is \\(np(1 - p) = 25/36 = 0.69444\\), meaning the standard deviation of the number of ones in 5 rolls is \\(\\sqrt{np(1-p)} = \\sqrt{5/36}\\).\nLet’s use R to roll some dice for us!\n\n# Rolling 1 dice\n# The \"r\" version of functions givexs random values\n# Not on midterm\nrbinom(n = 1, size = 5, prob = 1/6)\n\n[1] 1\n\n\n\n# Rolling 1000 dice\n# Not on midterm\ndice_1000 &lt;- rbinom(n = 1000, size = 5, prob = 1/6)\nmean(dice_1000) # We expect this to be 0.8333\n\n[1] 0.834\n\nvar(dice_1000) # We expect this to be 0.69444\n\n[1] 0.7111552\n\n\nThe results match! For 1000 rolls of the dice, we got close an average of 0.833 ones per sample (note that this is the number of ones, not a probability). The variance in the number of ones per roll for 1000 is close to the 0.69444 that we expected.\nThis is the sampling distribution for the number of 1s out of five rolls. However, we generall deal with the probability of a 1 out of 5 rolls.\n\n\nExampling Distribution\nThe following code is not testable - you are not expected to write anything like this. I’m taking repeated samples from a B(75, 0.4) distribution and calculating the proportion of successes for each sample.\n\nset.seed(4)\nn &lt;- 75\np &lt;- 0.4\n\nbinom_proportions &lt;- c() # empty vector, to be filled later\n\nfor(i in 1:1000){ # repeat this 1000 times:\n    # This is confusing: I'm getting *one* sample of size n,\n    # but R labels the number of samples as n\n    new_sample &lt;- rbinom(n = 1, size = n, prob = p)\n    \n    # Add the proportion of successes to the vector\n    binom_proportions[i] &lt;- new_sample/n\n}\n\nhist(binom_proportions, \n    breaks = 13, # what happens if you make this larger?\n    freq = FALSE) # Divide the heights of bars by the number of obs.\ncurve(dnorm(x, mean = p, sd = sqrt(p*(1-p)/n)), add = TRUE, col = 3, lwd = 3)\n\n\n\n\nCopy and paste the code above into a script file and observe what happens when you increase the number of breaks. Why does this happen?28\nEach sample has a 40% chance to be a success, and this is indeed shown in the sampling distribution. However, due to random sampling, we can get sample proprtions as low as 0.25 and has high as 0.55! Because of randomness, we’ll very rarely (if ever) get a sample value that perfectly matches the population value.\nThe lesson is this: Even if the true population proportion is 0.4, a sample value of, say, 0.5 is still consistent with the true proportion. This is analogous to saying that the true population mean is 162.3, but our random sample had a mean of 165. The sample value is different from the population value, but still close enough (releative to the variance) to be reasonable."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "href": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "title": "8  Sampling Distributions",
    "section": "8.7 Conclusion: Statistics is the Study of Variance",
    "text": "8.7 Conclusion: Statistics is the Study of Variance\nIn both of the sampling distributions above, the mean of the sampling distribution was the mean of the population. The difference between the population and the sampling distribution is the variance. In both sampling distributions, the variance decreases as n increases. If you sample the entire population every time you do a sample, there will be no variance in your estimate!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#self-study-questions",
    "href": "L10-Sampling_Distributions.html#self-study-questions",
    "title": "8  Sampling Distributions",
    "section": "8.8 Self-Study Questions",
    "text": "8.8 Self-Study Questions\n\nWhen do we use \\(N(\\mu, \\sigma/\\sqrt{n})\\) versus \\(N(\\mu, \\sigma)\\)? When do we use \\(N(p, \\sqrt{p(1-p)/n})\\) versus \\(N(np, \\sqrt{np(1-p)})\\)? This distinction is extremely important.\nIf the population is \\(N(2,4)\\) and we take a sample of size 10, explain why \\(\\frac{\\bar X - 2}{4/\\sqrt{10}}\\) follows a standard normal distribution. This is extremely important.\nWhat does it mean for the sample mean to be the same as the population mean? Will they be the same every time you take a sample?\nPlay around with the “normBinom” app shown above. Why is the normal distribution not appropriate when np&lt;10 or n(1-p)&lt;10?\nIn the “Histogram of binom_proportions”, what happens when you increase the number of breaks? What causes this phenomenon?"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#exercises",
    "href": "L10-Sampling_Distributions.html#exercises",
    "title": "8  Sampling Distributions",
    "section": "8.9 Exercises",
    "text": "8.9 Exercises\nFor these questions, suppose we know that Candian womens’ heights are normal with a mean of 163cm and a standard deviation of 7cm\n\nWhat’s the probability of an individual woman, randomly sampled from the population, being taller than 170 cm?\nWhat’s the probability that the sample mean is larger than 170cm if:\n\n\\(n = 9\\)\n\\(n = 16\\)\n\\(n = 25\\)\n\nWhat do you notice about the probabilities in question 2?\nGiven heights are Normal with a mean of 163 and a standard deviation of 7, what sample size do we need to ensure that 95% of potential sample means are within 2cm of the population mean (that is, 2cm on either side of the mean)?\n\n\n\nAnswer to Questions 1 and 2\n\n\n# Question 1\n1 - pnorm(170, mean = 163, sd = 7)\n\n[1] 0.1586553\n\n# Alternate solution:\n# Find the Z-score:\n(170 - 163) / 7 \n\n[1] 1\n\n# A z-score of 1 means that 170 is 1 sd away from the mean\n# Probability of being more than 1sd above the mean:\n1 - pnorm(1) # Can also use the z-table for pnorm(1)\n\n[1] 0.1586553\n\n# Question 2\n1 - pnorm(170, mean = 163, sd = 7/sqrt(9)) # (a)\n\n[1] 0.001349898\n\n1 - pnorm(170, mean = 163, sd = 7/sqrt(16)) # (b)\n\n[1] 3.167124e-05\n\n1 - pnorm(170, mean = 163, sd = 7/sqrt(25)) #(c)\n\n[1] 2.866516e-07\n\n\n\n\n\n\nSolution to Question 3\n\nAs the sample size increases, the variance decreases (since we’re dividing by the square root of \\(n\\)). This means that values further away from the mean are less and less likely.\n\n\n\n\nSolution to Question 4\n\nWe know that the middle 95% of any normal distribution is within 2 standard deviations of its mean. Because of this, we need 2cm to be equal to 2 standard deviations of the sampling distribution. (Well, technically, it should be -qnorm((1 - 0.95)/2) = 1.96.)\nWe know that the sample mean will have a normal distribution with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma/\\sqrt{n}\\), so we’re using \\(\\sigma/\\sqrt{n}\\) as the standard deviation. Because we don’t have the sample size, we don’t have a value to put into qnorm() and we’ll need to do some math ourselves.\nThe two paragraphs above tell us that 2cm is equal to 1.96 standard deviations, and the standard deviation of the sampling distribution (the standard error) is \\(7/\\sqrt{n}\\). Setting these equal, we get \\(2 = 1.96 *7/\\sqrt{n}\\) which means \\(n = (1.96 * 7/2)^2 = 47.0596\\).\nWe’re not done yet! We can’t sample 47.0596 people! We want to be within two standard deviations of the mean, so we want to make sure that when we round, we don’t go above 2cm. Since the variance of the sampling ditribution decreases as \\(n\\) increase, the confidence interval is smaller when \\(n\\) is larger. Because of this, we always want to round up. Even if our answer came to 12.0001, we would round up to 48 to get our final answer.\nA short version of this solution: Using R or the normal table, the middle 95% is within 1.96 standard deviations of the mean. Since we’re dealing with a sampling distribution, the standard deviation is \\(7/\\sqrt{n}\\). Setting these two numbers equal, we get \\(2 = 1.967/\\sqrt{n}\\) and we solve for \\(n\\). We round up, because we want our interval to be less than 2cm on either side and because a larger sample has lower variance."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#crowdsourced-questions",
    "href": "L10-Sampling_Distributions.html#crowdsourced-questions",
    "title": "8  Sampling Distributions",
    "section": "8.10 Crowdsourced Questions",
    "text": "8.10 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nWhat factor primarily contributes to the sampling distribution having a lower variance than the population distribution?\n\nreduction in standard deviation within the sample\nthe wider range of possible values in the population\nutilization of a different statistical methodology for sampling\nsummarization of multiple observations into a single value during sampling\n\n\n\n\nSolution\n\n\nis the correct answer. When creating a sampling distribution, multiple samples from a population are taken to create a statistic for each sample. As more samples are taken, the statistic will represent the true population parameter better. Essentially, as more values are summarized into one, there is a reduced variability in the sampling distribution compared to the population.\n\n\n\n\nLet X represent the heights, in metres, of white birch trees in Saskatchewan. Given that it follows a normal distribution with a mean of 8 and a standard deviation of 5, the heights of a randomly selected group of n=6 trees are measured. Find the probability that the sample mean tree height is at most 10 m.\n\n\n\nSolution\n\nFirst, let’s label all of the information we have been provided:\n\nsample size: \\(n = 6\\)\npopulation mean: \\(\\mu = 8\\)\npopulation stdev: \\(\\sigma = 5\\)\n\nSince there is a sample size of 6 trees, the mean of the sampling distribution will still remain 8 but the standard deviation of the sampling distribution is \\(\\sigma/\\sqrt{n} = 5 / \\sqrt{6} = 2.04\\)\nSince the height can be a maximum of 10m, we are trying to find P(\\(\\bar x\\) &lt; 10). We can find this by standardizing and using the z-table:\nP(x6 &lt; 10) = P(Zx6 &lt; (10-8) / 2.04) = P(Zx6 &lt; 0.98) = 0.8365\nTherefore, in a sample of 6 trees, there is a 83.65% (or 0.8365) chance that a silver birch tree in Saskatchewan is at most 10 m tall.\nIn R:\n\npnorm(10, mean = 8, sd = 5 / sqrt(6))\n\n[1] 0.8364066\n\n\n\n\n\nthe scores on a standardized test are normally distributed with a mean of 75 and a standard deviation of 10.\n\nwhat percentage of students scored above 85?\nif the top 15% of students receive a special award, what is the minimum score required to receive the award?\nif a random sample of 50 students is selected, what is the probability that the sample mean score is greater than 78?\n\n\n\n\nSolution\n\n\nto find the percentage of students who scored above 85, we can use the z-score formula:\n\nwhere: z = x - μ / σ\nx = score\nμ=75 (mean)\nσ=10 (standard deviation)\ncalculating the z-score: z=85−75 / 10 ​=1\nby looking at a normal distribution table, we find that the percentage of students who scored above z or 1 was 15.87%\n\nto find the minimum score required to receive the award (the score corresponding to the top 15%): using the inverse standard normal distribution table or calculator, we find that the z-score corresponding to the top 15% is approximately z=1.04. now, we can use the z-score formula to find the corresponding score: x=μ+z⋅σ\n\nx=75 + 1.04⋅10 = 85.4\n\nfor a random sample of 50 students, we need to use the central limit theorem. since the sample size is large (n &gt; 30), the sample mean will be approximately normally distributed. the mean of the sample distribution will be the same as the population mean (μ=75), and the standard deviation of the sample distribution (σˉ​) will be the population standard deviation divided by the square root of the sample size: σˉ​= ​σ​ / √n = 10 / √50 = 1.41\n\nwe calculate the z-score for x=78:\nz = 78 - 75 / 1.41 = 2.13\nusing the standard normal distribution table, we find that the probability of obtaining a z-score greater than 2.12 is approximately 0.017. therefore, the probability that the sample mean score is greater than 78 is approximately 0.017, or 1.7%.\nIn R:\n\n# a) P(X &lt; q) = pnorm(q), so P(X &gt; q) = 1 - pnorm(q)\n1 - pnorm(85, mean = 75, sd = 10)\n\n[1] 0.1586553\n\n# b) qnorm calculates the value q such that P(X &lt; q) = p, where p is given.\nqnorm(1 - 0.15, mean = 75, sd = 10)\n\n[1] 85.36433\n\n# c)\n1 - pnorm(78, mean = 75, sd = 10 / sqrt(50))\n\n[1] 0.01694743\n\n\n\n\n\nWhich of the following best describes the Central Limit Theorem (CLT)?\n\nThe CLT states that as the sample size increases, the sampling distribution of the sample sum approaches a uniform distribution, regardless of the shape of the population distribution.\nThe CLT asserts that the sampling distribution of the sample mean will be perfectly normal only if the population distribution is normal.\nThe CLT guarantees that for any population with a finite mean and variance, the sampling distribution of the sample mean approaches a normal distribution as the sample size becomes large.\nAccording to the CLT, the mean of the sampling distribution of the sample mean is always equal to the population mean, regardless of the sample size.\n\n\n\n\nSolution\n\nThe correct answer is C) The CLT guarantees that for any population with a finite mean and variance, the sampling distribution of the sample mean approaches a normal distribution as the sample size becomes large."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#footnotes",
    "href": "L10-Sampling_Distributions.html#footnotes",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nRecall: population refers to the population of interest. The population mean is the true mean of the population.↩︎\nIn statistics, error does not mean mistake.↩︎\nAssuming we have a good sample.↩︎\nIf you don’t have access to R right now, try this one.↩︎\nYou’ll probably see it in the next stats course you take.↩︎\ni.e. the sampling distribution of the sample mean↩︎\nNote: these numbers actually come from a sample, and we don’t know that the population is normal. We’re making some massive assumptions here.↩︎\nYou will not need to do something like this on a test.↩︎\nRemember the most important fact from probability: Multiplying probabilities only works when they’re independent.↩︎\nTake a moment and make sure you understand this relationship. Write out a description of it. Call a grandparent and try to explain it to them.↩︎\nOr the same app as before, with population set to Exponential.↩︎\nWhich controls how skewed the population distribution is.↩︎\nI will either ask you questions where n &lt; 30 (non-normal sampling distr.) or n &gt; 50 (normal sampling distr.), nothing in between.↩︎\nAlthough, in this case, the normal approximation is biased, but the bias decreases as n increases and you’re not expected to know these details.↩︎\nAnswer: right↩︎\ni.e. \\(\\hat p\\) = number of success divided by number of trials.↩︎\nOr sometimes 15. Again, I won’t test you on the grey areas.↩︎\nRecall that the mean and sd of a Binomial distribution are np and np(1-p), respectively.↩︎\nAnswers: Skewed; positive probability below 0 or above n; symmetric.↩︎\nAgain, we use the rule of thumb that \\(np&gt;10\\) and \\(n(1-p)&gt;10\\).↩︎\nThis is important.↩︎\nNot n*p, since the proportion of heads is x/n.↩︎\nWhich is simply the square root of the variance.↩︎\nNotice how they’re equal when n = 1. When n=1, we’re just taking individuals from the population and calling each individual a sample.↩︎\nIn other words, 1 successes in 5 trials, where a success is defined as “rolling a one”.↩︎\nHint: What are the possible values of \\(\\hat p\\)?↩︎"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "href": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "title": "9  Welcome to Inference!",
    "section": "9.1 Inference Basics",
    "text": "9.1 Inference Basics\n\nProbability vs. Inference\nIn probability, we have distributions and calculate how likely given values are. In inference, we have a value that came from a distribution and try to determine things about that distribution.\nRecall: Sampling Distributions\n\nIf the population is \\(N(\\mu,\\sigma)\\), the sampling distribution of the sample mean is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nAssuming an SRS, 95% of sample means should be within 2\\(\\sigma/\\sqrt{n}\\) of the population mean.3\n\n\n\nOne Last Silly Example\nSuppose the average heart rate of a population is 70bpm, with a standard deviation of 5bpm. What’s the probability of getting a mean heart rate that’s further than 4bpm away from the mean when using samples of size 9?\nLet’s solve this the same way as before, just for some practice. A population standard deviation of 7 and a sample size of 9 means the standard deviation of the sampling distribution - also known as the standard error, is \\(7/\\sqrt{9}\\) = 2.33.\nWe’re looking for values below 66bpm or above 74bpm in the sampling distribution. Since we know the normal distribution is symmetric, we can just find the probability of a heart rate below 664 and then double it.\nSolution 1: We can find the z-score as \\((x-\\mu)/\\sigma = (70-66)/2.33 \\approx -1.71\\)5, then we can calculate the probability of a Z value less than -3. In R6, this is pnorm(-1.71) = 0.046. Doubling this, there is a 8.72 percent chance of a heart rate below 66bpm.\nSolution 2: Let’s jump straight to R. We know we’re looking for \\(2*P(\\bar X \\le 66)\\) where \\(X\\sim N(70, 7/\\sqrt{9})\\), and this can be plugged in directly: 2 * pnorm(66, mean = 70, sd = 7/sqrt(9)) = 0.0865, or an 8.65%7\n\n\nFlippin’ it: Confidence intervals\nLet’s flip this on it’s head. Suppose you have a sample and calculate the mean and standard deviation. Let’s say you’re looking at the average heart rate of people who have started a treatment for heart issues. We want their heart rates to go to 70bpm but our sample mean is 77bpm with a standard deviation across patients of 5bpm. Is it reasonable to say that this mean came from a population centered at 70bpm? In other words, there’s a difference between 70 and 77, but is the variance high enough that this difference is simply due to random variation?\nInstead of asking “Using known population parameters, what’s the probability that a sample mean is further than 2\\(\\sigma\\) away?”, we can ask “If your sample mean is further than 2\\(\\sigma\\) from a proposed population mean, is it reasonable to say that our sample came from that particular population?”\nNotice the subtle shift - we’re now talking about something that we can do with just a sample. The Sampling Distributions section always assumed that the population mean was known and told us about potential sample means. We’re now shifting our perspective: given a sample mean, what are the potential population values?\nThe basic idea in this lecture is as follows: the sample should be similar to the population but a little bit off. What are the potential values of the population mean that are compatible with what we observed?"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "href": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "title": "9  Welcome to Inference!",
    "section": "9.2 Confidence Intervals",
    "text": "9.2 Confidence Intervals\n\nBackground\nGiven data, we want to make an inference about the population. Since \\(P(\\bar X = \\mu) = 0\\), we can’t just calculate the probability that we have the correct population mean. It’s always going to be 0!\nHowever, we can make guesses based on ranges! With confidence intervals, we create a range around our estimate that (hopefully) contains the true population mean. It won’t contain the true mean every time, but if we do things right, we can quantify our confidence that it does.\nAll CI’s that we learn in this class have the form8: \\[\n\\text{Estimate} \\pm \\text{Margin of Error}\n\\]\n\n\nThe Margin of Error (MoE)\nIf the population is normal with mean \\(\\mu\\) and sd \\(\\sigma\\), then the Margin of Error is\n\\[\nMoE = (z^*)*(\\sigma/\\sqrt{n}) = \\text{Critical Value}*\\text{Standard Error}\n\\]\n\n\\(z^*\\) is a critical value. This is where we get our “confidence” from. This value is always positive.\n\\(\\sigma/\\sqrt{n}\\) is the standard deviation of the sampling distribution, which is also called the Standard Error.\n\n\n\nCritical Values\nIf \\(z^* = \\infty\\), it means that the confidence interval is infinitely wide. That is, we’re 100% confident that the true population mean is in the interval!\nIf \\(z^* = 0\\), it means the CI is just the point estimate. In other words, we’re 0% confident.\nUsually, we choose a confidence level in between 0 and 100. Values of 90%, 95%, or 97.5% are common. These values strike a nice balance between being useful and being less than infinity.\n\n\nCalculating critical values: 0.95%\nIf \\(X\\sim N(\\mu, \\sigma)\\), then the sampling distribution is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nTo make a confidence interval, we want a range of values \\((L, U)\\) such that \\(P(L &lt; \\bar X &lt; U) = 0.95\\).\nThe normal distribution is symmetric. If we want 95% in the middle, then we need 0.025 below L and 0.025 above U. This is equivalent to values such that \\(P(\\bar X &lt; L) = 0.025\\) and \\(P(\\bar X &lt; U) = 0.975\\).\nWe can find a \\(z^*\\) value such that \\(P(Z &lt; -z^*) = 0.025\\), then use the formula \\(x = z\\sigma+\\mu\\). However, since we’re using \\(\\bar X\\) instead (which has a standard deviation of \\(\\sigma/\\sqrt{n}\\)9 instead of \\(\\sigma\\)), this is \\(\\bar x = z^*\\sigma/\\sqrt{n} + \\mu\\).\nWe can do the same with \\(P(Z &lt; z^*) = 0.975\\) and find \\(\\bar x = z^*\\sigma/\\sqrt n + \\mu\\).\n\n\nWhat is \\(z^*\\)?\nFor \\(P(\\bar X &lt; L) = 0.025\\), \\(-z^* = -1.96\\) (almost -2).\nFor \\(P(\\bar X &lt; U) = 0.975\\), \\(z^* = 1.96\\) (almost 2).\nIn other words, it’s symmetric!\n\n\nBuilding a Confidence Interval\nA Confidence Interval is an interval of “reasonable” values for the population mean based on what we got in a sample. In other words, it’s a collection of values for \\(\\mu\\) that are within 2 standard deviations of the sampling distribution, but using the estimated value of the mean.\nFor simplicity, we’re still going to use the population standard deviation to construct this interval. The sample standard deviation will be used later, but it adds a level of complexity that we’re going to ignore for now.\nFor the standard normal distribution, 95% of the distribution is in the interval (-1.96, 1.96)10. Instead of standardizing (calculating the Z-score), we’re going backwards from a Z score to the distribution of \\(\\bar X\\).\nWe use the Z-score formula \\(Z = \\frac{\\bar X-\\mu}{\\sigma/sqrt{n}}\\), which uses \\(\\sigma/\\sqrt{n}\\) instead of \\(\\sigma\\) because \\(\\sigma/\\sqrt{n}\\) is the standard deviation of the distribution of \\(\\bar X\\)11.\nThe lower bound for the interval for \\(Z\\) is -1.96. To un-standardize this, we can rearrange the formula and get \\(\\bar x = z\\sigma/\\sqrt{n} + \\mu\\). However, we’re actually interested in \\(\\mu\\)! Our formula for the lower bound is \\(\\mu = \\bar x-1.96\\sigma/\\sqrt{n}\\). By a similar argument, we can find that the upper bound is \\(\\mu = \\bar x + 1.96\\sigma/\\sqrt{n}\\).\nFor any value of \\(z^*\\), the two ends of the interval can be written in one expression: \\[\n\\bar x\\pm z^*\\sigma/\\sqrt{n}\n\\]\n\n\n\n\n\n\nConfidence Interval for the Population Mean\n\n\n\nThe confidence interval is defined as:\n\\[\n\\text{all values of } \\mu \\text{ that are in the range } \\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\nThis is the middle 95% of the sampling distribution if it were centered at the sample mean. It gives a range of population means that could reasonably have resulted in our observed sample mean.\n\n\n\n\nSome notation: \\(\\alpha\\)\nA \\((1-\\alpha)\\%\\)CI is is defined as \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\nwhere \\(P(Z &lt; z^*) = \\alpha/2\\).\n\nFor a 95%CI, \\(\\alpha = 0.05\\) and \\(\\alpha/2= 0.025\\).\n\n\\(z^*\\) is found by finding the value such that \\(P(Z &lt; z^*) = 0.025\\).\nqnorm(0.025) = -1.96, so \\(z^* = 1.96\\).\n\nFor a 89%CI, \\(\\alpha = 0.11\\) and \\(\\alpha/2 = 0.055\\).\n\nqnorm(0.055) = -1.56, so \\(z^* = 1.6\\).\n\n\n\n\nExample\nFor the heart rates example, suppose we got a sample mean of \\(\\bar x = 77\\) in a sample with 9 participants. Further suppose that we know from a previous study that the standard deviation of heart rates is 7bpm. Construct a 95% CI for the population mean.\nTo do this, we must calculate \\(\\bar x \\pm z^*\\sigma/\\sqrt{n}\\). Let’s gather the values we need.\n\n\\(\\bar x\\) and \\(\\sigma\\) are given.\n\\(\\sqrt{n} = \\sqrt{9} = 3\\).\nThe critical value defines the two values on a standard normal curve such that 95% is in the middle. In other words, we need 2.5% below \\(-z^*\\) and 2.5% above \\(z^*\\). It’s easiest to calculate probabilities of the form \\(P(Z\\le z^*) = 0.025\\), so we’ll go with that.\n\nSince we hav \\(P(Z\\le q) = p\\) and we’re given \\(p\\), we use the qnorm() function12. qnorm(0.025) = -1.96, so \\(z^*=1.96\\).\n\n\nAll together, this means our 95%CI is: \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n} = 77 \\pm 1.96 * 7/\\sqrt{9}\n\\] Using R as a fancy calculator, we get a 95% CI of (72.43, 81.57).\nLet’s take a moment to consider what this means.\n\nIf \\(\\bar x\\) were the true center of the sampling distribution, then the middle 95% would be between 72.43 and 81.57.\n\nIn terms of our empirical rule, 72.43 to 81.57 is within two standard errors of the mean. Again, we’re using standard errors because this is a sampling distribution, i.e. the sd of the means is the population sd divided by the square root of \\(n\\).\n\nWe defined the CI as “all reasonable proposed values of \\(\\mu\\)”. In our original question, we were inquring about whether 70bpm is a reasonable population mean. According to this CI, it is not.\nIf we had used a different confidence level, we would have had a different CI. Try this example again to get the following results:\n\nA 90% CI is (73.16201, 80.83799)13\nAn 89%14 CI is (73.27088, 80.72912)\nAn 85% CI is (73.64109, 80.35891)\nAn 80% CI is (74.00971, 79.99029)\nA 50% CI is (75.42619, 78.57381)\n\n\nNotice how the interval gets smaller as I lower the confidence?\n\nA 0% CI would be a single value. As we’ve seen, there’s a 0% chance of any single value in a normal distribution, and the 0% CI communicates this: We are 0% confident that the sample mean is exactly equal to the population mean.\nA 100% CI would be infinitely wide. That means we would accept any population value as “reasonable”, and we’re 100% confident that the true population mean is in our interval. This is true that we’re 100% confident, but this isn’t useful at all! We must accept risk of failure in order to make progress.15\n\n\n\nInterpretations\n\n\n\n\n\n\nInterpretation of the CI\n\n\n\nOur particular sample mean is almost surely not exactly equal to the population mean. However, 95% of all possible samples will result in a 95% CI that includes the population mean.\n\n\nWhat does that mean? Let’s start by imagining a CI that was actually centered at the population mean. We know that there’s a 95% chance of a value from the middle 95% of a normal distribution, so 95% of sample means are in the middle 95% of the sampling distribution. This of course leaves a 5% chance that any given sample will result in a mean that is unusually far from the population mean - it’s good to remember this as a possiblity!\nNow let’s imagine taking a random sample and getting a mean. That is, we took a random value from the sampling distribution. Now take that 95% interval that’s centered at the population mean, and shift it so it’s centered at our new sample mean. If our sample mean is within the middle 95%, then that interval still contains \\(\\mu\\). If the sample mean is outside the middle 95% of the sampling distribution, then that interval does not include \\(\\mu\\). This will happen 5% of the time, and there’s nothing we can do about that.\nFor the heart rate example, we have a sample that has a mean of 77bpm. If the population were truly normal with a mean of 70 and a sd of 7, then the sampling distribution of \\(\\bar X\\) with samples of size 9 is N(70, \\(7/\\sqrt{9}\\)) with a middle 95% of the true sampling distribution being defined as (65.42675, 74.57325)16. If the sampling distribution is truly N(70, \\(7/\\sqrt{9}\\)), then 95% of the sample means we might get are in the interval (65.42675, 74.57325). To get the CI from data, we take an interval with the same width (74.57325 - 65.42675 = 9.1465) and center it on our sample mean17. Since 95% of the means are withon 2 sd of the population mean, then 95% of the intervals that are centered at \\(\\bar x\\) will contain the population mean.\nTo put this another way: We find the width of the middle 95% of the sampling distribution. If this width were centered at \\(\\mu\\), then 95% of all sample means will be contained in this interval. Instead, we have a single sample mean, so we center it there. We’re not sure whether this sample mean was actually in the interval, so we say that we’re 95% confident that this interval contains the population mean.\nA couple of important notes:\n\nThere is no randomness in a 95% CI. The mean is fixed, the sd is fixed, the population mean is fixed.\nIt is NOT true that “95% of the time, the population mean falls in the CI”.\n\nThis is a classic gotcha. Everything here is fixed. 95% of the intervals we construct will contain the population mean, we’re just not sure if this particular one actually does.\n\nBy the way the CI is constructed, it will contain the population mean 95% of the time. We have no idea whether any particular one does, but 95% of them do.\n\nOn any given day, there’s a 10% chance of rain. However, it either rained yesterday or it didn’t. There’s not a 10% chance that it rained yesterday - it’s either 0% or 100%.\n\n\n\n\nSummary\nIf \\(X\\sim N(\\mu,\\sigma)\\), then a \\((1-alpha)\\%\\)CI is \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\] where \\(P(Z &lt; z^*) = \\alpha/2\\) can be found with qnorm() (or a z-table).\n\nA 95% is based on finding the middle 95% of the sampling distribution, but centering it around \\(\\bar x\\).\n95% of the intervals constructed this way will contain the true population mean.\n\nA given interval has either a 0% chance or a 100% chance\n\nA point of sillyness: This assumes that \\(\\sigma\\) is known."
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#exercises",
    "href": "L12-Intro_to_Inference-CI-pvals.html#exercises",
    "title": "9  Welcome to Inference!",
    "section": "9.3 Exercises",
    "text": "9.3 Exercises\nVerify the following CIs.\n\nA 95% CI when \\(\\bar x = 160\\), \\(X\\sim N(162.3, 7.11)\\) and \\(n=25\\)\nA 99.7% CI when \\(\\bar x = 160\\), \\(X\\sim N(162.3, 7.11)\\) and \\(n=25\\)\nA 95% CI when \\(\\bar x = 160\\), \\(X\\sim N(162.3, 7.11)\\) and \\(n = 36\\)\nA 95% CI when \\(\\bar x = 160\\), \\(\\bar X\\sim N(162.3, 7.11/\\sqrt{49})\\)\n\n\n\nSolution\n\nThe answers to the above are:\n\n\nA1: (157.21, 162.79)\n\n\nA2: (155.78, 164.22)\n\n\nA3: (157.68, 162.32)\n\n\nA4(158.01, 161.99)\n\n\n\n\n\nThe CIs in questions 1-4 all the same sample mean and population sd, but different sample sizes and/or confidence levels (\\(\\alpha\\)). Comment on the width of the intervals and how this relates to the sample size/\\(\\alpha\\) value.\nExplain every part of the following R code, which calculates a 95% CI for the heart rate example: 77 - qnorm((1 - 0.95)/2) * 7 / sqrt(9).\n\nWhy 77, not 70?\nWhy qnorm, not pnorm?\nWhy (1 - 0.95)/2, not just 0.95 or 0.05?\nWhy is it divided by the square root of 9?\nWhy is this the upper limit of the interval?18\n\nSimilarly, explain why qnorm((1 - 0.95)/2, mean = 77, sd = 7/sqrt(9)) will produce the same lower bound as question 5. Write a similar calculation to get the same upper bound.\nExplain why, after we’ve collected our sample, the 95% is not a probability.\nA group of researchers is studying the reaction time of a species of birds to changes in environmental stimuli. They collect a sample of 36 birds and record their reaction times. The sample mean reaction time is found to be 2.5 seconds, and the standard deviation is 0.8 seconds. Calculate a 95% confidence interval for the true mean reaction time of the entire population. Provide the interval and interpret the results in the context of the study.\n\n\n\nSolution\n\n\n2.5 + qnorm((1 - 0.95)/2) * 0.8 / sqrt(36)\n\n[1] 2.238671\n\n2.5 - qnorm((1 - 0.95)/2) * 0.8 / sqrt(36)\n\n[1] 2.761329\n\n\nOur 95% CI is (2.23, 2.76). We are 95% confident that the true population reaction time is in this interval.\n\n\n\nThe CI we calculated in the question above was 0.265 on either side, which leaves a lot of possible values. Suppose we want the width of the 95% CI to be 0.05 seconds on either side (for a total width of 0.1 seconds). We only care about the width, so it doesn’t matter what the sample mean is. We can’t change the population sd or the confidence level, so all that’s left to work with is the sample size. Find \\(n\\) to make the CI half-width equal to 0.05.\n\n\n\nSolution\n\nThe half-width of the interval is also called the Margin of Error, or MoE. We want MoE = 0.05, and we know that MoE = \\(z^*\\sigma/\\sqrt{n}\\). Setting these equal,\n\\(0.05 = 1.96 * 0.8 / sqrt{n} \\implies \\sqrt{n} = 1.96 * 0.8 / 0.05 \\implies n = 31.36^2 = 983.4\\)\nSince the sample size can’t be a fraction, we must round this number. If we round down, we have a smaller sample size and therefore a wider interval. Instead, we must round up! 983.4 is the minimum sample size, so if we want a round number we always have to go up19. The final answer is that we need 984 birbs to get a 95% CI that has a total width of 0.1.\nWithout rounding: (-qnorm(0.025) * 0.8 / 0.05)^2 = 983.4135, and so our answer doesn’t change!"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#crowdsourced-questions",
    "href": "L12-Intro_to_Inference-CI-pvals.html#crowdsourced-questions",
    "title": "9  Welcome to Inference!",
    "section": "9.4 Crowdsourced Questions",
    "text": "9.4 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA survey is conducted to estimate the average number of hours university students spend per week. A 95% confidence interval for the mean number of study hours is calculated to be (6.3, 8.8). What does this confidence interval imply?\n\nThere is a 95% chance that the true mean number of study hours falls between 6.3 and 8.8.\nThe mean number is study hours fall between 6.3 and 8.8 with 95% confidence\n95% of students spend between 6.3 and 8.8 hours studying per week\nThere is a 5% chance that the true mean number of study hours is outside the 6.3 to 8.8 range.\n\n\n\n\nSolution\n\nb is the correct answer - a 95% confidence interval means that if we were to repeat this study multiple times and calculate confidence intervals for each sample study, then around 95% of these intervals would contain the true population mean. We could be 95% confident that the true mean number of study hours falls between 6.3 and 8.8.\n\n\n\nA research study estimates a 95% confidence interval for the mean height of students at Ganton Heights Public High School to be between 160 cm and 170 cm. What does this confidence interval represent over the long run?\n\nThere is a 95% chance that the true mean height of students falls between 160 cm and 170 cm.\n95% of the students in the school have heights between 160 cm and 170 cm.\nIf we were to repeat this study multiple times and calculate a 95% confidence interval each time, 95% of those intervals would contain the true mean height of students.\nThe mean height of students in the school is exactly between 160 cm and 170 cm.\n\n\n\n\nSolution\n\nThe correct answer is (c). The confidence interval represents the long term frequency that intervals generated from independent random samples of fixed size will contain the true population mean. In this situation, a 95% confidence interval represents that if the study was repeated multiple times, 95% of those intervals constructed from random sample statistics would contain the true mean height of students at Ganton Heights Public High School. Option (a) would also be correct, if we were not looking at the long run frequency of this 95% confidence interval.\n\n\n\nConcerning Margin of Error, which of the following statements about critical values is true?\n\n\\(z^*\\) value can only be 1, 2, or 3, as per the Empirical rule\n\\(z^* = 0\\) means that the confidence interval can not be found\n\\(z^* = \\infty\\) means that the true population mean is 100% within the confidence interval\nStandard deviation is different from standard error\n\n\n\n\nSolution\n\nThe correct statement is c).\nEquation for Margin of Error is \\(MoE = (z^*) \\frac{\\sigma}{\\sqrt{n}}\\)\nThe (\\(z^*\\)) represents the critical value which indicates how many standard deviations from the mean the confidence interval lies. A z-score can not be negative therefore it can not be any integer. A z-score of 0 does not indicate that the confidence interval cant be found, it tells us that the true population mean is definitely not within the confidence interval. As mentioned in Devan Beckers course notes, an infinite z score means that the confidence interval is infinitely wide. That tells us that we would be 100% confident that the true population mean is within the confidence interval.\n\n\n\nWhen building confidence intervals based on sample standard deviations, why is the t-distribution used rather than the standard normal distribution?\n\nBecause every confidence interval is based on the sample standard deviation and changes slightly. Therefore it needs a distribution that takes variability into account.\nThe t-distribution is not used in real-world computations; it is only utilized in theoretical ones.\nBecause the smaller intervals are provided by the t-distribution than by the conventional normal distribution.\nBecause the confidence interval construction is not appropriate for the typical normal distribution.\n\n\n\n\nSolution\n\nA; since confidence intervals slightly change due to the variability in the sample data, the t-distribution is used when creating confidence intervals based on sample standard deviations."
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "href": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "title": "9  Welcome to Inference!",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThis is using the empirical rule - the actual value is closer to 1.96.↩︎\nWe prefer problems with \\(\\le\\) in them.↩︎\nNote that we’re using the standard error, so we divide by the square root of the sample size.↩︎\nYou’re also welcome to try this with the Z-table, but R will be used on exams.↩︎\nThis differs from our previous answer due to rounding - R is always more accurate than rounding yourself and using the Z-table!↩︎\nNote that there are many other kinds of confidence intervals beyond this class! We’re just going to stick to these ones.↩︎\nthe standard error↩︎\nThis is standard notation for intervals: (lower bound, upper bound).↩︎\nTo be clear, the formula is the exact same: it’s (value - mean)/standard deviation, we just need the right values for the mean and standard deviation!↩︎\nYou could also find the closest thing to 0.025 in the body Z-table, if that’s easier for you - but be careful that you’re looking at the body rather than the margins!↩︎\nI used R with no rounding for these - it’s fine if your values are slightly off. Rounding errors will not be distractors on the exam.↩︎\nWhy do I use 89 in so many examples? The 95% CI is standard, but 95 is also chosen completely arbitrarily. Some authors have argued that 89% is less affected by outliers, and if we’re going with an arbitrary number then it might as well be a prime number!↩︎\nSorry for the dad advice (aka dadvice).↩︎\nThis is a CI centered at the population mean - try it yourself!↩︎\nVerify that the width of the CI above is 9.1465↩︎\nHint: run qnorm((1-0.95)/2) yourself, then explain why it’s negative.↩︎\nEven if we got 983.0001, we would round up!↩︎"
  },
  {
    "objectID": "L13-p_vals.html#overview-of-tests-of-significance",
    "href": "L13-p_vals.html#overview-of-tests-of-significance",
    "title": "10  Tests of Significance",
    "section": "10.1 Overview of Tests of Significance",
    "text": "10.1 Overview of Tests of Significance\n\nPhilosophy\n\nWe start with a “null” hypothesis, \\(H_0\\), which states that nothing “interesting” is going on.\n\nThe mean is exactly what we guessed, \\(H_0: \\mu = \\mu_0\\)1\nThe effect of the drug is the same in both groups.\nSomething something “same as” something something.\n\nWe have an alternative hypothesis - things are different.\n\n\\(H_A: \\mu &gt; \\mu_0\\) (or \\(&lt;\\), or \\(\\ne\\))\n\nWe do our study and get our mean (for now, assume \\(\\sigma\\) known)\nWe check if our observed mean is “too unlikely” under the null.\n\nIf the null hypothesis is true, is our observed mean preposterous?\nThis is where the dreaded p-value comes in.\n\nWe make a decision - reject or don’t reject \\(H_0\\) - based on our p-value.\n\nTo summarize: We make a “guess” about the population. We collect data, and we determine whether or not our data is compatible with our guess. If it isn’t, then it’s the guess that must be wrong; not the data2.\nThe assumptions are the same as the assumptions for CIs:\n\nNormal population (or large sample size)\n\\(\\sigma\\) known\n\nWe will get away from this assumption later; for now it’s nice to ease into the concepts.\n\nSimple Random Sample (Independent Observations)"
  },
  {
    "objectID": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "href": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "title": "10  Tests of Significance",
    "section": "10.2 p-value by Example: Trailmaking Test for Fatigue",
    "text": "10.2 p-value by Example: Trailmaking Test for Fatigue\nThe following image shows the output of a “trailmaking” app. Subjects are shown the numbers on a touch screen and are tasked with drawing a line3 starting at 1, then 2, and so on without touching the other numbers. The time is recorded.\n\nIn my research, this app was given to aerial forest fire fighters. Flying a plane is a very challenging task to begin with, made much more challenging when there’s an active fire! The hypothesis is that pilots are measurably fatigued after a fire. However, this hypothesis must be converted into a mathematical construct that we can do something with!\nPilots perform the test many times before a long flight and once after. In samples from the aerial firefighters who were non-fatigued, it was found that completion time follows a normal distribution with mean 15 seconds and standard deviation 1.2 seconds4. We hypothesize that it took longer than that after the flight. \\[\\begin{align*}\nH_0: \\mu &= 15\\\\\nH_A: \\mu &&gt; 15\n\\end{align*}\\] The hypotheses above are created entirely based on the research question. We can (must) write the hypothesis before collecting data. \\(\\bar x\\) does NOT appear in hypotheses. Instead, “15” (\\(\\mu_0\\)) and the “&gt;” come from the hypotheses that fatigued pilots take longer than the population.\n\nResults\nWe caclulated a mean of 15.9 seconds from 16 pilots. Is this slower than 15 seconds? Obviously, these numbers are different, but is this a big difference? To tell whether two numbers are “far apart”, we need some sense of scale. In statistics, scale is given to us in the form of variance.\nThe population standard deviation is given as 1.2 seconds. How many standard deviations away from the hypothesized value is our sample mean? Well, since it’s a SAMPLE MEAN, the standard deviation is \\(1.2/\\sqrt{16} = 0.3\\) (again, this is also called the standard error). Our sample mean of 15.9 is 3 standard deviations5 above the hypothesized means.\nThe p-value for this is the probability of observing a value at least as far from the hypothesized mean, assuming that the hypothesized mean is the true mean6.\nUsing pnorm(), our p-value is P(Z &gt; 3) = 1 - P(Z &lt; 3) = 0.00137. Is our sample mean “unlikely” assuming that the null hypothesis is true?\nThe definition of “unlikely” will generally need to be given in the question. Usually, a significance level of \\(\\alpha = 0.05\\)8 is used9.\nSince our p-value is 0.0013 &lt; 0.05, our observed mean is “too unlikely.” So our hypothesis must be wrong!10 We conclude that the average time to complete the trail has increased, i.e. \\(\\mu &gt; 15\\)11. In this case, we say our result is statistically significant.\n\n\nSummary\nFrom the question, we got our hypotheses:\n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu &gt; 15\n\\end{align*}\\]\nWe caclulated our test statistic12, which is the z-score of our observed mean assuming that the null hypothesis is true:\n\\[ z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}  = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 0.9/0.3 = 3\\]\nWe found up P(Z &gt; \\(z_{obs}\\))13 using 1 - pnorm(3) to get our p-value of 0.0013.\nSince this is a small probability (our p-value is less than our significance value of \\(\\alpha = 0.05\\)), we reject the null hypothesis in favour of the alternative.\nThis is the general approach to hypothesis testing: hypotheisize, calculate, find a normal value, then conclude."
  },
  {
    "objectID": "L13-p_vals.html#two-sided-p-values",
    "href": "L13-p_vals.html#two-sided-p-values",
    "title": "10  Tests of Significance",
    "section": "10.3 Two Sided p-values",
    "text": "10.3 Two Sided p-values\nIf your hypotheses are: \n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu \\ne 15\n\\end{align*}\\]\nthen you’re going to need to change things. In particular, you need to double the p-value for a one-sided test14. This is where the phrase “at least as extreme” comes in - we would reject anything this far away on either side.\nThe following shiny app demonstrates this. In particular, note what happens when you have a two sided alternative hypothesis and you double the wrong tails15.\n\nshiny::runGitHub(repo = \"DB7_OER/TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\nTwo Sided Example\nGiven \\(\\sigma = 2\\), \\(n = 25\\), and \\(\\bar x = 6.6\\), test the hypothesis that the true population mean is not equal to 6 at the 10% level16.\n\\[\\begin{align*}\nH_0: \\mu = 6\\\\\nH_A: \\mu \\ne 6\n\\end{align*}\\]\ntest stat: \\(z_{obs} = \\frac{6.6 - 6}{2/\\sqrt{5}} = \\frac{0.6}{0.4} = 1.5\\)\nFind using R: P(Z &gt; 1.5) = P(Z &lt; -1.5) = pnorm(-1.5) = 0.0668\np-value = 2*0.0668 = 0.1336\nConclude: p &gt; \\(\\alpha\\), therefore do not reject. The p-value is not significant.\n\n\nCritical Values\nFor a two-sided test at the 5% level, what is the largest test statistic that would not be rejected?\nSince it’s a two-sided test at 5%, we would reject anything in the 2.5% area in either tail. Using the Z-table (or qnorm(0.05/2)), this would come from a test-statistic of 1.96. So if our test stat is 1.97, it would have a p-value below 0.05, and if it’s 1.95 it would have a p-value above 0.05.\nIn hypothesis testing, the critical value denotes the point at which z statistics17 are significant. If your z statistic is larger than 1.96, it will be statistically significant at the 5% level (for a two-sided test). This way, we can test significance without even calculating the p-value. Our conclusion will simply be that \\(p&lt;0.05\\), but this is often sufficient - it’s not important if the p-value is 0.044 versus 0.045.18\n\n\nHard Exam-Style Question\n\nA study reported that their two-sided p-value for \\(H_0:\\mu = 0\\) was significant at the 5% level, but not the 1% level.\nThey reported a mean of 10 and a sample size of 36\n\nWhat values could their standard deviation be?\nSolution:\n\nAt the 5% level, \\(z^* = 1.96\\), so:\n\n\\(1.96 = \\frac{x - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{10 - 0}{\\sigma/6}\\)\nRearranging, \\(\\sigma= \\frac{6*10}{1.96} = 30.61\\)\nSanity check: pnorm(10, 0, 30.61/sqrt(36)) = 0.975, as expected.\n\nAt the 1% level, \\(z^*\\) = -qnorm(0.01/2) = 2.576\n\n\\(\\sigma= \\frac{6*10}{2.576} = 23.292\\)\nSanity check: pnorm(10, 0, 23.292/sqrt(36)) = 0.995, as expected.\n\n\nConclusion: The standard deviation is between 23.3 and 30.6.\nIn this example, notice how a smaller standard deviation means a smaller significance level!\n\n\nCI vs. p-value\nRecall the following two facts:\n\nCI: \\(\\mu\\) is in the interval \\(\\bar x \\pm z^*\\sigma/\\sqrt{n}\\)\nTest statistic: \\(z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}\\)\n\nAs homework, rearrange the test statistic equation for \\(\\mu_0\\).\nA new definition of confidence intervals: A \\((1-\\alpha)\\)% CI contains every \\(\\mu_0\\) that would NOT be rejected by a test at the \\(\\alpha\\)% significance level.\nThis is why we don’t say that we “accept” the null hypothesis. There are an infinite number of hypothesis values in the CI - we can’t “accept” them all! Also, our tests only work in reference to the alternate hypothesis. We can only reject/not reject in reference to \\(H_A\\)."
  },
  {
    "objectID": "L13-p_vals.html#self-study-questions",
    "href": "L13-p_vals.html#self-study-questions",
    "title": "10  Tests of Significance",
    "section": "10.4 Self-Study Questions",
    "text": "10.4 Self-Study Questions\n\nExplain the logic behind hypothesis testing in your own words. Make particular reference to the “at least as extreme as” part of the definition of a p-value.\nExplain why p-values are sample statistics.19\nWhat happens if a sample or study design is biased? In particular, suppose that the sample will systematically result in higher values that the population, and we’re testing \\(H_A:\\mu &gt; \\mu_0\\). What happens to the p-value?20\nFor CIs, I was adamant that we cannot speak of the probability that the population mean is inside the interval. We have now learned about the duality of CI and Hypothesis Testing, but we can speak of probability for test statistics21. What gives?22\nSuppose we are testing \\(H_A:\\mu &gt; 10\\) and we get a sample statistic \\(\\bar x = 10\\). What would the p-value for this be?\nFor a one-sided hypothesis test, what does it mean for our p-value to be larger than 0.5? Does this mean we did something wrong?23"
  },
  {
    "objectID": "L13-p_vals.html#exercises",
    "href": "L13-p_vals.html#exercises",
    "title": "10  Tests of Significance",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nSuppose we know that \\(\\sigma = 10\\) and that the population is normally distributed, but we don’t know the mean. We found a sample mean of 150 from a sample of size 25. Test the hypothesis that \\(\\mu_0 &lt; 155\\) at the 5% level.\n\n\n\nSolution\n\n\npnorm(150, mean = 155, sd = 10 / sqrt(25))\n\n[1] 0.006209665\n\n\nSince \\(p&lt;\\alpha\\), we reject the null hypothesis and conclude that the population mean is less than 155.\n\n\n\nA pharmaceutical company is testing a new drug designed to lower cholesterol levels. In a clinical trial with 80 participants, the average reduction in cholesterol is found to be 15 mg/dL and the population standard deviation is known to be 5 mg/dL. The company claims that the drug will reduce cholesterol by at least 12 mg/dL on average. Calculate the p-value for this claim based on the sample data, and determine whether there is sufficient evidence to support the company’s claim at a significance level of 0.05. Interpret the results in the context of the study.\n\n\n\nSolution\n\n\n1 - pnorm(15, mean = 12, sd = 5/sqrt(80))\n\n[1] 4.012556e-08\n\n\nWe reject the null hypothesis that the reduction in cholesterol is exactly 12 mg/dL in favour of the alternative that the reduction in cholesterol is less than 12 mg/dL. In other words, 15 is a statistically significantly larger reduction than 12."
  },
  {
    "objectID": "L13-p_vals.html#crowdsourced-questions",
    "href": "L13-p_vals.html#crowdsourced-questions",
    "title": "10  Tests of Significance",
    "section": "10.6 Crowdsourced Questions",
    "text": "10.6 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA pharmaceutical company asserts that their latest pain relief drug reduces the average duration of headaches by at least 30 minutes. To test this claim, a researcher conducts a study and gathers data from a sample of individuals suffering from headaches who took the medication. The null hypothesis (H0) states that the drug reduces the duration of the headache by exactly 30 minutes, whereas the alternative hypothesis (H1) states that it reduces it by more than 30 minutes. The research conducts a hypothesis test and obtains a p-value of 0.03. What inference should the research make at the significance level of 0.05.\n\nFail to reject the null hypothesis; there is insufficient evidence to conclude that the drug reduces headache duration by 30 minutes on average.\nFail to reject the null hypothesis; there is sufficient evidence to conclude that the drug does not reduce headache duration by 30 minutes on average.\nReject the null hypothesis; there is sufficient evidence to conclude that the drug reduces headache duration by 30 minutes on average.\nReject the null hypothesis; there is insufficient evidence to conclude that the drug reduces headache duration by 30 minutes on average.\n\n\n\n\nSolution\n\nc is the correct answer - since the p-value, 0.03 is less than the significance level, 0.05, the null hypothesis is rejected. This also means that there is sufficient evidence to conclude that the drug does reduce headache by at least 30 minutes on average, supporting the company’s claim.\n\n\n\nA basketball coach refutes the claim that the average player scores 15 points per game. Which hypothesis is used to test the claim?\n\n\\(H_0: \\mu = 15, H_A: \\mu \\ne 15\\)\n\\(H_0: \\mu \\ne 15, H_A: \\mu = 15\\)\n\\(H_0: \\mu &gt; 15, H_A: \\mu \\le 15\\)\n\\(H_0: \\mu \\ge 15, H_A: \\mu &gt; 15\\)\n\n\n\n\nSolution\n\nThe correct answer would be (a). The null hypothesis (\\(H_0\\)) in option (a) states that the average player scores exactly 15 points per game, which is the claim being tested by the basketball coach. The alternative hypothesis (\\(H_A\\)) would then be that the average player does NOT score exactly 15 points, which has been modeled by the HA in option (a). This also means that the player could score less or more than 15 points per game (allowing for a two-tailed test, which has been discussed in our lesson regarding tests of significance).\n\n\n\nA researcher conducts a study to determine if a new dietary supplement has an effect on blood pressure compared to placebo. After a randomized controlled trial, the researcher calculates a p-value of 0.03. If the researcher set the significant at 0.05 for a two-sided test, which of the following is the correct interpretation of the result?\n\nThere is a sufficient evidence to conclude that the supplement significantly increases blood pressure, as the p-value is less than 0.05\nThere is insufficient evidence to conclude that the supplement has an effect on blood pressure, as the p-value is less than 0.05.\nThere is sufficient evidence to conclude that the supplement has a significant effect on blood pressure, but it cannot be determined from the p-value alone if the effect is an increase or decrease. d.The p-value indicated that the supplement has no effect on blood pressure, as it is lower than the significance level\n\n\n\n\nSolution\n\nThe correct answer is C) because in a two-sided hypothesis test, the p-value is used to assess the strength of the evidence against the null hypothesis. A p-value of 0.03 indicated that there is a 3% probability of observing the study results, or more extreme , if the null hypothesis were true. Since this p value is less than the significance level of 0.05, the null hypothesis is rejected, suggesting a significant effect of the supplement on blood pressure. However, because the test is two-sided, the p-value alone does not indicate the direct effect (increase or decrease in blood pressure).\n\n\n\nIn a study investigating the effectiveness of a new teaching method on student performance, a hypothesis test is conducted. The null hypothesis states that the new teaching method has no effect on student performance, while the alternative hypothesis states that the new method improves performance. The significance level (\\(\\alpha\\)) is set to 0.05 for a one-sided test. If the calculated test statistic is 1.8 and the critical value from the Z-table is 1.645, which of the following conclusions is correct?\n\nFail to reject the null hypothesis because the test statistic is less than the critical value, indicating no significant improvement in student performance.\nReject the null hypothesis because the test statistic is greater than the critical value, indicating a significant improvement in student performance.\nFail to reject the null hypothesis because the test statistic is greater than the critical value, suggesting the data are not sufficiently extreme to indicate a significant improvement.\nReject the null hypothesis because the test statistic is less than the critical value, suggesting the new teaching method significantly decreases student performance.\n\n\n\n\nSolution\n\nThe correct answer is (b) Reject the null hypothesis because the test statistic is greater than the critical value, indicating a significant improvement in student performance. In hypothesis testing, if the calculated test statistic is greater than the critical value in a one-sided test (with the alternative hypothesis suggesting an increase), it indicates that the observed data are sufficiently extreme to reject the null hypothesis at the set significance level. Here, since 1.8 (test statistic) &gt; 1.645 (critical value) at \\(\\alpha\\) = 0.05, we have sufficient evidence to reject the null hypothesis and conclude that the new teaching method has a statistically significant positive effect on student performance. The conclusion is based on the premise that higher test statistics compared to the critical value indicate results that are not likely to occur under the null hypothesis.\n\n\n\nDr. Lorna conducts a study to determine if a new drug reduces blood pressure more effectively than the current standard treatment. After a carefully controlled trial, she computes the p-value of the observed effects. Which of the following statements best interprets the meaning of a p-value of 0.03 in the context of this study?\n\nThere is a 3% probability that the new drug is effective in reducing blood pressure.\nThere is a 97% chance that the new drug does not reduce blood pressure more effectively than the current standard treatment.\nThe probability of observing the data, or something more extreme, if the new drug has no better effect than the current standard treatment, is 3%.\nThe new drug reduces blood pressure by 3% more than the current standard treatment.\n\n\n\n\nSolution\n\nThe correct answer is (c) The probability of observing the data, or something more extreme, if the new drug has no better effect than the current standard treatment, is 3%.\nExplanation: The p-value is a measure of the strength of evidence against the null hypothesis. It quantifies the probability of observing the collected data if the null hypothesis were true. In this context, a p-value of 0.03 suggests that there is only a 3% probability of observing this size difference in blood pressure reduction between the new drug and the standard treatment. This low p-value suggests strong evidence against the null hypothesis, indicating that the new drug might indeed be more effective than the current standard treatment. The other options misinterpret the concept and meaning of the p-value.\n\n\n\nThe vitamin C content in tablet supplements of a certain brand is advertised to be an average of 500 mg. A researcher wonders if the actual mean vitamin C content is lower than the advertised value. To investigate, the researcher collected a random sample of 30 tablet supplements from this brand and calculated their average vitamin C content to be 456 mg. Which hypothesis is used to test the claim?\n\n\\(H_0: \\mu = 500, H_A: \\mu &gt; 500\\)\n\\(H_0: \\mu = 500, H_A: \\mu &lt; 500\\)\n$H_0: &lt; 500, H_A: $\n\\(H_0: \\mu \\apporx 500, H_A: \\mu &lt; 500\\)\n\\(H_0: \\mu &lt; 500, H_A: \\mu = 500\\)\n\n\n\n\nSolution\n\nThe correct answer is B: the null hypothesis (H0) asserts that the average vitamin C content in the tablet supplements is 500 mg, which is the claim made by the brand and being tested by the researcher. The alternative hypothesis (Ha) suggests that the average vitamin C content in the tablet supplements is less than 500 mg, which is the (alternative) claim proposed by the researcher."
  },
  {
    "objectID": "L13-p_vals.html#footnotes",
    "href": "L13-p_vals.html#footnotes",
    "title": "10  Tests of Significance",
    "section": "",
    "text": "Notation note: \\(\\mu_0\\) will always refer to the proposed population value.↩︎\nUnless it’s a bad sample/study design↩︎\n“trail”↩︎\nThese numbers actually come from the data of pre-flight trails, but we’re going to treat them as the population for now.↩︎\n15.9 is 3 steps of 0.3 above 15; (15.9 - 15)/0.3 = 3↩︎\nThis is the definition. The description must always include the part about “assuming that the hypothesized value is the true value”↩︎\nWe can only use a standard normal distrubution because the mean of the sampling distribution is assumed to be \\(\\mu_0\\), our hypothesized mean. If this weren’t the case, then we would not get a standard normal distribution and thus we wouldn’t be able to use this method. This is why the “assuming the null is true” bit is important.↩︎\nThe symbol \\(\\alpha\\) refers to the significance level, but also comes up in a \\((1-\\alpha)\\)%CI. Perhaps this is foreshadowing.↩︎\nPlease read the ASA’s statement on p-values. A previous short answer question exam question was based on this.↩︎\nAgain: if our guess is incompatible with our data, then it’s our guess that’s wrong, not the data.↩︎\nNotice how this conclusion brings back the context of the question.↩︎\nLabelled \\(z_{obs}\\).↩︎\nWe used \\(&gt;\\) rather than \\(&lt;\\) because \\(&gt;\\) appears in our alternate hypothesis.↩︎\nIf you do this and find a p-value that is larger than 1, you used the wrong tails!↩︎\nIn the app, it’s denoted “Use absolute value”. This is because you can find \\(P(Z &gt; |z_{obs}|)\\) so that you always get the upper tail↩︎\nThat is, at the \\(\\alpha=0.1\\) significance level.↩︎\n\\(z_{obs}\\)↩︎\nIf we had taken a different sample, we would have gotten a different p-value - p-values have a sampling distribution as well!!!↩︎\nThis implies that p-values have sampling distributions!↩︎\nWhile you’re at it, what happens to the CI?↩︎\n“p-value” is literally short for “Probability Value”.↩︎\nHint: what are we calculating probabilities for?↩︎\nHint: refer to the previous question.↩︎"
  },
  {
    "objectID": "L14-Inference_Cautions.html#interpreting-p-values",
    "href": "L14-Inference_Cautions.html#interpreting-p-values",
    "title": "11  Special Topics in Inference",
    "section": "11.1 Interpreting p-values",
    "text": "11.1 Interpreting p-values\nA p-value is the probability of a result that is at least as extreme as the one we observed, given that the null hypothesis is true.\nIt’s a measure of evidence against the null. We assume that the null is true, then ask how likely our sample would be. There isn’t a problem with our sample, so if it’s unlikely then it must be our assumption that is wrong.\n\n\n\n\n\n\n“Given that the null hypothesis is true”\n\n\n\nAny interpretation of a p-value that does not assume that the null hypothesis is true is a bad interpretation.\n\n\nThe following interpretations are not valid:\n\nThe probability of getting data like this.\nProbability of our data by chance alone.\n\nWould be correct if we made reference to the null\n\nThe probability that the null hypothesis is true\n\nThis is just so wrong, but it unfortunately appears in a lot of published papers.\n\n\nAlways look for wording that assumes that the null is true, and searches for evidence against it."
  },
  {
    "objectID": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "href": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "title": "11  Special Topics in Inference",
    "section": "11.2 Statistical Versus Practical Significance",
    "text": "11.2 Statistical Versus Practical Significance\nSuppose a new drug claims to increase your lifespan significantly. Wow! That sounds great!\nAfter hearing this claim, you dig into the paper that made the claim, and found that the drug increases the average lifespan by 3 hours. How can they claim this was significant???\nThis is the difference between statistical and practical significance. The probability of a result at least as extreme, given that the null hypothesis is true, says nothing of how extreme the results are. A very small effect size can be statistically significant, even if it’s not a noticable change in practice.\nFurthermore, maybe this new drug costs $1,000 per day and has intense nausea as a side-effect. A statistically significant difference says absolutely nothing about the practical effects.\n\n\n\n\n\n\np-values were never meant to be the goal of a study.\n\n\n\nThey are yet another tool in the health researcher’s repertoire, meant to test whether data provide enough evidence against a very particular hypothesis."
  },
  {
    "objectID": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "href": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "title": "11  Special Topics in Inference",
    "section": "11.3 Choosing a Significance Level",
    "text": "11.3 Choosing a Significance Level\nTo build intuition, we start from the extremes:\n\nA significance level of \\(\\alpha = 0\\) will never reject the null hypothesis.\n\nNo amount of evidence will convince you.\n\nA siginficance level of \\(\\alpha = 1\\) will always reject the null hypothesis.\n\nAny evidence will convince you.\n\n\nWhen setting a confidence level, you must consider how much evidence you require. To quote Carl Sagan: “Extraordinary claims require extraordinary evidence.”\nHere are some examples:\n\nA new cancer treatment costs 10 times as much and the patient will never heal back to 100% health. To justify such a procedure, we want to be reeeeeaaaaalllly sure that it works, so we might set a significance level of \\(\\alpha=0.001\\) before we collect any data.\nWe are making a slight change to an advertising strategy that is based on scientific evidence. We’re fairly certain it will work, and the consequences of it not working are very small. A larger significance level, perhaps \\(\\alpha = 0.1\\), might be appropriate.\n\n\\(\\alpha = 0.1\\) is probably the largest significance level you will ever encounter.\n\n👽 Aliens example: There’s an abberation in an image taken by a digital camera. According to the manufacturer, such an abberation would occur in 0.0001% of the pictures. If we think it’s aliens, that’s an extraordinary claim! We’d need some very strong evidence. Do you think that 0.0001% is strong enough? (We’ll return to this later in the chapter.)"
  },
  {
    "objectID": "L14-Inference_Cautions.html#hypothesis-errors",
    "href": "L14-Inference_Cautions.html#hypothesis-errors",
    "title": "11  Special Topics in Inference",
    "section": "11.4 Hypothesis Errors",
    "text": "11.4 Hypothesis Errors\nWhen you test a hypothesis, there are two types of errors: You could reject when the null is true or you could fail to reject when the null is false. The following matrix summarises this:\n\n\n\n\nH_0 is TRUE\nH_0 is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nGood!\n\n\n\nIn other words:\n\nType 1: False Positive\nType 2: False Negative\n\nThere’s another important point here: rejecting the null hypothesis does not mean that it’s actually false! Any number of things might have happened, such as including an outlier or taking a biased sample.\nSimilarly, failing to reject a null does not mean that it’s true. We’ve already talked about this a bit - confidence intervals are all values that would not be rejected by a hypothesis test, so there are many plausible null hypotheses! However, we can also fail to reject the null even though it’s false. This can also happen for multiple reasons:\n\nSample size is too small.\n\nThe distance between the null and the sample mean is calculated relative to the standard error. The standard error decreases with a larger sample, so if our sample isn’t big enough then we might not have collected enough evidence to reject the null, regardless of whether it’s true.\n\nLarge variability in the data.\n\nThis is the other thing that can increase the standard error. With more variation, the distance between the null and the sample mean doesn’t seem as large!\nWe can fix this with better sampling strategies and with better study designs, or by getting a larger sample size!\n\nOur significance level is too high.\n\nThis isn’t really something that we can change after we’ve seen the data.1\n\n\n\nThe Probability of Type 1 Errors\nWhat’s the probability your reject the null, even though it’s true? Let’s say we reject the null if the p-value is, say, less than 5%. This means that any value in the 5% tails of the distribution would lead to us rejecting the null hypothesis - even though it’s true!2 The probability that we do this is 5%, since there’s a 5% chance that we’ll see a value that is “too unlikey” at the 5% level.\nAs usual, I like to demonstrate things via simulation. Here’s the setup:\n\nSet the population parameters as \\(\\mu = 0\\) and \\(\\sigma = 1\\)\nSimulate normal data\nDo a two sided test for \\(H_0: \\mu = 0\\)\n\nNote that this null hypothesis is TRUE\n\nCount how many times we reject the null.\n\n\nset.seed(21); par(mar = c(2,2,1,1)) # unimportant\n## set an empty vector, to be filled with p-values\npvals &lt;- c() \n\nfor(i in 1:10000){ # repeat 10,000 times\n    # Simulate 30 normal values with a population mean of 0 and sd of 1\n    newsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n    # Test whether the population mean is 0\n    newsample_mean &lt;- mean(newsample)\n    newsample_sd &lt;- 1/sqrt(30) # Assuming population sd is known\n    my_z_test &lt;- 2 * (1 - pnorm(abs(newsample_mean), mean = 0, sd = newsample_sd))\n    # record the p-value (the output of t.test has some hidden values)\n    pvals[i] &lt;- my_z_test\n}\n## Testing at the 5% level\nsum(pvals &lt; 0.05) / length(pvals) # should be close to 0.05\n\n[1] 0.0481\n\n\nSince we’re testing at the 5% level, this value is close to 5%! It’s a little tricky to get your head around: If we think 5% is too unlikely, then we reject the null. However, things that have “only” a 5% chance happen about 5% of the time!\nThe histogram below shows all of the p-values we generated. The 5% cutoff isn’t anything special - a test at the 10% level will falsely reject the null 10% of the time. A test at the 90% level will falsely reject the null 90% of the time!\n\n## Fun fact: under the null hyothesis, all p-values are equally likely\n## this fun fact is not relevant to this course.\nhist(pvals, breaks = seq(0, 1, 0.05))\nabline(v = 0.05, col = \"red\", lwd = 3)\n\n\n\n\n\n\nThe Probability of Type 2 Errors\nFor a two-sided test, our hypotheses are: \\[\\begin{align*}\nH_0: \\mu &= \\mu_0\\\\\nH_A: \\mu &\\ne \\mu_0\\\\\n\\end{align*}\\]\nIf the null is actually false3, what’s \\(\\mu\\)? All we know is that it isn’t \\(\\mu_0\\).4 It could be a little above \\(\\mu_0\\), in which case it might be hard to reject \\(\\mu_0\\). It could be a far above \\(\\mu_0\\), in which case it might be easy to reject \\(\\mu_0\\).\n\nEx1: \\(\\mu = 0.001\\), \\(\\sigma = 1\\), and \\(\\mu_0 = 0\\).\n\nHard to reject \\(\\mu_0\\) since it’s so close to \\(\\mu\\) (low power)\nEasy to not reject the false \\(\\mu_0\\) (high probability of type 2 error)\nMost \\(\\bar x\\)’s would be close to \\(\\mu_0\\), relative to the standard error.\n\nEx2: \\(\\mu = 0.001\\), \\(\\sigma = 0.00000001\\), and \\(\\mu_0 = 0\\).\n\nEasy to reject \\(\\mu_0\\) since it’s so far from \\(\\mu\\) (high power)\nHard to not reject the false \\(\\mu_0\\) (low Type 2)\n\n\nPower is our ability to correctly reject a false null hypothesis, and is defined as 1 - P(Type 2 Error)\nNote that these examples are both missing the Standard Error, which incorporates sample size. The power depends on the distance between \\(\\mu\\) and \\(\\mu_0\\) relative to the standard error, not just the population standard deviation. We can partially control the standard error by having a better study design5 and a larger sample size, both of which would give us more power.\n\n\nPower by Simulation (DIY)\nThe following code calculates the power. Run it many times, changing \\(\\mu_0\\), \\(\\sigma\\), and \\(n\\) to see what happens to the power.\n\n## Set parameters\nmu &lt;- 0 # don't change this, but change the other parameters\nmu_0 &lt;- 0.1\nsigma &lt;- 0.5\nn &lt;- 50\n\n## Record p-vals\np_vals &lt;- c()\nfor(i in 1:10000){\n    newsample &lt;- rnorm(n, mu, sigma)\n    p_vals[i] &lt;- 2 * (1 - pnorm(abs(mean(newsample)), 0, sigma/sqrt(n)))\n}\n## The proportion of times the null was (correctly) rejected\nmean(p_vals &lt; 0.05) # Power\n\n[1] 0.0483\n\nmean(p_vals &gt; 0.05) # P(Type 2 Error)\n\n[1] 0.9517"
  },
  {
    "objectID": "L14-Inference_Cautions.html#multiple-comparisons",
    "href": "L14-Inference_Cautions.html#multiple-comparisons",
    "title": "11  Special Topics in Inference",
    "section": "11.5 Multiple Comparisons",
    "text": "11.5 Multiple Comparisons\nSuppose we have a coin that’s heads 5% of the time. What’s the probability of at least one heads in 10 flips?\nAs we saw in previous lectures: P(at least 1 heads in 10 flips) = 1 - P(no heads in 10 flips). We can calculate this in R:\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nWhy did I do go back to flipping coins? Did I forget which chapter I’m in?\nConsider the following problem:\nSuppose you’re testing 10 hypotheses at the 5% level. Assuming all of the null hypotheses are true, what’s the probability that at least one of them is significant?\nSince we’re testing at the 5% level, P(Type 1 Error) = 0.05, so\nP(\\(\\ge\\) 1 rejection in 10 hypotheses) =\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nIn other words, there’s about a 40% chance that we’d get at least one significant result even though all of the null hypotheses are true.6\n\n\n\n\n\n\nThe Multiple Comparisons Problem\n\n\n\nWhen checking more than one hypothesis, the probability of an error increases!\nThis happens for both Type 1 and Type 2 errors, but is especially important for Type 1 errors. If you test \\(n\\) errors at the \\(\\alpha\\)% level, then the probability of a Type 1 error is \\(1 -(1 - \\alpha)^n\\).\n\n\nSo how do we avoid the multiple comparisons problem? There are generally two ways to do it:\n\nSet a Family-Wise Error Rate, rather than an error rate for individual hypothesis tests.\n\nIf you’re going to check 10 p-values, use a smaller cutoff.\nThere are several ways to do this, with the most popular being the Bonferroni correction: for m values, a cutoff of \\(\\alpha/m\\) will result in rejecting at least one test \\(\\alpha\\)% of the time. For example, if you want a test at the 5% level but you’re testing 10 values, you should reject any individual hypothesis only if the p-value is less than \\(\\alpha/m = 0.05/10\\).\n\nOnly check one p-value!\n\nFor most studies, you should have single, well-defined hypothesis. State this hypothesis ahead of time, do all of your data preparation and get it loaded into R, then only test that hypothesis.\nIf you check a second hypothesis, then your significance level is a lie! Testing two true null hypotheses at the 5% level will result in a significant result 9.75% of the time.\n\n\nFailure to account for multiple hypothesis testing is bad science and it’s a path to the dark side. Consider this fantastic tool by fivethirtyeight. Play around with it - by checking a bunch of hypotheses, you can hack your way into finding one that supports your own point of view! In this particular example, your goal is to prove that either (a) the economy does better when a democrat in the white house or (b) the economy does better when a republican is in the white house. Both of these can be demonstrated with a statistically significant result if you check enough hypotheses!\nLet’s return to the 👽 aliens example. We observed an abberation that only happens in 0.0001% of the pictures taken. However, we took thousands of pictures! Even though this event is rare, it had many chances to happen. This is exactly what multiple hypothesis testing is demonstrating: rare events will happen if you give them enough chances! Rejecting the null when it is actually true is a rare event, but it can easily happen if we check a lot of p-values!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#summary",
    "href": "L14-Inference_Cautions.html#summary",
    "title": "11  Special Topics in Inference",
    "section": "11.6 Summary",
    "text": "11.6 Summary\n\nType 1 Error: Reject a true null\n\nProbability is \\(\\alpha\\)\n\nType 2 Error: Fail to reject a false null\n\nProbability depends on the distance between \\(\\mu\\) and \\(\\mu_0\\), relative to the standard error. In more advanced classes, you will calculate this or have something to calculate it for you.\n\nMultiple comparisons problem: The more hypotheses you test, the more likely it is that at least one of them is falsely labelled significant.\n\nTo prevent this, stop checking so many p-values or adjust your expectations!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#self-study-questions",
    "href": "L14-Inference_Cautions.html#self-study-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.7 Self-Study Questions",
    "text": "11.7 Self-Study Questions\n\nWe set up the null hypotheses as “nothing interesting is going on”. In light of this, explain why power is a good thing.\nIf we’re testing 5 hypotheses, what significance level should we use for each such that P(at least one type 1 error) = 0.05?\nIn simple (non-mathy) terms, explain why increasing sample size increases power.\nIf we have the hypotheses \\(H_0:\\mu = 1\\) versus \\(H_0:\\mu = 2\\), we can directly calculate the power. Run the following code to open the Shiny app, and interpret the results.\n\n\nshiny::runGitHub(repo = \"DB7-CourseNotes/TeachingApps\", \n    subdir = \"Tools/SimplePower\")"
  },
  {
    "objectID": "L14-Inference_Cautions.html#participation-questions",
    "href": "L14-Inference_Cautions.html#participation-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.8 Participation Questions",
    "text": "11.8 Participation Questions\n\nQ1\np-values are the probability of observing our data by chance alone.\n\nTrue\nFalse\n\n\n\nQ2\np-values are the probability of getting data at least this extreme under the null hypothesis.\n\nTrue\nFalse\n\n\n\nQ3\np-values are a measure of evidence against the null hypothesis.\n\nTrue\nFalse\n\n\n\nQ4\n\n\n\n\n\\(H_0\\) is TRUE\n\\(H_0\\) is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nHooray!\n\n\n\nIn a particular hypothesis test, rejecting the null means diagnosing a patient with cancer.\nSelect all that are true.\n\nA low significance level means that I require strong evidence before I declare cancer.\nA high power means that I’m more likely to diagnose someone with cancer, assuming they actually have cancer.\nLow type 2 error means that I am less likely to miss true cancer diagnoses.\nThe probability of type 1 error is equal to the significance level \\(\\alpha\\).\n\n\n\nQ5\n\n\n\n\n\\(H_0\\) is TRUE\n\\(H_0\\) is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nHooray!\n\n\n\nIn a particular hypothesis test, rejecting the null means diagnosing a patient with cancer.\nWhich of the following is true about Type 1 and Type 2 error?\n\nIncreasing \\(\\alpha\\) means I’m less likely to diagnose someone with cancer.\nHigh power means I’m more likely to diagnose someone with cancer who actually has cancer.\nThe probability that my diagnosis is correct is 1 - P(Type 2 error).\nA smaller significance level means I’ll have less power."
  },
  {
    "objectID": "L14-Inference_Cautions.html#crowdsourced-questions",
    "href": "L14-Inference_Cautions.html#crowdsourced-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.9 Crowdsourced Questions",
    "text": "11.9 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA type 1 error occurs when?\n\nA null hypothesis is not rejected but should be rejected\nA null hypothesis is rejected but should not be rejected\nA test statistic is incorrect\nNone of the above are correct\n\n\n\n\nSolution\n\n\nis the correct answer – In statistical hypothesis testing, a type 1 error (also known as a “false positive”) refers to the situation where the null hypothesis is incorrectly rejected when it is actually true. This means that the test concludes there is a significant effect or difference when, in reality, there is none.\n\n\n\n\nBottles of water have a label stating that the volume is 14 oz. A ST231 class suspects the bottles are under‐filled and plans to conduct a test. A Type I error in this situation would mean:\n\nThe ST231 class concludes the bottle has less than 14 oz, when the mean actually is 14 oz\nThe ST231 class concludes the bottles has less than 14 oz when the mean actually is less than 14 oz\nThe ST231 class has evidence that the label is incorrect\nA type 1 error does not occur but a Type 2 error does\n\n\n\n\nSolution\n\n\nis the correct answer because it aligns with the definition of a Type I error, which occurs when the null hypothesis (in this case, that the bottles have 14 oz) is incorrectly rejected, leading to the conclusion that the bottles have less than 14 oz when, in fact, they do have 14 oz.\n\n\n\n\nA researcher conducts a hypothesis test with a significance level of 0.05. What does this significance level represent?\n\nThere is a 5% chance of making a Type l error.\nThere is a 5% chance of making a Type ll error.\nThere is a 95% confidence level in the results\nThere is a 95% chance of the null hypothesis being true.\n\n\n\n\nSolution\n\na is the correct answer - the significance level 0.05 meant that there is a 5% chance of committing a Type l error, which means to reject the null hypothesis when it is actually true. The significance level is usually chosen by researchers to control the risk of making a Type l error, it does not directly represent the probability of Type ll error. The significance level does not specifically represent the confidence interval.\n\n\n\nA researcher is designing a study to determine the effectiveness of a new drug in reducing blood pressure. They want to make sure that their study has enough power to detect a clinically meaningful difference in blood pressure if it actually exists. Which of the following actions would increase the statistical power of the study.\n\nIncreasing the sample size\nDecreasing the significance level\nLowering the effect size\nUsing a one-tailed test instead of a two-tailed test\n\n\n\n\nSolution\n\na is the correct answer - increasing the sample size would increase the statistical power. A larger sample size gives more information and reduces the variability in the estimate of the population parameter (mean or the effect size), making it more precise and easier to detect smaller effects. The rest of the options don’t increase the statistical power.\n\n\n\nA new production procedure, according to the corporation, lowers the average product fault rate to less than 1%. To assess this assertion, a statistical test is performed. A defect rate of less than 1% is suggested by the alternative hypothesis (H1), contrary to the null hypothesis (H0), which maintains that the defect rate stays at 1% or higher. The business rejects the null hypothesis after determining that the new manufacturing procedure effectively lowers the defect rate based on its analysis of the data. In this case, what kind of mistake might the corporation have made?\n\nThe company correctly rejected the null hypothesis when the defect rate remains at 1% or higher.\nThe company erroneously rejected the null hypothesis when the defect rate remains at 1% or higher.\nThe company correctly failed to reject the null hypothesis when the defect rate is less than 1%.\nThe company erroneously failed to reject the null hypothesis when the defect rate is less than 1%.\n\n\n\n\nSolution\n\nThe right answer is option (b). In this instance, the corporation made a Type 1 error by mistakenly rejecting the null hypothesis (H0) when it was true (the failure rate is still 1% or greater). Due to a Type 1 error, the business came to the false conclusion that the new manufacturing method effectively lowers the failure rate, which could have led to poor resource allocation and decision-making.\n\n\n\nA team of experts is looking into how a new teaching strategy affects maths students’ performance. In order to identify any significant changes in test results between the old and new teaching approaches, they want to make sure that their study has enough statistical power. Which of the following steps would most effectively boost their study’s statistical power?\n\nIncreasing the sample size of students participating in the study.\nDecreasing the alpha level (significance level) used for hypothesis testing.\nReducing the effect size of the difference in test scores between the teaching methods.\nChanging from a two-tailed test to a one-tailed test.\n\n\n\n\nSolution\n\nA is the right answer. The study’s statistical power would be improved by expanding its sample size. More information is available and the estimate of the population parameter’s variability is decreased with a larger sample size, which facilitates the detection of minor impacts. There would be no direct increase in statistical power with options B, C, and D. While it wouldn’t impact statistical power, lowering the alpha threshold would lessen the possibility of Type I mistakes. Reducing the effect size would make it more difficult to identify differences, and switching from a two-tailed test to a one-tailed test would not automatically result in a higher power.\n\n\n\nA researcher is conducting a study to evaluate the effectiveness of a new diagnostic test for a rare disease. The null hypothesis is that the diagnostic test correctly identifies individuals without the disease, while the alternative hypothesis is that the test incorrectly identifies individuals without the disease as positive. The significance level is set at 0.05.\n\nDescribe Type I and Type II errors in the context of this study.\nWhat are the consequences of making each type of error?\nHow would you reduce the risk of each type of error in this study?\n\n\n\n\nSolution\n\n\nType I Error occurs when the researcher incorrectly rejects the null hypothesis, deciding that the diagnostic test shows the individuals without the disease as positive when, in fact, they do not have the disease. Type II Error occurs when the researcher fails to reject the null hypothesis, concluding that the diagnostic test correctly identifies individuals without the disease when, in reality, they have the disease.\nThe consequences of Type I Error results in unnecessary medical interventions, psychological distress for individuals incorrectly diagnosed as positive, and increased healthcare costs. Type II Error, on the other hand, may result in missed diagnoses, delayed treatment, progression of the disease, and potential harm to patients due to lack of appropriate medical care.\nTo reduce the risk of Type I Error, the researcher can ensure careful validation of the diagnostic test through extensive testing on control groups and ensuring proper calibration to minimize false positives. Additionally, rigorous statistical analysis and replication of results can help verify the test’s accuracy. To minimize the risk of Type II Error, the researcher can increase the sample size to improve statistical power, as a result reducing the likelihood of missing true positives. The researcher can also conduct thorough follow-up examinations or use diagnostic methods to confirm or rule out diagnosis made by the test.\n\n\n\n\nA study is being conducted to evaluate the effectiveness of a new treatment method in treating a specific medical condition. The researcher is considering the sample size for the study. Which of the following statements best describes the relationship between sample size and statistical power in this scenario?\n\nIncreasing the sample size may or may not affect statistical power depending on the effect size.\nIncreasing the sample size will always increase statistical power.\nThere is an inverse relationship between sample size and statistical power.\nIncreasing the sample size generally increases statistical power, but other factors such as effect size may also play a role.\n\n\n\n\nSolution\n\n\nIncreasing the sample size generally increases statistical power, but other factors such as effect size may also play a role. As the sample size increases, so does the power of the significance test. This is because a larger sample size restricts the distribution of the test statistic, meaning that the standard error of the distribution is reduced and the acceptance region is reduced which as a result increases the level of power."
  },
  {
    "objectID": "L14-Inference_Cautions.html#footnotes",
    "href": "L14-Inference_Cautions.html#footnotes",
    "title": "11  Special Topics in Inference",
    "section": "",
    "text": "We can think long and hard about it before seeing the data, but once we see the data we are commited to a particular significance level. Anything else is borderline fraud, depending on the circumstances.↩︎\nRecall: the p-value is the probability of a result that is at least as extreme assuming that the null hypothesis is true!↩︎\nFalse in the population, not just rejected due to a sample.↩︎\nI once saw a bag in a grocery store with a label that said “It’s Not Bacon”. I had no idea what was in that bag. In the video I said it was kale, but that turns out to be false.↩︎\nto reduce \\(s\\)↩︎\nNote that if the hypotheses are all based on the same data then they’re probably not independent.↩︎"
  },
  {
    "objectID": "L15-CI_for_Means.html",
    "href": "L15-CI_for_Means.html",
    "title": "12  Confidence Intervals in Practice",
    "section": "",
    "text": "13 Self-Study Questions\npucks &lt;- stats$Height[stats$Sport == \"Hockey\"]\nmean(pucks)\n\n[1] 165.2128\n\nsd(pucks)\n\n[1] 4.55038\nA plot of the values is below to help with your interpretation:"
  },
  {
    "objectID": "L15-CI_for_Means.html#recap",
    "href": "L15-CI_for_Means.html#recap",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.1 Recap",
    "text": "12.1 Recap\n\nSilly confidence intervals\nIf \\(X\\sim N(\\mu,\\sigma)\\), where \\(\\sigma\\) is known, then a \\((1-\\alpha)\\)CI for \\(\\mu\\) based on \\(\\bar x\\) is: \\[\n\\bar x \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(z^*\\) is found such that \\(P(Z &lt; -z^*) = \\alpha/2\\),\n\nor we could have found \\(z^*\\) such that \\(P(Z &gt; z^*) = \\alpha/2\\),\nor \\(P(Z &lt; z^*) = 1 - \\alpha/2\\),\nor \\(P(Z &gt; -z^*) = 1 - \\alpha/2\\).\n\nA natural question is: why not use \\(s\\), the sample standard deviation?\nTo demonstrate why we can’t just use \\(s\\), I have set up a simulation. I like simulations.\nYou can safely skip the simulations if you’re the type who wants to just memorize a fact and will be sure to perfectly remember it later on. The upshot is this: since we’re estimating the standard deviation, the normal distribution doesn’t apply. Instead we use the \\(t\\) distribution whenever we use \\(s\\).\n\n\nSimulation Setup\n\nTake random values from the standard normal distribution.\nCalculate the mean and sd.\nCalculate the 95% confidence interval with \\(\\sigma\\) and with \\(s\\), both using a \\(z\\) value.\nRecord whether the population mean is in the interval.\nCount how many intervals contain the population mean.\n\nShould be 95% of them!\n\n\nBefore we begin, I want to show some R code for finding confidence intervals. If you’re given that \\(\\bar x = 7.28\\), \\(n=15\\), \\(\\sigma = 1.24\\), and you want to calculate a 95% CI:1\n\nz_star &lt;- abs(qnorm(0.05/2))\nlower_bound &lt;- 7.28 - z_star*1.24/sqrt(15)\nupper_bound &lt;- 7.28 + z_star*1.24/sqrt(15)\nc(lower_bound, upper_bound)\n\n[1] 6.652485 7.907515\n\n\nAlternatively, we can use c(-1, 1) to stand in for “\\(\\pm\\)”. The code is a little weird to get your head around, but trust me - it works!\n\n7.28 + c(-1, 1)*z_star*1.24/sqrt(15)\n\n[1] 6.652485 7.907515\n\n\nSuppose that, unbeknownst to us, the true population mean was 7. To check if this is in our calculated confidence interval, we have to check that it’s larger than the lower bound AND less than the upper bound:\n\n7 &gt; 7.28 - z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n7 &lt; 7.28 + z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n\nThis can be combined into code as follows:\n\n(7 &gt; 7.28 - z_star*1.24/sqrt(15)) & (7 &lt; 7.28 + z_star*1.24/sqrt(15))\n\n[1] TRUE\n\n\nThis is enough to set up the simulation. Basically, we’re going to generate a random data set from a known population, then check if the confidence interval contains the true mean. We’ll do this thousands of times, and check which proportion contain the true mean. We’re hoping it’s 95%!\n\n\nSimulation Code\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05 / 2))\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor (i in 1:100000) { # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1) * z_star * pop_sd / sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1) * z_star * samp_sd / sqrt(n)\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.94887\n\nmean(s_does)\n\n[1] 0.92991\n\n\nThe CI based on \\(s\\) only contains \\(\\mu\\) 93% of the time! This is a pretty big discrepancy. What happens when you increase the sample size, n?2\nThe reason for this discrepancy is shown in the next section:"
  },
  {
    "objectID": "L15-CI_for_Means.html#the-variance-has-variance",
    "href": "L15-CI_for_Means.html#the-variance-has-variance",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.2 The Variance has Variance",
    "text": "12.2 The Variance has Variance\nRecall that the Sampling distribution is all possible values of a statistic when sampling from a population. We’ve covered the sampling distribution for the sample mean: Every time you take a sample, you get a different mean. The distribution of these sample means is \\(N(\\mu,\\sigma/\\sqrt{n})\\).\nThe same idea applies to the sample variance! Every time you take a sample, you get a different variance. The sampling distribution is not a normal distribution. In the next section, we’ll demonstrate this fact.\n\nSimulation: sample statistics\nI’m going to generate a bunch of samples from a \\(N(0, 0.2)\\) distribution. I’ll calculate the mean and variance from each distribution, then plot the histogram.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_means &lt;- c()\nsample_vars &lt;- c()\n\nfor (i in 1:100000) {\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    sample_means[i] &lt;- mean(new_sample)\n    sample_vars[i] &lt;- var(new_sample)\n}\n\npar(mfrow = c(1, 2))\nhist(sample_means, breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Means\")\ncurve(dnorm(x, pop_mean, pop_sd / sqrt(n)), add = TRUE,\n    col = 4, lwd = 2)\n## (n-1)s^2/sigma^2 follows a chi-square distribution on\n## n-1 degrees of freedom. If you understand this, you are\n## far too qualified to be taking this course. This fact\n## is outside the scope of the course.\nhist(sample_vars * (n - 1) / (pop_sd^2), breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Vars\")\ncurve(dchisq(x, n - 1), add = TRUE, col = 2, lwd = 2)\n\n\n\n\nAs you can tell from the fact that I knew how to draw the correct curve on the plots, the sampling distributions for the mean and variance are well known. Also, the sampling distribution for the variance is skewed, and therefore cannot be normal!\nWhen we use \\(\\bar x+ z^*s/\\sqrt{n}\\), \\(\\bar x\\) has variance, but so does \\(s\\).3 This is why the CI changes. When we know \\(\\sigma\\), the Margin of Error (MoE) is always the same. When the standard deviation changes for each sample, so does the MoE.\n\n\nSimulation: The Distribution of the Margin of Error\nThe sampling distribution of the Margin of Error is interesting to look at. This section is entirely optional - you just need to know that each sample has a different margin of error.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_MoEs &lt;- c()\nz_star &lt;- abs(qnorm(0.5/2))\n\nfor(i in 1:100000){\n    new_sample &lt;- rnorm(n=n, mean=pop_mean, sd=pop_sd)\n    sample_MoEs[i] &lt;- z_star*sd(new_sample)/sqrt(n)\n}\n\nhist(sample_MoEs, breaks = 25,\n    main = \"Sampling Dist of MoE\")\nabline(v = z_star*pop_sd/sqrt(n), col = 6, lwd = 2)\n\n\n\n\nThe vertical purple line is \\(z^*\\sigma/\\sqrt n\\).4 This is just a re-scaling of the sampling distribution of the sample variance, so it’s also skewed! Furthermore, the average MoE using \\(s\\) is smaller than the MoE using \\(\\sigma\\), even though it’s right-skewed:\n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Average MoE (s)\" = mean(sample_MoEs))\n\n    MoE (sigma) Average MoE (s) \n     0.04265848      0.04148352 \n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Median MoE (s)\" = median(sample_MoEs))\n\n   MoE (sigma) Median MoE (s) \n    0.04265848     0.04104300 \n\n\nThis is why the CI using \\(s\\) doesn’t capture the true mean as often - it’s giving us smaller intervals!"
  },
  {
    "objectID": "L15-CI_for_Means.html#removing-the-silliness",
    "href": "L15-CI_for_Means.html#removing-the-silliness",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.3 Removing the Silliness",
    "text": "12.3 Removing the Silliness\nThe distribution of the sample variance is not important.5 Instead, we care about the confidence intervals.\nI’m going to write this yet again: since \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\)), \\[\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\n\\] That is, you take the sample means, subtract the mean of the means, and divide by the standard error6, and you get a standard normal distribution.7\nOn the other hand, if we use \\(s\\) (which has it’s own variance), \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\n\\] where \\(n-1\\) is the degrees of freedom (or df).8 This is called the \\(t\\) distribution, and is a lot like the normal distribution but it has higher variance.\nBefore we move on, notice how the formula with \\(\\sigma\\) results in N(0,1), which does not require any information for our sample. In the \\(t\\) distribution, we need to know the sample size!\n\nThe t distribution\nThere are two main features of the \\(t\\) distribution that I want you to know:\n\nIt’s centered at 0, just like N(0,1).\nIt’s more variable than the normal distribution.\n\nThe second point is demonstrated in the following plot:\n\n\n\n\n\nThe red line corresponds to a sample size of 2.9 As the colours move through red to blue, we increase the sample size. At \\(df = \\infty\\), the \\(t\\) distribution is exactly the same as the N(0,1) distribution. For anything smaller, the \\(t\\) distribution puts more probability in the tails.\nThis shows up in the critical values:\n\nabs(qnorm(0.05/2)) # z^*\n\n[1] 1.959964\n\nabs(qt(0.05/2, df = 15 - 1)) # t^* n = 15\n\n[1] 2.144787\n\nabs(qt(0.05/2, df = 30 - 1)) # n = 30\n\n[1] 2.04523\n\nabs(qt(0.05/2, df = 50 - 1)) # n = 50\n\n[1] 2.009575\n\n\nNote that, just like how qbinom finds the value such of a binomial distribution such that 0.025% of the distribution is to the left and qnorm finds the z-values such that 0.025 is to the left, qt10 finds the t-value.\n\nn_seq &lt;- seq(2, 100, by = 2)\nt_seq &lt;- abs(qt(0.05/2, df = n_seq-1))\nplot(n_seq, t_seq, type = \"b\",\n    ylab = \"abs(qt(0.05/2, df = n - 1))\",\n    xlab = \"n\",\n    # the code for the title is not important.\n    main = bquote(\"As df -&gt; infinity, t\"^\"*\"*\" -&gt; z\"^\"*\"))\nabline(h = abs(qnorm(0.05/2)), col = 3, lwd = 2)\n## this code just puts a label on the axis - not important\naxis(2, abs(qnorm(0.05/2)), \"z*\", col = 3, font = 2, col.axis = 3)\n\n\n\n\nSince there’s more probability in the tails, you have to go further out to find the point such that 0.025 of the distribution is to the left.11 The \\(t\\) distribution allows for more variance due to the variance of \\(s\\), and it does this by having larger critical values.\n\n\nThe \\(t\\)-distribution\nThe \\(t\\) distribution has higher variance than the Normal distribution due to the extra uncertainty in estimating \\(s\\)."
  },
  {
    "objectID": "L15-CI_for_Means.html#the-t-confidence-interval",
    "href": "L15-CI_for_Means.html#the-t-confidence-interval",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.4 The \\(t\\) Confidence Interval",
    "text": "12.4 The \\(t\\) Confidence Interval\nNow that you understand the reasoning behind using wider confidence intervals, I can show you the formula/ \\[\n\\bar x \\pm t_{n-1}^*s/\\sqrt{n}\n\\]\nwhere \\(t^*_{n-1}\\) comes from abs(qt(alpha/2, df = n-1)).12\nThis has the same interpretation as the Z CI: 95% of the intervals constructed this way will contain the true population mean. This does NOT mean that there’s a 95% chance that the interval contains the true mean.\nWhat’s that? Of course, I can demonstrate by simulation! Thanks for asking! The following code is copied and pasted from above, only the critical value has been changed.\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, n - 1)) # NEW\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor(i in 1:100000){ # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1)*z_star*pop_sd/sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1)*t_star*samp_sd/sqrt(n) # NEW\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.95069\n\nmean(s_does)\n\n[1] 0.95091\n\n\nNow both of them contain the mean 95% of the time!13 The difference between them is that the t CI doesn’t have as much information as the Z CI - the Z CI knows what the population sd is, but the t CI doesn’t. This is kinda magical: using math, we can get the truth with fewer assumptions!"
  },
  {
    "objectID": "L15-CI_for_Means.html#examples",
    "href": "L15-CI_for_Means.html#examples",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.5 Examples",
    "text": "12.5 Examples\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(\\sigma = 0.01\\), find the 92%CI.\n\nThis is a bit of a trick: I gave you \\(\\sigma\\)! This always refers to the population standard deviation, so that’s what it is here. The Z CI can be found with the R code:\n\n\n\n0.4 + c(-1, 1)*abs(qnorm(0.08/2)) * 0.01/sqrt(100)\n\n[1] 0.3982493 0.4017507\n\n\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(s = 0.01\\), will a 92%CI be wider than or smaller than the CI from Example 1?\n\nWe use \\(t\\) to account for the extra variance we have when we estimate \\(s\\). More variance means wider tails! The CI will be wider!\n\n\n\n0.4 + c(-1, 1)*abs(qt(0.08/2, df = 100-1)) * 0.01/sqrt(100)\n\n[1] 0.3982312 0.4017688\n\n\nIt’s only slightly wider. The sample size is large enough that the variance in the estimate is small.14 Try this again with a smaller \\(n\\) and see what happens to the difference!\n\nIf \\(n=16\\) and the 95%CI for \\(\\mu\\) is (10, 15), what’s the variance?\n\nA general form of the CI is \\(\\bar x \\pm t^* s/\\sqrt{n}\\).\n\n\\(\\bar x\\) is in the centre, so \\(\\bar x\\) is 12.5\n\nThe MoE is 2.5, so \\(t^* s/\\sqrt{n} = 2.5\\).\n\n\\(t^*\\) is qt(0.05/2, 16 - 1) = 2.131\n\n\\(2.131s/\\sqrt{16} = 2.5\\), so \\(s = 2.5\\sqrt{16}/2.131 = 4.69\\)\nThe variance is \\(4.69^2 = 21.9961\\)"
  },
  {
    "objectID": "L15-CI_for_Means.html#real-example",
    "href": "L15-CI_for_Means.html#real-example",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.6 Real Example",
    "text": "12.6 Real Example\nThe following heights were collected from Laurier’s athletics page.\n\n# \nft &lt;- c(5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5)\ninch &lt;- c(7, 10, 0, 6, 7, 10, 7, 8, 7, 10, 3, 10, 1, 4, 2, 5, 6, 6, 9, 8, 10, 11)\n\nheights &lt;- 30.48 * ft + 2.54 * inch\nhist(heights)\nabline(v = 162.3, col = \"red\")\n\n\n\n\n\nmean(heights)\n\n[1] 170.7573\n\nlength(heights)\n\n[1] 22\n\n\nAssuming that we know the population standard deviation \\(\\sigma = 7.1\\), we can make a 89% CI:\n\\[\n\\bar x \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\n\\]\n\nmean(heights) + qnorm((1 - 0.89) / 2) * 7.1 / sqrt(length(heights))\n\n[1] 168.338\n\nmean(heights) - qnorm((1 - 0.89) / 2) * 7.1 / sqrt(length(heights))\n\n[1] 173.1765\n\n\nBut is it reasonable to say that the standard deviation of athletic women’s heights is the same as the standard deviation of the heights in the population of all the women?\n\nThe t CI\nIt’s probably silly that we think that the standard deviation for all Canadian women applies here.\nWith a different sample, we would have gotten a different sample standard deviation! We need to account for this extra source of variance while creating the CI - we do this with the t distribution.\n\nmean(heights) + qt((1 - 0.89) / 2, df = length(heights) - 1) * sd(heights) / sqrt(length(heights))\n\n[1] 168.0833\n\nmean(heights) - qt((1 - 0.89) / 2, df = length(heights) - 1) * sd(heights) / sqrt(length(heights))\n\n[1] 173.4313\n\n\nWe’ll rarely have to go through this calculation again in this class. Instead, R does the calculations for us and we do the hard work (interpretations).\n\nt.test(heights, conf.level = 0.89)\n\n\n    One Sample t-test\n\ndata:  heights\nt = 106.57, df = 21, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n 168.0833 173.4313\nsample estimates:\nmean of x \n 170.7573 \n\n\n\n\nBonus - Webscraping\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Read in web pages\nbasketball &lt;- read_html(\"https://laurierathletics.com/sports/womens-basketball/roster\")\nsoccer &lt;- read_html(\"https://laurierathletics.com/sports/wsoc/roster\")\nhockey &lt;- read_html(\"https://laurierathletics.com/sports/whock/roster\")\n\n# Extract the stats tables\nbstats &lt;- html_table(basketball)[[3]]\nsstats &lt;- html_table(soccer)[[3]]\nhstats &lt;- html_table(hockey)[[3]]\n\n# Combine into one data frame\nbstats$Sport &lt;- \"Basketball\"\nhstats$Sport &lt;- \"Hockey\"\nsstats$Sport &lt;- \"Soccer\"\n# There are, like 5 things here that experienced R users might not know.\nstats &lt;- bind_rows(bstats, hstats, sstats) |&gt;\n    select(Number = No., Pos = Pos., Height = Ht.,\n        Year = `Academic Year`, Major, Sport,\n        EligYear = `Elig. Yr.`) |&gt;\n    tidyr::separate(Height, sep = \"-\",\n        into = c(\"ft\", \"inch\")) |&gt;\n    mutate(Height = as.numeric(ft) * 30.48 + as.numeric(inch) * 2.24) |&gt;\n    select(-ft, -inch)\nhead(stats)\n\n# A tibble: 6 × 7\n  Number Pos   Year  Major         Sport      EligYear Height\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;         &lt;int&gt;  &lt;dbl&gt;\n1     14 G     4th   Business      Basketball       NA   168.\n2      9 F     2nd   Kin/Phys. Ed. Basketball       NA   175.\n3     11 F     3rd   Poli Sci      Basketball       NA   183.\n4      4 G     2nd   Philosophy    Basketball       NA   166.\n5      5 G     3rd   Business      Basketball       NA   168.\n6     13 G     2nd   Kin/Phys. Ed. Basketball       NA   175.\n\n\nThe code above gets all of the heights from the hockey, soccer, and basketball teams (thank you Laurier Athletecs for having well-structured wed pages!).\nLet’s take a minute to explore these data:\n\nggplot(stats) +\n    aes(x = Sport, y = Height) +\n    geom_boxplot() +\n    labs(title = \"Heights by sport\")\n\n\n\n\nFor now, let’s focus on the basketball team. The following code finds a 95%CI. It also tests the hypothesis that the true mean height is 0, so we’ll ignore that for now.\n\n# Just the heights of basketball players\nballers &lt;- stats$Height[stats$Sport == \"Basketball\"]\nmean(ballers)\n\n[1] 172.1771\n\nsd(ballers)\n\n[1] 4.641504\n\nlength(ballers)\n\n[1] 14\n\nggplot() +\n    aes(x = ballers) +\n    geom_histogram(bins = 15, fill = \"dodgerblue\", colour = 1) +\n    geom_vline(xintercept = 162.3, colour = \"red\", linewidth = 2)\n\n\n\n\nIn the histogram above, our data look somewhat normal. If this sample is representative of the population, then we might guess that the population15 looks somewhat normal. However, a sample size of 14 is probably too small to make reliable conclusions.\nI put a red line at the height of 163.2, the height of women in the general population. It seems like 163.2 would be not be a reasonable value in this sample, but could it be a reasonable guess at the overall mean? Let’s use a CI and a t-test to find out:\n\nt.test(ballers, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  ballers\nt = 138.8, df = 13, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 169.4972 174.8571\nsample estimates:\nmean of x \n 172.1771 \n\n\nA 95% CI is (169.5, 174.9), which clearly does not contain 163.2. Since we’re 95% confident that the true population mean is in our CI, so 162.3 is unlikely to be the true mean.\nTo test the hypothesis that this is greater than the general population, we still do a t-test. We just have to tell R what kind of test we want to do. As always, we have to set the significance level. Since we’re not skeptical about the alternate hypothesis, let’s set it to 10%. This says that we don’t need particularly strong evidence before we reject the null hypothesis.\nWe are testing whether the basketball players are taller than the general population. In symbols:\n\\[\nH_0: \\mu = 163.2 \\text{ versus } H_A: \\mu &gt; 163.2\n\\]\nThe R code to test this is below:\n\nt.test(ballers, mu = 162.3, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  ballers\nt = 7.9623, df = 13, p-value = 1.177e-06\nalternative hypothesis: true mean is greater than 162.3\n95 percent confidence interval:\n 169.9803      Inf\nsample estimates:\nmean of x \n 172.1771 \n\n\nOur p-value is 0.00000177. This is a very small p-value, which means we have strong evidence against the null. We reject the null hypothesis, and conclude that the average of female varisty basketball players is taller than the general population (an unsurprising result, but nice to have confirmation)."
  },
  {
    "objectID": "L15-CI_for_Means.html#summary",
    "href": "L15-CI_for_Means.html#summary",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.7 Summary",
    "text": "12.7 Summary\nThis lesson could have been two sentences: The sample standard deviation has variance, so each confidence interval based on \\(s\\) is slightly different. To account for this, we use the \\(t\\) distribution. Then again, when someone tells me their name at a party I immediately forget it. Hopefully this long-winded exploration helps you understand why these facts are true and how they’re relevant to the course.\nNote that all of the best practices for inference still apply! We can still get smaller intervals by taking better samples with larger sample sizes, and we still have to be careful to never speak of a calculated confidence interval in terms of chance.\nThe \\(t\\) confidence interval is actually used in practice. We saw some code that calculates the interval for us in this lecture, and we’ll never have to use qt() again! (Except possibly to demonstrate knowledge on tests.)"
  },
  {
    "objectID": "L15-CI_for_Means.html#crowdsourced-questions",
    "href": "L15-CI_for_Means.html#crowdsourced-questions",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.9 Crowdsourced Questions",
    "text": "12.9 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nWhich of the following statements is false about degrees of freedom (df) for a one-sample t-test?\n\ndf = n - 1, where n is the number of items in your sample.\nDegrees of freedom refer to how many values in your data can change without breaking any rules.\nThe higher the degrees of freedom, the more the t-distribution resembles a normal distribution.\nDegrees of freedom always equal the number of items in your dataset.\n\n\n\n\nSolution\n\nD is false because degrees of freedom (df) are calculated based on the sample size minus the number of parameters estimated. For example, in a simple t-test, df = n - 1, where n is the sample size. Degrees of freedom are not simply equal to the number of items but reflect the number of values that are free to vary in the calculation of a statistic, taking into account any constraints such as parameters estimated.\n\n\n\nA survey was carried out to estimate the average daily screen time of university students. A 90% confidence interval for the mean daily screen time was found to be (3 hours, 5 hours). How should this confidence interval be interpreted?\n\nThe true average daily screen time of all university students is between 3 hours and 5 hours.\nThere is a 90% confidence that the average daily screen time for university students is within the 3 to 5 hours range.\n90% of university students have a daily screen time between 3 and 5 hours.\nThere is a 10% chance that the true average daily screen time for university students is not between 3 and 5 hours.\n\n\n\n\nSolution\n\nb is the correct answer - The 90% confidence interval means that if we were to replicate this survey multiple times, each time calculating a new 90% confidence interval for the mean daily screen time from different samples, about 90% of these intervals would be expected to contain the true mean daily screen time for the population. It demonstrates our level of confidence in the interval’s ability to capture the true population parameter, not the distribution of individual screen times.\n\n\n\nSuppose a random sample of 100 students is taken from a large university. The average number of hours these students spend studying per week is found to be 15 hours, with a standard deviation of 3 hours. Construct a 95% confidence interval for the mean number of hours all students at the university spend studying per week.\n\n\n\nSolution\n\nTo construct a confidence interval for the population mean, we’ll use the formula:\n- Confidence interval = Sample mean ± Margin of error\nwhere the margin of error is calculated as:\n- Margin of error = Z × standard deviation / sqrt(Sample size) ​\nFor a 95% confidence interval, the critical value Z is 1.96 (based on the standard normal distribution).\nPlugging in the values:\nMargin of error = 1.96 × 3 / sqrt(100) = 1.96 × 3/10 = 0.588\nSo the confidence interval is:\n15 + 0.588 = 15.588\n15 - 0.588 = 14.412\nIn R:\n\n15 - qnorm((1 - 0.95) / 2) * 3 / sqrt(100)\n\n[1] 15.58799\n\n15 + qnorm((1 - 0.95) / 2) * 3 / sqrt(100)\n\n[1] 14.41201\n\n\n\n\n\nA random sample of 50 students at Laurier have a mean GPA of 3.2, and a standard deviation of 0.5. determine a 95% confidence interval for the population mean GPA of all of the students attending Laurier.\n\n\n\nSolution\n\nCI = Sample Mean +/- Margin of Error\nMargin of Error = Z * sd/sqrt(n)\nUsing r or a z table we find that the Z value for a 95% confidence interval is 1.96\nMargin of Error = 1.96 * 0.5/sqrt(50)\nMargin of Error = 0.138\nNow we can Calculate the 95% confidence interval\n(3.2-0.138, 3.2+0.138)\nCI: (3.062, 3.338)\nTherefore, the population mean will fall into the range from 3.062 to 3.338 with 95% confidence.\nIn R:\n\n3.2 + qnorm(((1 - 0.95) / 2)) * 0.5 / sqrt(50)\n\n[1] 3.06141\n\n3.2 - qnorm(((1 - 0.95) / 2)) * 0.5 / sqrt(50)\n\n[1] 3.33859"
  },
  {
    "objectID": "L15-CI_for_Means.html#footnotes",
    "href": "L15-CI_for_Means.html#footnotes",
    "title": "12  Confidence Intervals in Practice",
    "section": "",
    "text": "You’ll need to do this sort of thing on a test/assignment.↩︎\nRe-run the code and try it!↩︎\nBoth are random variables.↩︎\nRecall that this never changes since \\(\\sigma\\) is fixed.↩︎\nAnd very complicated.↩︎\nthe standard deviation of the sampling distribution↩︎\nThe word “standard” shows up way too much. Statisticians are bad at naming things.↩︎\nThis is another example of statisticians being bad at naming things.↩︎\nThe degrees of freedom is \\(n-1\\).↩︎\nThe person reading this is a cutie.↩︎\nTry this for other \\(\\alpha\\) values and larger \\(n\\).↩︎\nNote: I’m not even going to bother writing out the \\(P()\\) notation for \\(t^*_{n-1}\\) because you’ll never use it. You’ll only ever need to find \\(t^*_{n-1}\\) in this course.↩︎\nThis means it’s working!↩︎\nRecall: For both the sample mean and the sample proportion, the variance of the sampling distribution decreases as \\(n\\) increases.↩︎\nIn this example, the population refers to the population of female varsity basketball players.↩︎"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "href": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "title": "13  t-Tests for a Mean",
    "section": "13.1 When we use \\(s\\), we use \\(t\\)",
    "text": "13.1 When we use \\(s\\), we use \\(t\\)\nWe’ve been over this in confidence intervals, and the same thing applies to hypothesis tests! If the population is normal (or the sample size is large enough) and we have an SRS, then \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\]\nAgain, the \\(t\\) distribution is used to account for the extra variability from the estimated standard deviation.1\nThis means our test statistic is \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\]\nSince this is a \\(t\\) distibution, we use pt(t_obs, df = n -1), possibly one minus and/or double, depending on the alternate hypothesis.2\nThat’s it. That’s the big difference. When we estimate the standard deviation, we use the t-distribution.\n\n\n\n\n\n\nThe t-test for a population mean\n\n\n\nGiven a sample mean \\(\\bar x\\) and a sample standard deviation \\(s\\), our test statistic is: \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\] Our hypotheses and calculations of the p-value work the same as they did for the z-test."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#examples",
    "href": "L16-Hypothesis_Tests_for_Means.html#examples",
    "title": "13  t-Tests for a Mean",
    "section": "13.2 Examples",
    "text": "13.2 Examples\n\nPilot Fatigue\nIn the pilot fatigue example from the Understanding p-values lecture, we assumed that we had the population sd. I lied - it was actually a sample statistic! We should have used a t-test, not a z test.\nRecall:\n\n\\(H_0: \\mu = 15\\) versus \\(H_A: \\mu &gt; 15\\) with\n\\(n = 16\\), \\(\\bar x = 15.9\\), \\(s = 1.2\\) (not \\(\\sigma\\)) \\[\nt_{obs} = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 3\n\\]\n\nUsing the \\(t\\) distribution, our p-value is:\n\n1 - pt(3, df = 16 - 1)\n\n[1] 0.004486369\n\n\nThis is larger than our previous p-value of 0.0013. This will always be the case: if the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\n\n\n\n\n\n\np-values from a t-test are larger than a z-test (if you have \\(\\sigma=s\\))\n\n\n\nWe almost never know the population standard deviation, so we have extra uncertainty. With extra uncertainty, we require more evidence! Recall that a p-value is a measure of evidence against a null."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "href": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "title": "13  t-Tests for a Mean",
    "section": "13.3 Matched Pairs",
    "text": "13.3 Matched Pairs\nA matched pairs design allows us to use a one-sample t-test when it looks like we have two samples3. Since the pairs are matched, we can calculate the differences between pairs and treat this like a single vector of observations. It is honkey tonk ridonkulous to say that we know the true population standard deviation for the difference in observations, so a \\(z\\) test could never be appropriate.\nConsider the following example of a matched pairs experiment. Given a sample of brave volounteers, we create a small cut on both hands and put ointment on one of the two cuts4. This study design eliminates the variation in healing times for different people since both cuts are on the same person! For each individual, we observe a difference. That is, one observation per person!\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Ointment\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\nNote: Differences were calculated as “With minus Without”! This will be important for setting up the alternative hypothesis later.\nThe important thing here is that last row of this table now represents our data - we can forget that the other two rows exist! In other words, we have one observation per person, rather than two sets of observations.\nThis is where the assumption that we know the population standard deviation is especially preposterous: we’re looking just at the differences! Even if there’s a true value of the sd for healing time for all people, the standard deviation of the difference between healing times isn’t a reasonable quantity to speak of.\nSince we’re looking at the difference, we no longer have a hypothesized value of \\(\\mu_0\\). Instead, we hypothesize that the average pairwise difference is 0, i.e. \\(\\mu_{with\\; minus\\; without} = \\mu_{diff} = 0\\)5. The alternative is “with” &lt; “without”, i.e. \\(\\mu_{diff} &lt; 0\\).6\n\nx &lt;- c(-0.78, 0.01, -0.33, -0.7, -0.2, 0.61, -0.87, -2.1)\nxbar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nt_obs &lt;- (xbar - 0)/(s/sqrt(n)) # xbar is with - w/out\n# Notice that we use pt() instead of pnorm()\npt(t_obs, df = n - 1) # Alternative is &lt;\n\n[1] 0.04662624\n\n\nSo our p-value is approximately 0.04. At the 5% level, the null hypothesis would be rejected and we would conclude that the ointment works7. At the 1% level, we would conclude that it doesn’t have a significant effect. This is why it’s important to know the significance level before calculating the p-value - we shouldn’t get to choose whether our results are statistically significant!\n\nt-tests in Practice\nDo you think that researchers in the field are typing test statistics into their calculator? Of course not! We’re finally at the point in this class where the methods are so commonly used that the built-in functions in R can calculate them.\n\nwith_oint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.0, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4  , 6.7, 2.88, 7.8, 6.32)\ndifference &lt;- with_oint - without\nt.test(difference, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.04817\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n         -Inf -0.007063183\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that the output shows a one-sided confidence interval. This isn’t a big leap from what you know: a confidence interval consists of all of the values that would not be rejected by a hypothesis test, and this works for one-sided as well as two-sided alternate hypotheses!\nTo get a two-sided confidence interval, we can either leave alternative at it’s default value or set it to \"two.sided\". We can also change the significance level with the conf.level argument. For an 89%CI:\n\nt.test(difference, alternative = \"two.sided\", conf.level = 0.89)\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.09635\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n -1.04730428 -0.02519572\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that this calculated a two-sided p-value, which is twice what we saw before (and no longer significant at the 5% level!)."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#recap",
    "href": "L16-Hypothesis_Tests_for_Means.html#recap",
    "title": "13  t-Tests for a Mean",
    "section": "13.4 Recap",
    "text": "13.4 Recap\n\nHypothesis Tests in General\n\nDecide on a hypothesis.\n\n\\(H_0: \\mu = \\mu_0\\) versus \\(H_a: \\mu [\\ne,&gt;,\\text{ or }&lt;] \\mu_0\\)\n\nChoose a significance level \\(\\alpha\\).\n\nSmaller leverl = require more evidence to reject the null.\n\nGather data\n\nIndependent observations from same population; random sample.\n\nCalculate the test statistic based on \\(\\bar x\\), \\(s\\), and \\(\\mu_0\\).\n\nSampling distribution is based on the null hypothesis.\n\nCalculate the p-value according to the form of the alternate hypothesis.\n\nIf \\(&lt;\\), then pnorm(z_obs); if \\(&gt;\\), then 1 - pnorm(z_obs); if two sided, double the correct one.\n\n\n\n\nHypothesis Test Example\nNew York is sometimes called “the city that never sleeps”. At the 5% level, do the following data provide evidence that the average New Yorker gets less than 8 hours of sleep per night?\n\n\n\n\\(\\bar x\\)\n\\(s\\)\n\\(n\\)\n\n\n\n\n7.73\n0.77\n25\n\n\n\n\nHypotheses: \\(H_0: \\mu = 8\\), \\(H_a:\\mu &lt; 8\\).\n\\(t_{obs}\\) = \\(\\dfrac{7.73 - 8}{0.77/5} = -1.75\\)\np-value = pt(-1.75, 24) = 0.0464\nConclude: Since p &lt; \\(\\alpha\\), we reject the null hypothesis.\n\nWe have found statistically significant evidence that New Yorkers sleep less than 8 hours per night on average.\n\n\n\n\nConfidence Intervals\n\nChoose a confidence level \\(\\alpha\\).\n\n“100(1-\\(\\alpha\\))%CI\n\nCollect data\n\nIndependent observations from same population; random sample.\n\nFind the critical value \\(t^*_{n-1}\\)\n\nWe will not need \\(z^*\\) again, except possibly as comparison.\n\nCalculate \\(\\bar x \\pm t^*s/\\sqrt{n}\\)\nConclude: 95% of the intervals constructed this way will contain the true population mean.\n\n\n\nConfidence Interval Example\nConstruct a 95% CI for the New York sleep example.\n\n\n\n\\(\\bar x\\)\n\\(s\\)\n\\(n\\)\n\n\n\n\n7.73\n0.77\n25\n\n\n\n\n\\(\\alpha\\) = 0.05\n\\(t^*_{n-1}\\) = qt(0.025, 24) = -2.0639.\n\\(\\bar x \\pm t^*_{n-1}s/\\sqrt{n} = 7.73 \\pm 2.0639*0.77/5 = (7.41, 8.05)\\)\nConclude: we are 95% confident that the true average nights sleep in New York is between 7.41 and 8.05.\n\nThis interval includes 8, so 8 would not be rejected by a hypothesis test?!?!"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#participation-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#participation-questions",
    "title": "13  t-Tests for a Mean",
    "section": "13.5 Participation Questions",
    "text": "13.5 Participation Questions\n\nQ1\nWhat is the standard error?\n\n\\(\\sigma/\\sqrt{n}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\\(\\sqrt{s_1^2/n_1 + s_2^2/n}\\)\nThe standard deviation of the sampling distribution.\n\n\n\nQ2\nWhat is the standard deviation of the sampling distribution?\n\nThe standard deviation of the population divided by the square root of the sample size (\\(\\sigma/\\sqrt{n}\\)).\nThe standard deviation of the value of a sample statistic across all possible samples from the population.\nThe same as the standard deviation of the population.\nThe average distance to the mean of the population.\n\n\n\nQ3\nWhy does the sampling distribution have a lower variance than the population?\n\nBecause the standard deviation is smaller than the variance.\nBecause the population has a larger number of possible, so the variance is smaller.\nBecause outliers are not as likely in a sample.\nBecause we are summarising many observations from a sample into a single value.\n\n\n\nQ4\nAfter conducting a study, we found a p-value of 0.04. Did we find a statistically significant result?\n\nYes, since the p-value is less than 0.05\nNo, since the p-value is less than 0.05\nWe failed to set the significance lavel ahead of time, so we have to be very careful about concluding significance.\n\n\n\nQ5\nAfter conducting a study, we found a 95% confidence interval for \\(\\mu\\) from -0.1 to 1.9. What can we conclude?\n\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu \\ne 0\\) would not be significant at the 5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu &gt; 0\\) would not be significant at the 5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu \\ne 0\\) would not be significant at the 2.5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu &gt; 0\\) would not be significant at the 2.5% level.\n\n\n\nQ6\nUnder which condition does the CLT not apply?\n\nFor \\(\\bar x\\), the sample size is between 3 and 60 but a histogram of the sample appears normal.\nFor \\(\\bar x\\), the sample size is much larger than 60.\nFor \\(\\hat p\\), the sample size is much larger than 60.\nFor \\(\\hat p\\), we have checked \\(np&gt;10\\) and \\(n(1-p)&gt;10\\)\n\n\n\nSolution\n\n41431"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "title": "13  t-Tests for a Mean",
    "section": "13.6 Self-Study Questions",
    "text": "13.6 Self-Study Questions\n\nExplain why: If the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\nIf a test is statistically signficant, does that mean there’s a large effect size? That is, does a hypothesis test tell you anything about the size of the effect?\n\nCompare this to confidence intervals.\n\nCan we interpret a \\(t\\) confidence interval as “all null hypothesis values that would not be rejected”?\nRe-do the ointment example, but using without - with.\n\nDraw a t distribution and mark the two test statistics, then fill in the area that corresponds to the p-value."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#crowdsourced-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#crowdsourced-questions",
    "title": "13  t-Tests for a Mean",
    "section": "13.7 Crowdsourced Questions",
    "text": "13.7 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA researcher believes that the average sleep duration for adults in a certain city is less than the national average of 8 hours. To test this hypothesis, the researcher collects a sample of 50 adults from the city and finds that the mean sleep duration in the sample is 7.5 hours with a standard deviation of 1.2 hours. At a 5% significance level, can the researcher conclude that the average sleep duration for adults in the city is less than 8 hours?\n\n\n\nSolution\n\nSet up the hypotheses:\n\nNull hypothesis (H0​): The average sleep duration is 8 hours (\\(\\mu = 8\\)).\nAlternative hypothesis (H1​): The average sleep duration is less than 8 hours (\\(\\mu &lt; 8\\)).\n\nCalculate the test statistic using the sample mean, population mean, standard deviation, and sample size: $t_{obs} = = = -2.946\nThe p-value can be found using the t-distribution on \\(n-1\\) degrees of freedom:\n\npt(-2.946, df = 50 - 1)\n\n[1] 0.002457447\n\n\nSince this is less than 0.05, we reject the null hypothesis. There is evidence at the 5% level that the true mean sleep duration is less than 8 hours.\n\n\n\nA nutritionist claims that the new diet plan they have designed results in a more significant average weight loss than the generally accepted average of 5 pounds after a 4-week program. To validate this claim, the nutritionist collects data from 40 individuals who followed the diet plan and finds that the mean weight loss among the participants is 5.8 pounds with a standard deviation of 0.9 pounds. At a significance level of 1%, is there enough evidence to support the nutritionist’s claim?\n\n\n\nSolution\n\nNote that this is a matched pairs t-test!\nSet up the hypotheses:\n\nNull hypothesis (H0): The average weight loss is 5 pounds (\\(\\mu = 5\\)).\nAlternative hypothesis (H1): The average weight loss is more than 5 pounds (\\(\\mu &gt; 5\\)).\n\nCalculate the test statistic using the formula for a t-test: \\[\nt_{obs} = \\frac{\\bar x - \\mu_0}{s/\\sqrt{n}} = \\frac{5.8 - 5}{0.9/\\sqrt{40}} = 5.62\n\\]\nUsing the t-distribution on \\(n - 1\\) degrees of freedom:\n\n1 - pt(5.62, df = 40 - 1)\n\n[1] 8.727633e-07\n\n\nWe get a value much smaller than our significance level of 0.01! We reject the null, and conclude that the average weight loss is more than 5 pounds.\n\n\n\nA coffee company has engineered an espresso machine that is advertised to make cups of espresso with an average of 80mg of caffeine. When being sampled, 36 cups of espresso were made and tested for their caffeine content. The sample mean came out to be 82mg with a standard deviation of 6mg. Conduct a 2-sided hypothesis test at a significance level of 5%. State Null hypothesis, Alternate Hypothesis, and significant level. Calculate test statistic and determine whether claims made by the coffee company should be accepted.\n\n\n\nSolution\n\nOur hypotheses are \\(H_0:\\mu = 80\\) versus \\(H_0:\\mu \\ne 80\\).\n\\[\nt_{obs} = \\frac{\\bar x - \\mu_0}{s/\\sqrt{n}} = \\frac{82 - 80}{6 / \\sqrt{36}} = 2\n\\]\nThe p-value can be found as:\n\n2 * (1 - pt(2, df = 6 - 1))\n\n[1] 0.1019395\n\n\nSince our p-value is larger than alpha, we do not reject the null. We have not gathered evidence that the claim of 80mg is incorrect.\n\n\n\nA clinical trial is conducted to compare the mean blood pressure reduction (in mmHg) achieved by a new medication against a standard treatment. The mean reduction for the new medication is calculated with a 95% confidence interval of (2, 8). Which of the following conclusions is most appropriate if testing the null hypothesis that the new medication does not differ from the standard treatment in terms of mean blood pressure reduction?\n\nReject the null hypothesis at the 5% significance level because the confidence interval does not include 0, indicating a significant difference in mean reductions.\nFail to reject the null hypothesis because the confidence interval includes the mean reduction of the standard treatment, indicating no significant difference.\nReject the null hypothesis only if the mean reduction for the standard treatment is outside the interval (2, 8), indicating a significant difference.\nAccept the null hypothesis because confidence intervals are only useful for estimating the range of possible values, not for testing hypotheses.\n\n\nIn addition, what kind of test is this?\n\n\nSolution\n\nThe correct answer is (a) Reject the null hypothesis at the 5% significance level because the confidence interval does not include 0, indicating a significant difference in mean reductions. The 95% confidence interval represents the range of values within which we are 95% confident the true mean difference lies. If the null hypothesis were true (no difference in mean reductions), we would expect the interval to include 0. However, since the interval (2, 8) does not include 0, we have evidence that the mean reduction from the new medication is significantly different from the standard treatment at the 5% significance level. This conclusion is based on the principle that if a 95% confidence interval for a difference does not include 0, the difference is statistically significant at the 5% level. Hence, the confidence interval directly informs the hypothesis test outcome.\nThis is a matched pairs test, since it’s looking at the same patient before and after treatment. Instead of a sample of people before treatment and a sample of people after treatment, the researchers can look at a single sample of the differences.\n\n\n\nA psychology researcher is interested in the effects of a new therapy on reducing anxiety levels. To test the effectiveness of the therapy, the researcher selects a random sample of 15 patients and records their anxiety levels before and after undergoing the therapy. The differences in anxiety level for the patients are as follows:\n\nDifferences 3, 5, -1, 4, 6, 2, 7, 4, 3, 5, 2, 4, 6, 8, 3\nAssuming the differences in anxiety levels are normally distributed, conduct a one-sample t-test to determine if the therapy leads to a significant reduction in anxiety levels at a 5% significance level.\n\nExplain why this is a matched pairs test.\nState the null and alternative hypotheses.\nCalculate the test statistic.\nDetermine the critical t-value.\nConclude whether or not there is sufficient evidence to support that the therapy leads to a significant reduction in anxiety levels.\n\n\n\nSolution\n\nA.) The researchers have a sample of differences, meaning that each value comes from two observations on a natural pairing (e.g. the same individual). This can be done as a one-sample t-test.\nB.) Null hypothesis (\\(H_0\\)​): The therapy has no effect on anxiety levels, so the mean difference in anxiety levels is 0 (\\(\\mu_d​=0\\)).\nAlternative hypothesis (\\(H_A\\)​): The therapy leads to a reduction in anxiety levels, so the mean difference in anxiety levels is greater than 0 (\\(\\mu_d &gt;0\\)).\nC.) The calculated test statistic (t-statistic) for the differences in anxiety levels is approximately 7.00.\nD.) The critical t-value for a one-tailed test at a 5% significance level with 14 degrees of freedom is approximately 1.76.\nE.) Since the calculated t-statistic (7.00) is greater than the critical t-value (1.76), we reject the null hypothesis. There is sufficient evidence to support that the therapy leads to a significant reduction in anxiety levels. This conclusion is based on the assumption that if the therapy had no effect, the likelihood of observing a sample mean difference as extreme as this, or more extreme, is very low under the null hypothesis. Therefore, the therapy appears to be effective in reducing anxiety levels among the patients in this study.\nIn R:\n\nt.test(c(3, 5, -1, 4, 6, 2, 7, 4, 3, 5, 2, 4, 6, 8, 3), alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  c(3, 5, -1, 4, 6, 2, 7, 4, 3, 5, 2, 4, 6, 8, 3)\nt = 6.9972, df = 14, p-value = 3.138e-06\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 3.043017      Inf\nsample estimates:\nmean of x \n 4.066667 \n\n\n\n\n\nA small tech startup is interested in estimating the average number of hours its employees spend on professional development activities per month. Due to the startup’s limited size, a random sample of 10 employees is selected, and the following number of hours spent on professional development activities per month are recorded:\n\nHours: 12,15,9,11,14,8,10,13,12,7\nAssuming the number of hours follows a normal distribution, calculate a 90% confidence interval for the average number of hours all employees at the startup spend on professional development activities per month.\n\nCalculate the sample mean and the sample standard deviation.\nDetermine the critical t-value for a 90% confidence interval.\nConstruct the 90% confidence interval.\nInterpret the confidence interval in the context of the study.\n\n\n\nSolution\n\n\nThe sample mean of hours spent on professional development activities per month is 11.1 hours, and the sample standard deviation is approximately 2.60 hours.\n\nB.) The critical t-value for constructing a 90% confidence interval with 9 degrees of freedom is approximately 1.83.\nC.)The 90% confidence interval for the average number of hours all employees at the startup spend on professional development activities per month is approximately (9.59, 12.61) hours.\nD.) Based on the sample data, we can be 90% confident that the true average number of hours spent on professional development activities by all employees at the startup falls between 9.59 and 12.61 hours per month\nIn R:\n\nhours &lt;- c(12, 15, 9, 11, 14, 8, 10, 13, 12, 7)\nt.test(hours, conf.level = 0.9)\n\n\n    One Sample t-test\n\ndata:  hours\nt = 13.494, df = 9, p-value = 2.818e-07\nalternative hypothesis: true mean is not equal to 0\n90 percent confidence interval:\n  9.592086 12.607914\nsample estimates:\nmean of x \n     11.1"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "href": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "title": "13  t-Tests for a Mean",
    "section": "",
    "text": "Which is used in the caclulation of the Estimated Standard Error.↩︎\nLike pnorm(), it always calculates the probability below the test statistic.↩︎\nWe’ll learn about two-sample t-tests in the next lecture.↩︎\nAnd most likely a bandage on both.↩︎\nIn other words, the healing times are the same for each subject↩︎\nThis is where it’s important to know that we did “with minus without”; we could have done without minus with, but then our alternate hypotheses would need to be “&gt;”.↩︎\nA p-value says nothing about the effect size, so we can’t say whether it’s practically significant↩︎"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "href": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "title": "14  Two-Sample t-Tests",
    "section": "14.1 How much can one more sample complicate things?",
    "text": "14.1 How much can one more sample complicate things?\n\nNotation: Subscripts everywhere!\nWe now have two samples.\n\\(\\bar X_1\\sim N(\\mu_1, \\sigma_1/\\sqrt{n_1})\\), where \\(s_1\\) is the estimated standard deviation of a given sample.\n\\(\\bar X_2\\sim N(\\mu_2, \\sigma_2/\\sqrt{n_2})\\) \nGoal: Are the means the same? I.e., is \\(\\mu_1 = \\mu_2\\)?\nWith two samples, the difference in means has a sampling distribution. What is that distribution? It’s difficult!\nThe easy part is the mean of the difference. The mean of the difference is the difference in means. \\[\n\\bar X_1 - \\bar X_2 \\sim N(\\mu_1 - \\mu_2, ???)\n\\]\nThe hard part is the standard deviation of the difference. Take a moment and think about what we’re talking about here. What does it actually mean for the difference in means to have variance?\nIt’s the same as it was before, it just seems a little more complicated. When we take a pair of samples then find their difference, we have calculated a statistic! For every pair of samples, we’ll get a different statistic. The variance that we seek is the variance of all of these statistics.\n\n\nThe standard deviation of a difference\nAgain, I’m going to use a simulation to demonstrate what happens if we take a bunch of pairs of samples, then find their difference.\nIn this case, I’m sampling x1 as 22 values from a normal distribution with a mean of 0 and a standard deviation of 2, whereas x2 has 33 observations and comes from a distribution with a mean of 0 and a standard deviation of 3. I’m finding their means, then finding their differences. I repeat this 10,000 times, keeping track of what the difference in means was.\n\n## approximating the sampling distribution\ndifferences &lt;- c()\nfor(i in 1:10000){\n    m1 &lt;- mean(rnorm(22, 0, 2))\n    m2 &lt;- mean(rnorm(33, 0, 3))\n    differences[i] &lt;- m1 - m2\n}\nsd(differences)\n\n[1] 0.6802144\n\n\n… it’s not at all obvious where this number comes from.\nBoth had a mean of 0, so the difference should be 0. But the standard deviation of the differences isn’t obvious. There is a nice formula for the mean - \\(\\sigma/\\sqrt{n}\\) - but it’s not obvious how this works for two samples with different variances and different sample sizes!\nIt turns out that the following equation is the correct one for the standard deviation of the differences. Recall that the standard deviation of a sampling distribution is known as the Standard Error (SE).\n\\[\nSE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nAnd to verify that our simulation matches this idea:\n\n## Check\nsd(differences)\n\n[1] 0.6802144\n\nsqrt(4/22 + 9/33) # Close enough\n\n[1] 0.6741999\n\n\n\n\nPutting it Together\nAltogether, this means that the difference between means has the following sampling distribution:\n\\[\n\\bar X_1 - \\bar X_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\right)\n\\]\nFrom this, we get the same general ideas as before. The hypothesis tests are based on the observed test statistic: \\[\nt_{obs} = \\frac{\\text{sample statistic} - \\text{hypothesized value}}{\\text{standard error}} = \\frac{(\\bar x_1 - \\bar x_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] where usually the hypothesized value is 0 so that we’re testing whether the true population means are the same.\n\n\nTwo-Sample Hypotheses\nThe usual null hypothesis invloves the equality of the means, with the alternative being “&gt;”, “&lt;”, or “≠”. This does not change: \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_0: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\nThe confidence interval is also the same idea: \\[\n\\text{sample statistic}\\pm\\text{critical value}*\\text{standard error} = (\\bar x_1 - \\bar x_2)\\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nIn both of these, we still need to know the degrees of freedom! As we saw last lecture, the \\(t\\)-distribution requires some information about the sample size. In the one-sample case, this was \\(n-1\\). However, we now have two potentially different sample sizes! What do we do?\nRecall that the \\(t\\)-distribution gets closer and closer to the normal distribution as \\(n\\) increases. The whole point of the \\(t\\) is to get us a little further from the normal in order to account for the variance in the sample standard deviation. For the two-sample case, there is a “correct” formula, but it’s big and scary and everything we’re doing now is approximate anyway. Instead, we use a more conservative approach to ensure that we’re not underestimating the variance.1\n\nTwo-Sample \\(t\\) degrees of freedom\nWhen doing hand calculations, we use the smallest sample size, then subtract 1. (R does a more accurate calculation that results in non-integer values.)\n\nIn the simulation we did earlier, the two samples had sizes of 22 and 33. In both a CI and a hypothesis test, we would use 21 as the value in qt() or pt() (which are the same idea as qnorm() and pnorm()).\n\nAside: “Pooled Variance”\nThere’s another formula out there that uses a so-called “pooled variance” for the standard error of the differences. This assumes that both populations have the exact same variance, and tries to use information from both to estimate the variance. It essentially treats the two samples as one big sample from the same population in order to calculate the standard deviation.\nThis also implicitly assumes that both populations are normal, and this is not based on the CLT. Instead, the populations need to be normal. This is a huge assumption - we can use normality from the CLT because the math checks out. Assuming normality of the population is just a wild guess that we can’t really check.\nIt is also very, very unlikely that the two populations have the same standard deviation.\nIf the two assumptions are met, then the pooled standard deviation is the “correct” formula. However, the SE that we saw before still works very well! If the assumptions are not met, then the pooled SE works poorly and the SE we’ve seen is still very good!\nExcept in exceptional cases, the SE that we’ve learned should be used. The idea of a pooled variance is a vestige of another age (and may show up if you use another textbook or search Google)."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#summary",
    "href": "L17-Two_Sample_hypothesis_Tests.html#summary",
    "title": "14  Two-Sample t-Tests",
    "section": "14.2 Summary",
    "text": "14.2 Summary\n\nTwo-Sample t-test and CI Overview\nWe are usually testing for the difference in means, i.e. \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_a: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_a:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\\[\nt_{obs} = \\frac{\\bar x_1 - \\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] \n\\[\n\\text{p-value} = P(T &lt; t_{obs}) = \\texttt{pt(t\\_obs, df = min(n1, n2) - 1)}\n\\]\n\\[\n\\text{A $(1-\\alpha)$ CI for $\\mu_d$ is }\\bar x_1 - \\bar x_2 \\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nwhere \\(t^*\\) is based on the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\)\nThe assumptions are still the same, with one notable addition:\n\nThe sampling distribution of the difference in means is approximately normal.\n\nNormal populations, approximately normal histograms, or large sample size.\n\nThe observations within each group are independent.\nThe observations between each group are independent."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#example-1-two-sample-versus-matched-pairs",
    "href": "L17-Two_Sample_hypothesis_Tests.html#example-1-two-sample-versus-matched-pairs",
    "title": "14  Two-Sample t-Tests",
    "section": "14.3 Example 1: Two-Sample versus Matched Pairs",
    "text": "14.3 Example 1: Two-Sample versus Matched Pairs\nFrom the Ointment example:\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Oint\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\nFirst we’ll do matched pairs. In this example, this is the correct test to use.\n\nwithoint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.01, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4, 6.7, 2.88, 7.88, 6.32)\ndiff &lt;- withoint - without\n\nhist(diff)\nabline(v = 0, col = \"red\", lwd = 2)\n\n\n\nt.test(x = diff, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -1.9421, df = 7, p-value = 0.04663\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n        -Inf -0.01332314\nsample estimates:\nmean of x \n   -0.545 \n\n\nFrom this t-test, we can see that there’s a mean difference of -0.545, which leads to a p-value of 0.04663. At the 5% level, the null is rejected and we conclude that there is a difference between the two groups. The ointment appears to make a difference!\nHowever, looking at the histogram, it looks like there may be an outlier. With a data set this small, one value can completely change our results!!! Remember that we’re dealing with means, and means are affected by outliers. This means that p-values are affected by outliers as well!\n\nThe Wrong Way\nWe could instead have done a two-sample test, which ignores the fact that the observations are paired. Since we know the pairings, this test is leaving out valuable information.\n\nboxplot(withoint, without)\nabline(v = 0, col = \"red\", lwd = 2)\n\n\n\nt.test(x = withoint, y = without, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  withoint and without\nt = -0.67584, df = 13.736, p-value = 0.2552\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf 0.8772719\nsample estimates:\nmean of x mean of y \n    5.155     5.700 \n\n\nThis result should not be trusted since it misses a key aspect of the data. The first value in the “Ointment” group corresponds to the first value in the “Without Ointment” group, they aren’t just two separate values - they’re measured on the same individual!\nRegardless, take a moment to look at the differences in the results between the two tests."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#example-2-body-mass-of-penguins",
    "href": "L17-Two_Sample_hypothesis_Tests.html#example-2-body-mass-of-penguins",
    "title": "14  Two-Sample t-Tests",
    "section": "14.4 Example 2: Body Mass of Penguins",
    "text": "14.4 Example 2: Body Mass of Penguins\nIn this example, we’ll look at the difference in body mass between male and female penguins.\nIn this case, there is no clear pairing between the penguins. If they were monogomous couples, then the differences in body mass might tell us about couples, but doesn’t say much about male and female penguins in general.2\nThis example gives us the opportunity to learn more notation in R! The previous example used t.test(x = ..., y = ...) to denote the two samples. If the data are neatly formatted in a data frame, then we can use the ~ notation to demonstrate that the body mass is split into different groups for male and female.\nFirst, we’ll draw a boxplot. This is a great way to compare two distributions, and can be made in a small amount of space. Pause for a moment and ask yourself whether these two groups intuitively look different.\n\nlibrary(palmerpenguins)\nboxplot(body_mass_g ~ sex, data = penguins, horizontal = TRUE,\n    xlab = \"Body Mass (grams)\", ylab = NULL)\n\n\n\nt.test(body_mass_g ~ sex, data = penguins)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -8.5545, df = 323.9, p-value = 4.794e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -840.5783 -526.2453\nsample estimates:\nmean in group female   mean in group male \n            3862.273             4545.685 \n\n\nNote: The notation will always be “variable we care most about” ~ “other variables”. In linear regression, this was y ~ x, and now it’s continuous ~ categorical.\nFrom the output above, we get a two-sided p-value as well as a two-sided CI, both confirming that the difference in body mass is different from 0. You can also see that it’s using “female minus male”, rather than “male minus female”. This is because R will put them in alphabetical order, so female comes first.\nYou may also be happy to hear that no, you will never have to manually enter the standard error formula! Let’s all say a big, collective thank you to the R programming language! Thank you! (You may need to interpret the idea of standard error in two-sample t-tests, though.)"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#example-3-birthweights-by-hand",
    "href": "L17-Two_Sample_hypothesis_Tests.html#example-3-birthweights-by-hand",
    "title": "14  Two-Sample t-Tests",
    "section": "14.5 Example 3: Birthweights (by hand)",
    "text": "14.5 Example 3: Birthweights (by hand)\n\n\nDo mothers who smoke give birth to children with a lower birthweight than mothers who don’t?\nTest this at the 5% level using the data on the right.\nWe’re going to do this by hand!\n\n\n\n\n\nSmoker\nNon-Smoker\n\n\n\n\nmean\n6.78\n7.18\n\n\nsd\n1.43\n1.60\n\n\nn\n50\n100\n\n\n\n\n\n\nThe null hypothesis is \\(H_0: \\mu_{smoke} = \\mu_{non}\\), which can be written as \\(H_0: \\mu_{non} - \\mu_{smoke} = 0\\).\nThe alternate hypothesis is \\(H_a: \\mu_{smoke} &lt; \\mu_{non}\\), which could be written as\n\n\\(H_a: \\mu_{non} - \\mu_{smoke} &gt; 0\\), or\n\\(H_a: \\mu_{smoke} - \\mu_{non} &lt; 0\\)\n\n\nIt doesn’t matter which we choose, but we have to know which we chose in order to calculate the corresponding p-value! Let’s use \\(\\mu_{n-s}\\).\nThe sample mean difference is \\(\\bar x_{non} - \\bar x_{smoke} = 7.18 - 6.78 = 0.4\\). The standard error is: \\[\nSE = \\sqrt{\\dfrac{s_s^2}{n_s} + \\dfrac{s_n^2}{n_n}} = \\sqrt{\\dfrac{1.43^2}{50} + \\dfrac{1.60^2}{100}} = 0.2578721\n\\]\nSince we used \\(\\bar x_{non} - \\bar x_{smoke}\\), our alternate tells us to find the p-value for a t-statistic larger than what we got.\n\\[\nt_{obs} = \\dfrac{(\\bar x_{non} - \\bar x_{smoke}) - (\\mu_{non} - \\mu_{smoke})}{SE} = \\dfrac{0.4 - 0}{0.2578721} = 1.55\n\\]\nSince we’re doing a right-tailed test3, we calculate our p-value as:\n\n1 - pt(1.55, 49)\n\n[1] 0.06378844\n\n\nTo summarise:\n\n\\(H_0: \\mu_{non} - \\mu_{smoke} = 0\\) versus \\(H_0: \\mu_{non} - \\mu_{smoke} &gt; 0\\)\n\\(t_{obs} = 1.55\\)\np-value is 0.06\n\nSince our p-value is larger than 0.5, we do not have a statistically significant result. We fail to reject the hypothesis that smokers have lower birthweights. We conclude that we do not have enough evidence to say that smoking is associated with lower birthweights."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#example-4-basketball-versus-hockey-players",
    "href": "L17-Two_Sample_hypothesis_Tests.html#example-4-basketball-versus-hockey-players",
    "title": "14  Two-Sample t-Tests",
    "section": "14.6 Example 4: Basketball versus Hockey Players",
    "text": "14.6 Example 4: Basketball versus Hockey Players\nIn a previous lecture, we looked at the heights of female basketball players to test whether their heights were consistent with the population. Now that we have the tools to compare two samples, let’s compare some teams! In what follows, we’re testing the the hypothesis that the basketball team and the hockey team are different heights. Let’s test at the 5% level since we have no strong reason to use a smaller or larger level4\n\nlibrary(dplyr)\nlibrary(ggplot2)\nstats &lt;- read.csv(\"wlu_female_athletes.csv\")\nbaskey &lt;- filter(stats, Sport %in% c(\"Hockey\", \"Basketball\"))\nboxplot(Height ~ Sport, data = baskey)\n\n\n\n\nFrom the boxplot, I would be absolutely flabbergasted if the two groups had the same mean! Astonished! Befuddled, even! This is an important part of any analysis: have expectations! You should know your data well before diving into a study. For example, looking at these boxplots reveals that there are no apparent outliers, and both groups look approximately symmetric. This is good.\nLet’s do the t-test.\n\nt.test(Height ~ Sport, data = baskey)\n\n\n    Welch Two Sample t-test\n\ndata:  Height by Sport\nt = 4.5266, df = 26.588, p-value = 0.0001121\nalternative hypothesis: true difference in means between group Basketball and group Hockey is not equal to 0\n95 percent confidence interval:\n  3.805256 10.123430\nsample estimates:\nmean in group Basketball     mean in group Hockey \n                172.1771                 165.2128 \n\n\nFrom the output, we can conclude that there is a statistically significant difference in the average height of female hockey and basketball players at Laurier.\nSome caveats:\n\nThese are small samples!!!\n\nRecall that R uses a more accurate value for df (we just use the smaller sample size then subtract one for hand calculations). It’s 26.588, which can be interpreted as something like the average of the two sample sizes.\n\nThis isn’t exactly a random sample!\n\nThe data were taken from the current seasons’ full teams. We don’t exactly have a wel–defined population here. Is the population all women? Only femal athletes? Only female varsity athletes? Only Canadian female varsity athletes in these particular sports? Or is it only female varsity athletes in these particular sports at Laurier? It’s hard to say what these results apply to.\n\nWe just tested hockey versus basketball. What about soccer? What about heights by position?\n\nIf we start testing many hypotheses, we run afoul of the multiple comparisons problem. Before collecting the data, this test had a 5% chance of being significant if the null hypothesis were true. So do all of the other tests. If we test enough times, we’ll eventually reject a true null hypothesis."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#conclusion",
    "href": "L17-Two_Sample_hypothesis_Tests.html#conclusion",
    "title": "14  Two-Sample t-Tests",
    "section": "14.7 Conclusion",
    "text": "14.7 Conclusion\n\nIf you can have matched pairs, you should use a matched pairs test.\nMost of the time, you’ll need to use a two-sample t-test.\n\nDon’t get fooled by equal sample sizes! Just because the sample sizes are the same doesn’t mean that the observations are paired!\n\nA two-sample t-test is based on the difference in means\n\nThe standard error is tricky - software will do this for you.\nThe degrees of freedom is the smallest sample size minus 1.\n\nUsed for the p-value for hypothesis tests and the critical value for confidence intervals.\nR uses a better value.\n\nThe null hypothesis is usually 0, and the alternate depends on the order in which you subtract the means."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#self-study-questions",
    "href": "L17-Two_Sample_hypothesis_Tests.html#self-study-questions",
    "title": "14  Two-Sample t-Tests",
    "section": "14.8 Self-Study Questions",
    "text": "14.8 Self-Study Questions\n\nWrite out the null and alternate hypotheses for the “Ointment” example and the “Penguins” example, and comment on the difference between the two examples. Make sure the alternate has the same “&gt;” or “&lt;” or “≠” that was used in R’s calculation! Hint: It matters if you do Group 1 minus Group 2 or if you do Group 2 minus Group 1!\nVeryify the p-values in the Ointment and Penguins examples. Use the t-value reported in the R output, and find the p-value using either pt(), 1 - pt, or 2 * (1 - pt()), as appropriate. Explain your answer.\nExplain why t-tests can only be used for comparing two groups, not three. (Hint: write out the null hypothesis - what value are we using in \\(\\mu = \\mu_0\\)?)\nExplain why the 95% CI still has the same interpretation as before.\nExplain why there is only one sampling distribution in a two-sample t-test, even though there are two populations.\nProvide a conclusion based on the following output (suppose you’re testing at the 10% level). The mpg variable is the miles per gallon of a vehicle, while am is the transmission type, with 0 = Automatic and 1 = Manual. mtcars is just the name of the data that these variables are in.\n\n\nt.test(mpg ~ am, data = mtcars)\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\n\nSelected Solutions\n\n\nOintment: \\(H_0: \\mu_{without-with} = 0\\) versus and \\(H_A: \\mu_{without-with} &lt; 0\\). Note that we can tell that it’s without-with by looking at the means of the two groups and the sign of the t-statistic.\n\nPenguins: \\(H_0: \\mu_{female} - \\mu_{male} = 0\\) versus \\(H_A: \\mu_{female} - \\mu_{male} \\ne 0\\).\nThe ointment example had \\(\\mu_{without - with}\\) as a single number, whereas the penguins example used \\(\\mu_{female} - \\mu_{male}\\), which is the difference between two numbers. This is because the ointment example was a one-sample t-test since we were treating the set of differences as a single sample; the penguins example is a two-sample t-test."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#crowdsourced-questions",
    "href": "L17-Two_Sample_hypothesis_Tests.html#crowdsourced-questions",
    "title": "14  Two-Sample t-Tests",
    "section": "14.9 Crowdsourced Questions",
    "text": "14.9 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA biologist is comparing the mean heights of two species of plants. Species A has a sample mean height of 15 cm with a standard deviation of 2 cm from a sample of 30 plants. Species B has a sample mean height of 17 cm with a standard deviation of 3 cm from a sample of 35 plants. What does the two-sample t-test assess in this scenario?\n\nWhether the sample means of Species A and Species B come from populations with the same variance.\nWhether the mean height of Species A is greater than the mean height of Species B.\nWhether there is a significant difference between the population means of the two species.\nWhether the combined mean height of both species exceeds a specific value.\n\n\n\n\nSolution\n\nC; The two-sample t-test in this scenario assesses whether there is a significant difference between the population means of Species A and Species B. It does not test for variance equality, the direction of the difference (unless specified as a one-tailed test), or any specific value exceeding the combined means.\n\n\n\nA researcher conducts an experiment to determine if there will be a significant difference in the growth rate of two different species of plants under identical laboratory conditions. The first group of 20 plants (Species A) showed an average growth of 5 cm over a month, while over the same period of time the second group of 20 plants (Species B) showed an average growth of 6 cm. Assume the growth rates follow a normal distribution. explain what steps you would take to determine if the observed difference is statistically significant. Consider a 5% significance level for your decision.\n\n\n\nSolution\n\n\ndetermine a null and alternative hypothesis:\n\nNull Hypothesis: There is no difference in the mean growth rates between Species A and Species B, therefore: \\(\\mu_A​−\\mu_B​=0\\).\nAlternative Hypothesis: There will be a difference in the mean growth rates between Species A and Species B, therefore: \\(\\mu_A​−\\mu_B\\)​ does not =0.\n\nCalculate the difference between the sample means.\n\n\\(\\mu_A​−\\mu_B\\)​: 6cm−5cm=1cm.\n\nUse a Statistical Test:\n\nbecause we are comparing the means of two groups, a two-sample t-test would be the best option. This will compare the difference in means to the variability within groups to see if the difference is significant. this can be calculated by using R\n\ndetermining the significance:\n\nIf the p-value from the t-test is less than 0.05 (which is our chosen significance level stated previously),the null hypothesis will be rejected, therefore we can determine that there is a significant difference in growth rates between the two species.\nIf the p-value is greater than 0.05, we do not reject the null hypothesis, and therefore we have determined that there is not enough evidence to say there is a significance difference in their growth rate."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#footnotes",
    "href": "L17-Two_Sample_hypothesis_Tests.html#footnotes",
    "title": "14  Two-Sample t-Tests",
    "section": "",
    "text": "In general, we would much, much, much rather overestimate the variance. The whole point of statistics is to avoid overconfidence in our estimates.↩︎\nRecall: penguins are especially likely to have homosexual relationships and tend to be more fluid in their gender roles than other animals.↩︎\nIf we had done a left-tailed test, we wouldn’t need the “1 -”. Explain why.↩︎\nA smaller level would be used if we required strong evidence before we reject the null, such as when a new treatment seems implausible.↩︎"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "title": "15  Large sample test for a proportion",
    "section": "15.1 Refresher",
    "text": "15.1 Refresher\nIn the lecture on sampling distributions, we learned that the sampling distribution of a sample proportion can be found as follows:\nIf \\(X\\sim B(n,p)\\) and \\(np&gt;10\\) and \\(n(1-p)&gt;10\\),1 then we can approximate the sampling distribution of \\(\\hat p\\) as follows: \\[\n\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\nThis relies on the population proportion \\(p\\) to find the standard error, but this is never available. If it was, then why are we trying to do inference?"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "title": "15  Large sample test for a proportion",
    "section": "15.2 Hypothesis tests for proportions",
    "text": "15.2 Hypothesis tests for proportions\nAs before, we write our hypotheses: \\[\nH_0:p = p_0 \\text{ vs. } H_A: p \\{&gt;or&lt;or\\ne\\} p_0\n\\]\nWe always write \\(H_0:p = p_0\\) and then fill in the value for \\(p_0\\), then we use that same value in the alternate hypothesis but use either \\(&gt;\\), \\(&lt;\\), or \\(\\ne\\) based on the wording of the question.\nAs before, we use the sampling distribution to find our p-value. In this case, though, we have a hypothesized value for the population proportion. In fact, we must assume that the null is true.2 If this is the case, we do have the standard error!\nI swear, this is the last time I introduce a new standard error for the sampling distribution of the sample proportion.3 Assuming \\(H_0\\) is true (and the conditions are met), \\[\n\\hat p \\sim N\\left(p_0, \\sqrt{\\frac{p_0(1-p_0)}{n}}\\right)\n\\]\n\nThe Test Statistic\nAs you can guess from the sampling distribution, the test statistic is:\n\n\n\n\n\n\nTest Statistic for a Test for Proportions\n\n\n\n\\[\nz_{obs} = \\frac{\\text{observed} - \\text{hypothesized}}{\\text{standard error}} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]\n\n\nand then we can use the normal distribution as usual: \\[\nP(Z \\{&gt;or&lt;or\\text{ further away than}\\} z_{obs}) = \\dots\n\\]\nwhere we use \\(&gt;\\) if the alternate hypothesis uses \\(&gt;\\), \\(&lt;\\) if the alternate hypothesis uses \\(&lt;\\), and we look at the two tails if the alternate hypothesis is \\(\\ne\\).\nA common question is: which \\(p\\) do we use to check normality? We’re supposed to check \\(np\\) and \\(n(1-p)\\), but do we use \\(\\hat p\\) or \\(p_0\\)?\nFor a hypothesis test, we assume the null is true, i.e. \\(p=p_0\\). We should use this assumption everywhere! For a hypothesis test about a proportion, we check whether \\(np_0&gt;10\\) and \\(n(1-p_0)&gt;10\\)4.\nFrom here, we proceed as usual. We check the observed test statistic against a normal distribution5 and see whether our data are too extreme to come from the distribution assumed in the null hypothesis."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "title": "15  Large sample test for a proportion",
    "section": "15.3 Example",
    "text": "15.3 Example\n\nMendelian Genetics\nTo test his theory that 75% of plants would inheret a dominant gene, Gregor Mendel cross bred pure breeds of pea plants. Out of 7324 plants, 5474 showed the dominant trait. At the 4.5% level, is this compatible with the hypothesis of 75% dominant?\nSolution:\n\nCheck: \\(np_0 = 7324*0.75 &gt; 10\\) and \\(n(1-p_0) = 7324*0.25 &gt; 10\\).\n\\(z_{obs} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.747 - 0.75}{\\sqrt{0.75*0.25/7324}} = -0.513\\)\n\\(p-val = 2 *P(Z &lt; z_{obs})\\) = 2*pnorm(-0.513) = 0.608\n\nWe doubled the \\(P(Z &lt; z_{obs})\\) because we want both tails. If you do this and your p-value is larger than 1, do \\(1 - P(Z &lt; z_{obs})\\) first and then double it.\n\nConclusion: Since our p-value is larger than \\(\\alpha\\), we do not reject the null. The hypothesis that 75% of plants inherent the dominant trait is compatible with the data.\n\nThe last step is important: always word your conclusion in the context of the study.\nThese methods are extremely widespread, so of course they’re implemented in R. Here’s a verification of our results:\n\nprop.test(x = 5474, n = 7324, p = 0.75)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  5474 out of 7324, null probability 0.75\nX-squared = 0.24923, df = 1, p-value = 0.6176\nalternative hypothesis: true p is not equal to 0.75\n95 percent confidence interval:\n 0.7372578 0.7572926\nsample estimates:\n        p \n0.7474058 \n\n\nThe output should look familiar - it’s very similar to the t-test output.\nWe can see that the p-value (be careful not to mix up the p-value and the estimate of \\(p\\), labelled p - these are very different things!) is a little different. Maybe it’s because of rounding errors? We calculated the z test statistic to the nearest 3 decimal places, maybe that wasn’t enough?\n\nx &lt;- 5474\nn &lt;- 7324\nphat &lt;- x / n\nse &lt;- sqrt(0.75 * (1 - 0.75) / n)\n2 * pnorm((phat - 0.75) / se)\n\n[1] 0.6081484\n\n\nNope, it’s not a rounding problem!\nThe actual answer is that R uses a continuity correction factor (which isn’t going to be on the test for this course). The correction factor “shifts” the data so that the normal distribution aligns with the center of the bar, rather than the edge. See the following plot for why.\n\n\n\n\n\nAs you can see, the normal distribution aligns with the side of the bar. For values below the mean (in this case, \\(n=10\\) and \\(p=0.4\\), so the mean is 4), the normal distribution is overestimating the areas to the left, whereas above the mean it’s underestimating the areas to the left. The correction factor shifts the normal distribution to the right by 0.5 so that it’s a better estimate of the areas below the curve.\nIf we run prop.test() without the correction factor, we get the exact same p-value that we saw before.\n\nprop.test(x = 5474, n = 7324, p = 0.75, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  5474 out of 7324, null probability 0.75\nX-squared = 0.26288, df = 1, p-value = 0.6081\nalternative hypothesis: true p is not equal to 0.75\n95 percent confidence interval:\n 0.7373269 0.7572253\nsample estimates:\n        p \n0.7474058 \n\n\nThese details are not important, just be aware that almost all tests for proportions are run with the continuity correction factor. You will not be doing this by hand on a test, so you can assume that R did the right things to make the calculations as correct as possible (but R will not do anything to make sure that the test is appropriate for the data).\n\n\nMendelian Genetics Confidence Interval\nRecall from last lecture the duality of the CI and the hypothesis test. For this question, a 95.5%6 CI matches what we used before.\nIn order to find the confidence interval, we again need the standard error! In the hypothesis test, we assumed that \\(p_0\\) was the true population proportion in order to proceed with the test. However, we don’t make this assumption for confidence intervals. In other words, there is no \\(p_0\\) value that we can use. We haven’t made any hypotheses for a p CI.\nWhat can we do? We don’t have \\(p\\) or \\(p_0\\), so we’re left with \\(\\hat p\\), the sample proportion that we calculated. In the t-test, this meant that we needed to switch to the \\(t\\) distribution. However, that was because there was really good theory to say that the \\(t\\) distribution is the correct distribution to use. There’s no such theory here.\nThe standard error is still the same, we just use \\(\\hat p\\) in place of \\(p\\): \\[\nSE = \\sqrt{\\dfrac{\\hat p(1 - \\hat p)}{n}}\n\\] and our confidence interval is, again, \\[\n\\text{Point Estimate }\\pm\\text{ Critical Value} * \\text{Standard Error} = \\hat p \\pm z^*\\sqrt{\\dfrac{\\hat p(1 - \\hat p)}{n}}\n\\]\n\n\n\n\n\n\nCIs for Proportions Only Work When the CLT Applies\n\n\n\nThe \\(t\\)-distribution allows us to do hypothesis tests and make CIs even for smaller samples when we’re not sure that the CLT applies. For proportions, we need a “large” sample.\n\n\nNow that we know all this, the CI can be found as: \\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}} = 0.747 \\pm 2.005\\sqrt{\\frac{0.747(1-0.747)}{7324}}\n\\]\nwhich results in the CI (0.737, 0.757). This matches the CI shown in the output of prop.test() above (double check this!).\n\n\n\n\n\n\nNon-Duality of Hypotheses and CIs\n\n\n\nFor the t-test, a CI can be interpreted as “every value that would not be rejected by a hypothesis test.”\nFor proportions, it is not true. This is because the CI and the hypothesis test use different standard errors. This is a very important point: the hypothesis test uses \\(p_0\\) in the standard error formula, while CIs use \\(\\hat p\\), and thus the standard errors will be different."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#exact-test-for-binomial",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#exact-test-for-binomial",
    "title": "15  Large sample test for a proportion",
    "section": "15.4 Exact Test for Binomial",
    "text": "15.4 Exact Test for Binomial\nIn this course, we have used the normal approximation to the binomial in order to do hypothesis tests. This is not the only way to do it: we don’t always need to use the approximation! There’s something called the “exact binomial test”, which is a hypothesis test that uses the binomial distribution rather than the normal approximation (this will not be on tests).\nThere are two main reasons why we might prefer the approximation, rather than using the exact test:\n\nIf we have a large sample, then the approximation and the exact test are very very close. The approximation is computationally simpler.\n\nIf we have a small sample, neither tests can accurately approximate the variance of the population, and thus the estimated standard error isn’t well estimated either.\n\nBecause the binomial distribution is discrete, the p-values for many different test statistics will be the same. By setting \\(\\alpha\\), we might not actually be getting \\(\\alpha\\).\n\nThis is a technical point that can be safely ignored when studying for tests.\n\n\nThe exact test can be performed using the binom.test() function in R. You will never have to choose between the two on an exam in this course. You will be given only the one that you need (usually binom.test() for one sample proportions, and prop.test() for two-sample proportions)."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#example-titanic-survivors",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#example-titanic-survivors",
    "title": "15  Large sample test for a proportion",
    "section": "15.5 Example: Titanic Survivors",
    "text": "15.5 Example: Titanic Survivors\nDid women have a 50% chance of surviving the sinking of the titanic? Let’s test this at the 10% level, since we’d be pretty easily convinced.\nSince this is categorical data, we’ll use a bar plot.\n\n# The code in this example is *not* testable. The output *is*.\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# The Titanic data is built in to R, but starts off in a weird format.\n# as.data.frame() makes it easier to deal with.\nTitanic = as.data.frame(Titanic)\n# This fancy code gives me all women, regardless of class, but only adults.\nsurvivors &lt;- Titanic %&gt;% filter(Age == \"Adult\") %&gt;%\n    group_by(Sex, Survived) %&gt;%\n    summarise(Freq = sum(Freq))\n\n# Barplot since it's categorical.\nggplot(survivors) +\n    aes(x = Sex, y = Freq, fill = Survived) +\n    geom_bar(position = \"Dodge\", stat = \"Identity\")\n\n\n\n\nFrom the bar plot, it’s pretty clear that the prportion of survivors is different from 50%. Let’s confirm this statistically!\nWe’re only interested in the female survivors for this analysis, so we’ll focus on that aspect of the bar plot.\n\nsurvivors\n\n# A tibble: 4 × 3\n# Groups:   Sex [2]\n  Sex    Survived  Freq\n  &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;\n1 Male   No        1329\n2 Male   Yes        338\n3 Female No         109\n4 Female Yes        316\n\n\nFrom this output, we know that there were 316 female survivors out of 316 + 109 = 425 women total. Let’s report these numbers to R:\n\nprop.test(x = 316, n = 425)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  316 out of 425, null probability 0.5\nX-squared = 99.849, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.698733 0.783840\nsample estimates:\n        p \n0.7435294 \n\n\nThe default hypothesis is that the proportion is equal to 0.5, so we didn’t have to change anything. From this, it is clear that the null hypothesis should be rejected. We conclude that the percentage of women who survived the sinking of the Titanic is different from 50%7."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#summary",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#summary",
    "title": "15  Large sample test for a proportion",
    "section": "15.6 Summary",
    "text": "15.6 Summary\nCIs and Hypothesis Tests work exactly as they did before, but now we’re dealing with proportions. Just like the change from one sample to two sample \\(t\\) tests, the standard error is important and difficult.\n\nFor hypothesis tests, the standard error uses \\(p_0\\).\nFor CIs, the standard error uses \\(\\hat p\\).\n\nInterpreting these confidence intervals and hypothesis tests is very similar to before, but you must keep in mind that they’re proportions. This mainly affects how you describe the end results.\nThere are two other wrinkles to consider when using proportions:\n\nThe default test for proportions is the normal approximation with continuity correction.\n\nIt is possible, but not recommended, to not use continuity correction.\n\nThere is also an exact test, but for large samples the approximation is faster and easier."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "title": "15  Large sample test for a proportion",
    "section": "15.7 Self-Study Questions",
    "text": "15.7 Self-Study Questions\n\nWhen do we use \\(\\hat p\\) in the standard error? When do we use \\(p_0\\)?\nExplain why we don’t estimate the standard error in a hypothesis test about a proportion.\nExplain in your own words why there’s no \\(t\\) version of a hypothesis test for proportions.\nWrite a good summary of the Mendelian genetics example. What did we conclude, and how is this knowledge useful?\nRepeat the Titanic survivors example but with male passengers."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#crowdsourced-questions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#crowdsourced-questions",
    "title": "15  Large sample test for a proportion",
    "section": "15.8 Crowdsourced Questions",
    "text": "15.8 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nMichael is interested in studying farmers’ preferences for a new variety of genetically modified (GM) corn. He plans to survey 100 farmers to see if more than 60% prefer the GM variety over traditional varieties. Can he use the normal approximation to analyze his survey results?\n\n\n\nSolution\n\nMichael can use the normal approximation in his study. The criteria for using the normal approximation for proportions are that both \\(np_0\\) and \\(n(1 - p_0)\\) must be greater than 5. Here, (the number of farmers surveyed) and (the hypothesized proportion of farmers prefer the GM variety).\nLet’s check if the conditions are met:\n\n\\(np_0 = 100 * 0.6 = 60\\), which is greater than 5.\n\\(n(1 - p_0) = 100 * 0.4 = 40\\), which is also greater than 5.\n\nSince both conditions are satisfied, the normal approximation can be used to analyze the survey results.\nA demonstration of this is shown below. Note how closely the normal approximation applies to the binomial. However, it’s a little bit off. This is why R uses “continuity correction” (although we don’t discuss the details of this in this course).\n\n\n\n\n\n\n\n\nTo study the cross breeding of heterozygous plants, Alex wants to perform a study on 8 Hamelin pea plants to test if 75% of the plants are of the dominant type after breeding. Can she assume the normal approximation in her study?\n\n\n\nSolution\n\nNo! To use the normal approximation \\(np \\gt 10\\) and \\(n(1-p) \\gt 10\\) must be obeyed. \\(8*0.75\\lt 10\\) and \\(8*(1-0.75) \\lt 10\\), thus Alex should increase her sample size prior to using a normal approximation."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "title": "15  Large sample test for a proportion",
    "section": "",
    "text": "That is, we have a large sample↩︎\nWe want to be strict about this so that it’s more convincing if we prove it wrong.↩︎\nI’m lying.↩︎\nAs before, both conditions must be true; it’s not enough for just \\(np_0&gt;10\\) alone.↩︎\nWe do not use a t distribution for this. The \\(t\\) distribution was used to account for the variance in the sampling distribution for \\(s\\) - the standard error for proportions is still based on \\(p\\), which means that there is no added variance!↩︎\n\\(\\alpha = 0.045\\), so \\(1 - \\alpha = 0.955\\).↩︎\nWe can only conclude what we see in our hypotheses!!!↩︎"
  },
  {
    "objectID": "L19-CI_for_Proportions.html#introduction",
    "href": "L19-CI_for_Proportions.html#introduction",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\n\nThis is the same as the last lesson\n\nBased on our data, we make an interval that we think describes the population.\nIn this case, we just have a different population distribution?\n\n\n\nAssumptions\nIn stats, assumptions give us power, but only if they’re good assumptions.\nAssumptions for a CI for \\(p\\) are the same as the assumptions for the binomial distribution, with the addition of an SRS."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#the-ci-for-p",
    "href": "L19-CI_for_Proportions.html#the-ci-for-p",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.2 The CI for \\(p\\)",
    "text": "16.2 The CI for \\(p\\)\n\nSampling Distribution of \\(\\hat p\\)\nAs we saw before the midterm, if the population is \\(B(n,p)\\), then under certain conditions,\n\\[\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\\]\n\n\nDeja-Vu\nSince \\(\\hat p \\sim N(p, \\sqrt{\\frac{p(1-p)}{n}})\\),\n\\[\n\\frac{\\hat p - p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]\nAgain, we can use the form \\(z = (x-\\mu)/\\sigma\\), but replace \\(x\\), \\(\\mu\\), and \\(\\sigma\\) with the correct values.\nA \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\]\n\n\nWe don’t know the variance, why not \\(t_{n-1}^*\\)?\n\nWe used \\(t_{n-1}^*\\) because we had to estimate \\(\\sigma\\)\nThere’s no \\(\\sigma\\) to estimate!\nThe variance of the Binomial distribution is entirely determined by \\(p\\)!\n\nBinom be crazy.\n\n\n\n\n… but Devan, we still don’t know \\(p\\)!\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\] which needs \\(p\\) in the second part of the equation.\nWhy not just plug in \\(\\hat p\\)?\nOkay fine. \n\\(\\sqrt{\\hat p(1-\\hat p)/n}\\) is called the estimated standard error, since its the sd of the sampling distribution, but it’s based on an estimate.\n\n\nFinal_Version_V2_Update_LastTry_Srsly.docx.pdf\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nwhere \\(z^*\\) is chosen such that \\(P(Z &lt; -z^*) = \\alpha/2\\).\n\n\nDevan Style: Simulation\n\nn &lt;- 100\np &lt;- 0.7\nSE_true &lt;- sqrt(p*(1-p)/n)\np_does &lt;- c()\nphat_does &lt;- c()\nthat_does &lt;- c()\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, df = n-1))\n\n\n\nDevan Style: Simulation\n\nfor(i in 1:10000){\n    new_sample &lt;- rbinom(n=1, size=n, prob=p)\n    phat &lt;- new_sample/n\n    SE_est &lt;- sqrt(phat*(1-phat)/n)\n    \n    pCI &lt;- phat + c(-1,1)*z_star*SE_true\n    phatCI &lt;- phat + c(-1,1)*z_star*SE_est\n    thatCI &lt;- phat + c(-1,1)*t_star*SE_est\n    \n    p_does[i] &lt;- pCI[1] &lt; p & pCI[2] &gt; p\n    phat_does[i] &lt;- phatCI[1] &lt; p & phatCI[2] &gt; p\n    that_does[i] &lt;- thatCI[1] &lt; p & thatCI[2] &gt; p\n}\n\n\n\nSimulation Results\n\nmean(p_does)\n\n[1] 0.9371\n\nmean(phat_does)\n\n[1] 0.9502\n\nmean(that_does)\n\n[1] 0.9502\n\n\nUsing the population proportion is… worse?\nDIY: Change \\(p\\) so that the normal approximation doesn’t apply."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#examples-and-cautions",
    "href": "L19-CI_for_Proportions.html#examples-and-cautions",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.3 Examples and Cautions",
    "text": "16.3 Examples and Cautions\n\nExample 1\nIt was found that 591 out of 700 people sampled supported a certain political position. Find a 91%CI.\n\nSince we have R, let’s use it!\nBoth prop.test() and binom.test() will give us a CI, with prop.test() calculating an approximation using the normal distribution and binom.test() calculating the exact value, without approximation. In general, you should always use binom.test() for one sample proportions.\n\nbinom.test(x = 591, n = 700, conf.level = 0.91)\n\n\n    Exact binomial test\n\ndata:  591 and 700\nnumber of successes = 591, number of trials = 700, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n91 percent confidence interval:\n 0.8192078 0.8670686\nsample estimates:\nprobability of success \n             0.8442857 \n\n\n\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nn &lt;- 70\nphat &lt;- 68/70\nse_est &lt;- sqrt(phat*(1-phat)/n)\nz_star &lt;- abs(qnorm(0.09/2))\n\nphat + c(-1, 1)*z_star*se_est\n\n[1] 0.9376692 1.0051879\n\n\n… so it would be reasonable to say that the popluation proportion is larger than 1???\n\nAbsolutely not! The normal approximation does not apply here since \\(n(1 - \\hat p) = 70*(1 - 68/70) = 2\\), and 2 is less than 101. The normal distribution can only be used when the sample size is large enough!!!\nInstead, we can use the exact test. This is much slower to calculate, but for \\(n = 70\\) there’s no issue.\n\nbinom.test(x = 68, n = 70, conf.level = 0.91)\n\n\n    Exact binomial test\n\ndata:  68 and 70\nnumber of successes = 68, number of trials = 70, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n91 percent confidence interval:\n 0.9108816 0.9951927\nsample estimates:\nprobability of success \n             0.9714286 \n\n\nNotice that\n\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nbinom.test(x = 68, n = 70)\n\n\n    Exact binomial test\n\ndata:  68 and 70\nnumber of successes = 68, number of trials = 70, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.9005711 0.9965209\nsample estimates:\nprobability of success \n             0.9714286"
  },
  {
    "objectID": "L19-CI_for_Proportions.html#crowdsourced-questions",
    "href": "L19-CI_for_Proportions.html#crowdsourced-questions",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.4 Crowdsourced Questions",
    "text": "16.4 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nAssuming all other factors are held constant, how does increasing the number of trials from 100 to 400 affect the width of the confidence interval for a population proportion?\n\nThe width of the confidence interval remains unchanged.\nThe width of the confidence interval decreases, leading to a more precise estimate.\nThe width of the confidence interval increases, indicating less precision in the estimate.\nThe change in the width of the confidence interval cannot be determined without knowing the population size.\n\n\n\n\nSolution\n\n\nIncreasing the sample size from 100 to 400, while keeping all other factors constant, decreases the width of the confidence interval for a population proportion. This is because the standard error of the proportion decreases as the sample size increases, leading to a more precise estimate of the population proportion."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#footnotes",
    "href": "L19-CI_for_Proportions.html#footnotes",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "",
    "text": "Citation needed.↩︎"
  },
  {
    "objectID": "L20-Two_Proportions.html#diy-confidence-intervals",
    "href": "L20-Two_Proportions.html#diy-confidence-intervals",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.1 DIY Confidence Intervals",
    "text": "17.1 DIY Confidence Intervals\nThis lesson is going to be a little different from the rest. I’m not going to give you the answers, I’m going to give you the tools.\n\nStandard Error for a Single Mean\nAs we’ve seen many times, this is \\[\nSE(\\bar X) = \\frac{\\sigma}{\\sqrt{n}} = \\sqrt{\\frac{\\sigma^2}{n}},\n\\] but we often use \\[\n\\hat{SE}(\\bar X) = \\frac{S}{\\sqrt{n}} = \\sqrt{\\frac{S^2}{n}},\n\\] which is the estimated standard error (the “hat” on top of the letters “SE” indicates that it’s estimated). For the rest of this lecture, we’ll always use the estimated standard error.\n\n\nStandard Error for the Difference in Means\nEven though we were subtracting means, we added their variances and then take their square root. \\[\n\\hat{SE}(\\bar X_1 - \\bar X_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\] It is an important fact that we add the variances and then take the square root.\nFrom this lesson, also note that we had to make the assumptions:\n\nThe individuals within a group are independent of other individuals in that group.\n\nFor example, if we sample people in our own family then the samples are not independent. People in the same family tend to have similar characteristics, so knowledge of the characteristics of one family member are informative about the others.1\n\nThe groups are independent.\n\nFor example, if we’re looking at the difference in mean heights between men and women, but we have spousal pairs. Spousal pairs have a smaller difference in height than the average difference in height.2\n\n\n\n\nStandard Error for a Single Proportion\nThis is nothing new, I’m just repeating it here: \\[\n\\hat{SE}(\\hat p) = \\sqrt{\\frac{\\hat p(1 - \\hat p)}{n}}\n\\]\n\n\nStandard Error for the Difference in Proportions\nThis is up to you to find! Keed these in mind:\n\nThe standard error cannot be negative, so you probably can’t subtract things.\nVariances can be added if we assume things are independent.\n\nMake these assumptions explicit!\n\n\n\n\nConfidence Interval Example\nThis question is from OpenIntro Introductory Statistics for the Life and Biomedical Sciences, First Edition.\nThe way a question is phrased can inﬂuence a person’s response. For example, Pew Research Center conducted a survey with the following question:\n\nAs you may know, by 2014 nearly all Americans will be required to have health insurance. [People who do not buy insurance will pay a penalty] while [People who cannot afford it will receive financial help from the government]. Do you approve or disapprove of this policy?\n\nFor each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the order of the two statements was reversed. The table below shows the results of this experiment. Calculate and interpret a 90% conﬁdence interval of the difference in the probability of approval of the policy.\n\n\n\n\nSample size \\(n_i\\)\nApprove %\n\n\n\n\nOriginal Ordering\n771\n47\n\n\nReversed Ordering\n732\n34\n\n\n\nSolution\nLet \\(p_1\\) be the proportion who approve when given the original ordering with sample size \\(n_1\\), and \\(p_2\\) be the proportion who approve when given the reversed ordering with sample size \\(n_2\\). This question is asking us to calculate a confidence interval for \\(p_1 - p_2\\).\nWe first check the conditions required to use the normal approximation.\n\nPew Research Center is basically the world expert on opinion polling, so the samples are probably good.\nWe can safely assume that the samples are independent.\nThe two statements were randomly assigned, so it’s safe to say that the two groups are independent.\n\\(n_1 * 0.47 = 771 * 0.47 = 362.37\\)\n\nThere are three other calculations to check. Check them!\n\n\nNow that that’s covered, we can make a confidence interval. The general form is: \\[\n\\text{Point Estimate}\\pm\\text{Critical Value}*\\text{Standard Error}\n\\] From your homework above, verify that you can calculate the standard error as 0.025.3\nOur point estimate is \\(\\hat p_1 - \\hat p_2 = 0.47 - 0.34 = 0.13\\). Since we doing a difference in proportions, our critical value comes from the normal distribution:\n\nqnorm((1 - 0.9)/2)\n\n[1] -1.644854\n\n\nSo our confidence interval is: \\[\n0.13 \\pm 1.65*0.025 = (0.09, 0.17)\n\\]\nWe are 90% confident that the true mean difference is between 0.09 and 0.17. This provides evidence that the two proportions are indeed different."
  },
  {
    "objectID": "L20-Two_Proportions.html#hypothesis-testing",
    "href": "L20-Two_Proportions.html#hypothesis-testing",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.2 Hypothesis Testing",
    "text": "17.2 Hypothesis Testing\nHere’s where things are a little less obvious - I’m not going to get you to find the standard error yourself!\nWe are generally looking at a hypothesis test for whether two proportions are equal, that is, \\[\nH_0: p_1 = p_2\\implies p_1 - p_2 = 0\n\\] with an alternative that they are not equal, or that one is bigger than the other. In other words, we’re looking at the hypotheses:4 \\[\\begin{align*}\nH_0: &p_{1-2} = 0\\\\\nH_A: &p_{1-2} \\ne 0\\text{ or }p_{1-2} &gt; 0\\text{ or }p_{1-2} &gt; 0\\\\\n\\end{align*}\\]\nIn the lesson on proportions, we saw that the standard error depended on the null hypothesis being true, since we calculate p-values under the assumption that the null hypothesis is true. How do we do that here?\n\nThe Pooled Proportion\nUnder the null hypothesis, \\(p_1 = p_2\\). That’s like saying that we observed a bunch of successes and failures from a single group, instead of two. Let \\(x_1\\) be the number of successes in the first group, and \\(x_2\\) the number for the second. Then \\[\n\\hat p = \\frac{x_1 + x_2}{n_1 + n_2}\n\\] That is, we observed \\(x_1 + x_2\\) successes out of \\(n_1 + n_2\\) trials.\nFor example, if we assume that two coins have the same probability of heads, the getting 5 heads in 9 flips for one coin and 3 heads out of 6 flips for the other. The two coins are assumed to be identical, so it’s like we flipped one coin 15 times and got 8 heads.\nAs before, the assumption that the null hypothesis is true is used everywhere. This means it’s true for testing whether the normal approximation is appropriate. We must test \\(n_1\\hat p\\), \\(n_1(1 - \\hat p)\\), \\(n_2\\hat p\\), and \\(n_2(1 - \\hat p)\\).\nFrom this, we might assume that our standard error is something like: \\[\n\\hat{SE}(\\hat p_1 - \\hat p_2) = \\sqrt{\\frac{\\hat p(1 - \\hat p)}{???}}\n\\] The ??? might seem like it should be \\(n_1 + n_2\\), but some advanced math shows that this doesn’t quite work. Again, this is from the problem of adding variances, but working with standard deviations. Instead, the standard error is: \\[\n\\hat{SE}(\\hat p_1 - \\hat p_2) = \\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\n\\]\nThis standard error is based on the null hypothesis, specifically the assumption that the groups are identical, so it’s as if we took two samples from the same population.\nAs before, the test statistic is \\[\n\\frac{\\text{sample statistic} - \\text{hypothesized value}}{\\text{standard error}} = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\] and this is compared to the normal distribution.\n\n\nHypothesis Test Example\nUsing the same example as before, we can set up our null hypothesis as \\(p_1 = p_2\\), and we’ll choose the alternate hypothesis \\(p_1 \\ne p_2\\).5 We’ll use the 5% level.\nThe “pooled” estimate is based on \\(x_1\\) and \\(x_2\\), which we can find based on \\(\\hat p_1\\) and \\(n_1\\). Since \\(\\hat p_1 = x_1/n_1\\), we can find \\(x_1 = \\hat p_1 n_1 = 771 * 0.47 = 362.37\\), which we’ll round to 362. Similarly, we’ll use \\(x_2\\) as 249. \\[\n\\hat p = \\frac{362 + 249}{771 + 732} = 0.4065\n\\]\nThe test statistic is calculated as \\[\n\\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = \\frac{(0.47 - 0.34) - 0}{\\sqrt{0.4065(1-0.4065)(1/771 + 1/732)}} = 5.12\n\\]\nWe all remember the all-important value of 1.96, right? The total area under the normal curve above 1.96 plus the area below -1.96 adds to 5%. If we get a z-score above 1.96 or below -1.96, we know that the p-value is smaller than 5%. Intuitively, 5.12 is a massive z-score, and thus will have a miniscule p-value.\n\n2 * (1 - pnorm(5.12))\n\n[1] 3.055357e-07\n\n\nThat’s a p-value of approximately 0.0000003. We can safely reject the null hypothesis.\nThis isn’t surprising, the original proportions were 0.47 and 0.35, with sample sizes of 771 and 732. Given the sample size, we expect a pretty small standard error and thus we shouldn’t be surprised that a difference of 0.13 counts as a “big” difference!"
  },
  {
    "objectID": "L20-Two_Proportions.html#example",
    "href": "L20-Two_Proportions.html#example",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.3 Example",
    "text": "17.3 Example\nThe following example comes from OpenIntro Statistics for Health and Life Sciences.\nThe use of screening mammograms for breast cancer has been controversial for decades because the overall benefit on breast cancer mortality is uncertain. Several large randomized studies have been conducted in an attempt to estimate the effect of mammogram screening. A 30-year study to investigate the effectiveness of mammograms versus a standard non-mammogram breast cancer exam was conducted in Canada with 89,835 female participants. During a 5-year screening period, each woman was randomized to either receive annual mammograms or standard physical exams for breast cancer. During the 25 years following the screening period, each woman was screened for breast cancer according to the standard of care at her health care center.\nAt the end of the 25 year follow-up period, 1,005 women died from breast cancer. The results by intervention are summarized below.\n\n\n\n\nDied\nSurvived\n\n\n\n\nMammogram\n500\n44,425\n\n\nControl\n505\n44,405\n\n\n\nAssess whether the normal model can be used to analyze the study results.\nSince the participants were randomly assigned to each group, the groups can be treated as independent, and it is reasonable to assume independence of patients within each group. Participants in randomized studies are rarely random samples from a population, but the investigators in the Canadian trial recruited participants using a general publicity campaign, by sending personal invitation letters to women identified from general population lists, and through contacting family doctors. In this study, the participants can reasonably be thought of as a random sample.\nThe pooled proportion \\(\\hat{p}\\) is\n\\[\n\\hat{p} = \\dfrac{x_{1} + x_{2}}{n_{1} + n_{2}} = \\dfrac{500 + 505}{500 + 44425 + 505 + 44405} = 0.0112\n\\]\nChecking the success-failure condition for each group: \\[\\begin{align*}\n\\hat{p} \\times n_{mgm} &= 0.0112 \\times \\text{44,925} = 503\\\\\n(1 - \\hat{p}) \\times n_{mgm} &= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\\n\\hat{p} \\times n_{ctrl} &= 0.0112 \\times \\text{44,910} = 503\\\\\n(1 - \\hat{p}) \\times n_{ctrl} &= 0.9888 \\times \\text{44,910} = \\text{44,407}\n\\end{align*}\\] All values are at least 10.6\nThe normal model can be used to analyze the study results.\nWe can use this information to do a hypothesis test for the equality of proportions.\nThe standard error is still: \\[\n\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)} = 0.000702\n\\] That’s quite small, but this is to be expected with such a large sample size.\nThe test statistic is \\(\\hat p_1 - \\hat p_2 / 0.000706 = -0.17\\). Again, using our intuition, this is way lower than our 1.96 value, so this is very much not a significant result.\nWe conclude that there is insufficient evidence to reject the null hypothesis; the observed difference in breast cancer death rates is reasonably explained by sampling error when the two proportions are equal.\nEvaluating medical treatments typically requires accounting for additional evidence that cannot be evaluated from a statistical test. For example, if mammograms are much more expensive than a standard screening and do not offer clear benefits, there is reason to recommend standard screenings over mammograms. This study also found that a higher proportion of diagnosed breast cancer cases in the mammogram screening arm (3250 in the mammogram group vs 3133 in the physical exam group), despite the nearly equal number of breast cancer deaths. The investigators inferred that mammograms may cause over-diagnosis of breast cancer, a phenomenon in which a breast cancer diagnosed with mammogram and subsequent biopsy may never become symptomatic. The possibility of over-diagnosis is one of the reasons mammogram screening remains controversial."
  },
  {
    "objectID": "L20-Two_Proportions.html#footnotes",
    "href": "L20-Two_Proportions.html#footnotes",
    "title": "17  Inference for the Difference in Proportions",
    "section": "",
    "text": "Recall that independence means that knowledge of one outcome gives you a better guess at other outcomes.↩︎\nIn this example, we could find the difference in heights between spouses, then use this collection of differences in a one-sample t-test, which gives us different information, but it’s also interesting.↩︎\nThis is to ensure you have the correct caclulation, you won’t need to do this on a test.↩︎\nThere may be a time in your life where you test whether \\(p_1 - p_2 = 0.25\\) or something like that, and you’ll need to modify the methods a little bit.↩︎\nYou might have also chosen \\(p_1 &gt; p_2\\) if you thought, before seeing the results of the study, that the original order would lead to more agreement.↩︎\nIt is worth noting that these values are very close to the original values we were given. If we were doing a confidence interval where we don’t use the pooled proportion, we could have just checked the values in the given table!↩︎"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#differences-in-proportions-independence",
    "href": "L21-ChiSquare_Multiple_Proportions.html#differences-in-proportions-independence",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.1 Differences in Proportions; Independence",
    "text": "18.1 Differences in Proportions; Independence\nRecall the following example from the lesson on multiple proportions.1 We were interested in whether getting a mammogram lead to fewer deaths due to breast cancer, and were presented the following data:\n\n\n\n\nDied\nSurvived\nTotal\n\n\n\n\nMammogram\n500\n44,425\n44,925\n\n\nControl\n505\n44,405\n44,910\n\n\nTotal\n1,005\n88,830\n89,835\n\n\n\nIn that lesson, we asked whether the proportion of people who died was the same in the mammogram group and the control group. This is a very specific approach, and in this lesson we will generalize it to many situations.\nThe question can be re-worded as “Does knowing that the patient got a mammogram tell us more about whether they survived?” This phrasing should sound familiar - it’s a question about independence! Instead of asking about the difference in proportions, we can ask about whether the survival of the patient is independent of the method of screening.\nIn essence, we’re checking all of the potential conditional probabilities. This includes P(Died | Mammogram) \\(\\stackrel{?}{=}\\) P(Died) as well as P(Mammogram | Died) \\(\\stackrel{?}{=}\\) P(Mammogram). Technically, these two statements are equivalent, so we can think about it whichever way is more useful. The test we’re about to describe also tests for whether P(Died | Control) \\(\\stackrel{?}{=}\\) P(Died) at the same time.\n\n\n\n\n\n\nTest for Independence of Columns and Rows\n\n\n\nThe Chi-Square test that we are about to learn is a test of whether the rows of a two-way table are independent of the columns. This works no matter how many rows/columns there are.\n\\[\nH_0: \\text{The rows are indepenent of the columns} vs. \\text{There is some form of dependence}\n\\] The Chi-Square test gives a significant result if there is any deviation from independence, even if it’s just one cell in the two-way table that doesn’t fit the pattern.\n\n\nThe interpretation of the test is that all of the rows look the same as each other; the counts in the rows are random deviations from the same distribution. The same interpretation applies to columns.\nIn this example, the test for a difference in proportions is the exact same idea as a test for independence of rows and columns, but this will generalize the same idea to any number of rows/columns."
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#expected-counts",
    "href": "L21-ChiSquare_Multiple_Proportions.html#expected-counts",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.2 Expected Counts",
    "text": "18.2 Expected Counts\nJust like in the tests for two proportions, we’re going to see what would have happened if there was actually no difference. That is, what would the table above look like if the outcome was independent of the screening method?\nAs you’ll clearly recall, we can multiply probabilities if they are independent. That is, \\[\nP(A\\text{ and }B) \\stackrel{indep}{=}P(A)P(B),\n\\] where, again, I stress that this is only true if events A and B are independent.\nFor hypothesis tests, we calculate things assuming that the null hypothesis is true. In this case, we assume that the events are independent and thus we can multiply their probabilities. So the proportion of people we expect to see in the “mammogram and died” group is: \\[\nP(\\text{mammogram and died}) = P(\\text{mammogram})P(\\text{died}) = \\left(\\frac{44925}{89835}\\right)\\left(\\frac{1005}{89835}\\right) \\approx 0.5662\n\\] Where did these numbers come from? \\(P(\\text{mammogram})\\) is the number of people in the mammogram row divided by the total number of people. That is, this is the proportion of people who were screened via mammogram, regardless of whether they survived. Similarly, 10005 is the number of people who did not survive, regardless of whether they were screened via mammogram.2 Note that this the probability of mammogram and died, not the probability of death given that they were screened via mammogram.\nThis is the proportion of patients, so the expected count is just \\(np\\), the sample size times the proportion. Notice what happens to the calculation when we include this number: \\[\n88935 * P(\\text{mammogram and died}) = 88935 \\left(\\frac{44925}{89835}\\right)\\left(\\frac{1005}{89835}\\right) = \\frac{1005*44295}{89835} = 502.6\n\\]\n\n\n\n\n\n\nExpected Counts for a Two-Way Table\n\n\n\n\\[\n\\text{Expect count for the cell in row }i\\text{, column }j = \\frac{(\\text{row }i\\text{ total})(\\text{column }j\\text{ total})}{\\text{table total}}\n\\]\n\n\nThe following table shows the actual counts and expected counts in the format actual(expected). For practice, double check the calculations!\n\n\n\n\nDied\nSurvived\nTotal\n\n\n\n\nMammogram\n500 (502.6)\n44,425 (44,422.4)\n44,925\n\n\nControl\n505 (502.4)\n44,405 (44,407.6)\n44,910\n\n\nTotal\n1,005\n88,830\n89,835\n\n\n\n\nThe Chi-Square Test Statistic\nUp until this exact moment, all our test statistics have been of the form (observed - hypothesized)/standard error. This ends here. Here we’ll introduce the Chi-Square test statistic, often written as \\(\\chi^2\\), which is greek letter “chi”, pronounced “kai”.\n\n\n\n\n\n\nThe \\(\\chi^2\\) Test Statistic\n\n\n\nAfter gathering the observed counts and calculating the expected counts, the “Chi-Square” test statistic is: \\[\n\\chi^2 = \\sum_{\\text{all cells}}\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\n\n\nThere are a couple important features of this value:\n\nThe numbers are squared so that negatives don’t cancel out wiht positives.\nWe divide by expected counts, which means that a large deviation is okay if it’s for a large count.\n\nFor example, 500 is 5 away from 505 and 44425 is 20 away from 44405, but the 500 and the 505 “feel” like they’re closer together because the counts are small. With large counts, we’re more forgiving of observed minus expected.\n\n\nThis test statistic is based on the normal approximation to the binomial distribution, so you’d better believe that there are some conditions before we can do a hypothesis test!\n\nEach individual must be independent of each other individual.\n\nThis is very different from assuming that the clomn variable is independent of the row variable.\nFor example, random sampling will ensure independence of individuals in the study.\n\nEach expected cell count must be larger than 10.\n\nSome textbooks use the looser rule that at most 1/5th of the expected counts are less than 5. This gets confusing, and you really just need to ensure that you have a large enough sample in each cell of the two-way table.\n\n\nFor the mammogram example, these conditions are satisfied. Verify that the \\(\\chi^2\\) test stat is 0.02.\nThe p-value for a \\(z\\) test statistic is calculated from the normal distribution, the p-value for a \\(t\\) test statistic is calculated from a \\(t\\) distribution, and the \\(\\chi^2\\) test statistic is calculated from a \\(\\chi^2\\) distribution!\nThe null hypothesis for this test is simply that the rows and columns are independent, with the alternate hypothesis being that this is false. Because of the way the \\(\\chi^2\\) statistic is calculated, any difference between observed and expected increases the test statistic. In other words, we only really care about the upper tail.\nBefore we can calculate a p-value, we need to know the degrees of freedom. Again, this is a confusing concept that is often best memorized. For a two-way table, the df is \\[\ndf = (r-1)(c-1)\n\\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns.\nWe can calculate the right-tailed p-value as follows:\n\n1 - pchisq(0.02, df = 1)\n\n[1] 0.8875371\n\n\nThat’s nearly 1, so there’s no reasonable significance level for which this test would be significant. We conclude that it’s reasonable to think that the rows and columns are independent3, and so we can say that there’s no difference in outcome across different methods of screening.4\n\n\n\n\n\n\nThe \\(\\chi^2\\) Test\n\n\n\nThe \\(\\chi^2\\) test calculates the difference in observed counts and what would be expected if the rows and columns were independent, then finds a one-tailed p-value to tell whether the observed and expected counts are too different.\nA significant p-value means there is some sort of dependence, even if it’s just one cell that is sufficiently different.\n\n\n\n\nThis Example in R\nPreparing the data for R is relatively difficult, so ignore those details.\n\nmammograms &lt;- as.table(cbind(c(500, 44425), c(505, 44405)))\ncolnames(mammograms) &lt;- c(\"Died\", \"Survived\")\nrownames(mammograms) &lt;- c(\"Mammogram\", \"Control\")\nmammograms\n\n           Died Survived\nMammogram   500      505\nControl   44425    44405\n\nchisq.test(mammograms)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  mammograms\nX-squared = 0.01748, df = 1, p-value = 0.8948\n\n\nAgain, R does a better calculation using continuity correction, which is out of the scope of this course.\nFor this simple example, note that this is the same as a two-sample test for proportions (but with a different form of contnuity correction):\n\nprop.test(x = c(500, 505), n = c(44425, 44405))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(500, 505) out of c(44425, 44405)\nX-squared = 0.017976, df = 1, p-value = 0.8933\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.001531197  0.001295859\nsample estimates:\n    prop 1     prop 2 \n0.01125492 0.01137259 \n\n\n\n\nAnother Example in R\nThe following data come from the help file for the chisq.test() function in R.\n\nparty_by_gender &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\n# The following line is just to make sure we get pretty output\n# It is NOT something you'd be expect to reproduce\ndimnames(party_by_gender) &lt;- list(gender = c(\"F\", \"M\"),\n    party = c(\"Democrat\",\"Independent\", \"Republican\"))\nparty_by_gender\n\n      party\ngender Democrat Independent Republican\n     F      762         327        468\n     M      484         239        477\n\n\nWe want to know whether the party affiliation is independent of the gender. By eye, it looks like there are more women in the democratic party, slightly more in the Independent party, and about the same in the republican party. However, there are more women in general in this study, so it’s not immediately obvious that this is a difference in party affiliation or a difference in sample sizes across groups. This is where the \\(\\chi^2\\) test works best!\nLet’s use the built-in R function to save us some work.5\n\nchisq.test(party_by_gender)\n\n\n    Pearson's Chi-squared test\n\ndata:  party_by_gender\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n\n\nWe can see that the \\(\\chi^2\\) test statistic is 30.07, the degrees of freedom is (2-1)*(3-1)=1*2=2, and the resultant p-value is about 3 times ten to the negative 7. This is definitely a statistically significant relationship, and we can conclude that there’s a difference in party affiliation across genders.\nNow that we know there’s a statistically significant difference, we can see where this difference is. We can look at which observed values are furthest from the expected values. Like in linear regression, we are looking at the residuals.\n\n\n\n\n\n\nResiduals for a \\(\\chi^2\\) Test\n\n\n\nFor the cell in row \\(i\\) and column \\(j\\), the residual is defined as: \\[\n\\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}}\n\\] This is just the square root of their contribution to the \\(\\chi^2\\) test statistic, which preserves the sign (expected counts that are too small are still negative).\n\n\n\n# Rounding the values for nicer display\nround(chisq.test(party_by_gender)$residuals, 2)\n\n      party\ngender Democrat Independent Republican\n     F     2.20        0.41      -2.84\n     M    -2.50       -0.47       3.24\n\n\nThe main thing that sticks out to me is that the count for republican women and republican men was about the same, but this is actually way more men than expected due to the sample size!"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#confidence-intervals",
    "href": "L21-ChiSquare_Multiple_Proportions.html#confidence-intervals",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.3 Confidence Intervals",
    "text": "18.3 Confidence Intervals\nLet’s not.6\n\nChi-Square and Power\nIn previous lectures, we saw that power is our ability to reject a false null.\n\nIf we’re looking for a “small” difference, we need a larger sample size.\n\nWhat’s a “small” difference in a Chi-Square test?\n\nNot calculating a single estimate, so there’s no one value to compare to a hypothesis.\nInstead, the Chi-Square statistic is the important thing. It’s an overall measure of the difference between observed and expected.\n\nThe \\(\\chi^2\\) statistic is divided by expected \\(\\implies\\) differences are less important when expected is larger. Recall:\n\\[\n\\chi^2 = \\sum_{\\text{all cells}}\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\n\nA value that’s 10 units away from the expected count…\n\nIs a large difference if the expected count is 2\nIs a small difference if the expected count is 2,000\n\n\nIn other words, the interpretation of the difference between observed and expected depends on the size of the expected count, and with a larger sample size we get a larger expected count.\nThis is the same as with means and proportions!!! There’s no “standard error” for Chi-Square tests, but there’s still a concept of “larger sample means smaller variance in the sampling distribution”!"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#chi-square-for-goodness-of-fit",
    "href": "L21-ChiSquare_Multiple_Proportions.html#chi-square-for-goodness-of-fit",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.4 Chi-Square for “Goodness of Fit”",
    "text": "18.4 Chi-Square for “Goodness of Fit”\nIn the lesson so far, the “expected” counts were the counts that would be expected if the null hypothesis were true, that is, if the rows and columns were independent. We can define the expected counts differently and still use the \\(\\chi^2\\) test!\nIn particular, we can check whether a hypothesized distribution works for a given set of data.7 For example, we can check whether the demographics of a study are the same as the demographics in the population. The following example comes from the OpenIntro textbook, where it discusses a study called the “FAMuSS” study.\n\n\n\n\nAfrican American\nAsian\nCaucasian\nOther\nTotal\n\n\n\n\nFAMuSS\n27\n55\n467\n46\n595\n\n\nUS Census\n0.128\n0.01\n0.804\n0.058\n1\n\n\nExpected\n79.16\n5.95\n478.38\n34.61\n595\n\n\n\nIn this example, we know the true distribution of ethnicities in the population, and we’re testing whether the demographics in the study follow this distribution.\nThe “Expected” counts are simply the census proportions times the sample size. We can see visually that there’s a difference, but are these differences big compared to sampling error? A hypothesis test will save us!\nWe can calculate the \\(\\chi^2\\) statistic in the exact same way: \\[\n\\chi^2 = \\sum_{\\text{all cells}}\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\] and compare this to a \\(\\chi^2\\) distribution. As before, I’m too lazy to do this by hand and I want R to do it for me. Let’s use the usual 5% significance level.\n\nobserved &lt;- c(27, 55, 467, 46)\nhypothesized &lt;- c(0.128, 0.01, 0.804, 0.058)\nchisq.test(x = observed, p = hypothesized)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 440.18, df = 3, p-value &lt; 2.2e-16\n\n\nAccording to R, the demographics are significantly different!\nFor more examples of the goodness-of-fit test, see this free, open-source textbook"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#crowdsourced-questions",
    "href": "L21-ChiSquare_Multiple_Proportions.html#crowdsourced-questions",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.5 Crowdsourced Questions",
    "text": "18.5 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nA study on consumer preference for four brands of lipbalm product yielded the following results, with a sample size of 200:\n\n\n\n\nBrand\nCount\n\n\n\n\nA\n40\n\n\nB\n50\n\n\nC\n70\n\n\nD\n40\n\n\n\nBased on market research, the company hypothesizes that the preferences are evenly distributed as follows: Brand A - 25%, Brand B - 25%, Brand C - 25%, and Brand D - 25%.\n\nCalculate the expected frequencies for each brand based on the hypothesized distribution.\nUsing the Chi-Square goodness of fit test, calculate the Chi-Square statistic to determine if there is a significant difference between the observed and expected frequencies. Assume a significance level of 5%.\n\n\n\nSolution\n\n\nCalculating Expected Frequencies:\n\nGiven the hypothesized distribution, each brand is expected to have an equal preference of 25% among the 200 respondents.\ntherefore, the expected frequency for each brand is 200×0.25=50.\n\nCalculating the Chi-Square Statistic:\n\nThe Chi-Square statistic (x2 ) is calculated using the formula: x2 = ((observed - expected)2)/expected\nFor Brand A: (40-50)2/50 = 2\nFor Brand B: (50-50)2/50 = 0\nFor Brand C: (70-50)2/50 =8\nFor Brand D: (40-50)2/50 = 2\nTotal x2 =2+0+8+2=12\n\n\nto interprete the results we must find the degrees of freedom: The degrees of freedom is equal to the number of categories minus one, df=4−1=3.\nIf x2 calculated (12) is greater than the critical value from the Chi-square table, we will reject the null hypothesis and conclude that there is a significant difference between the observed and expected frequencies\nNot covered in class, but this can be calculated as follows:\n\n1 - pchisq(12, df = 3)\n\n[1] 0.007383161\n\n\nAnd this is statistically significant at the 5% level, meaning we have found a deviation from equal probabilities in brand preference."
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#footnotes",
    "href": "L21-ChiSquare_Multiple_Proportions.html#footnotes",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "",
    "text": "Adapted from OpenIntro BioStats.↩︎\nIn other words, they’re the row probabilities regardless of column and the column probability regardless of row.↩︎\nWe’re searching for evidece against the null, we can never conclude that the null is true!↩︎\nWe can also say that there’s no difference in levels of screening across outcomes, but this isn’t meaningful given the context of the data.↩︎\nFor practice try to calculate these by hand!↩︎\nThere’s not really a single statistic that’s worth making a CI for. We could make one for each expected count, but that’s silly.↩︎\nThe name “Goodness of Fit” is often used for this, but it’s a bad name. The null hypothesis is that the observed data fit with the given distribution, but we never confirm the null so we can never say that it’s a “good” fit.↩︎"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#return-to-regression",
    "href": "L22-Inference_for_Regression.html#return-to-regression",
    "title": "19  Inference for Regression",
    "section": "19.1 Return to Regression",
    "text": "19.1 Return to Regression\nIn the lessons on regression, there was a recurrent theme: if the correlation was 0, then the slope was 0 (and vice-versa, since \\(b = rs_y/s_x\\)). However, in real data the correlation is never exactly 0. How do we know if it’s “close enough” to 0 to say that there’s no correlation between \\(x\\) and \\(y\\)? By comparing it to a standard error, of course!\nAfter the previous lesson (the Chi-Square test), this lesson is a return to form. We’re going back to t-tests! Hooray! But first, let’s do a quick recap on regression."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#regression-recap",
    "href": "L22-Inference_for_Regression.html#regression-recap",
    "title": "19  Inference for Regression",
    "section": "19.2 Regression Recap",
    "text": "19.2 Regression Recap\nIn regression, we’re trying to find parameters \\(a\\) and \\(b\\) in the equation \\(y = a + bx\\) to make sure that the fitted line is as “close” to the observed data as possible.\nTo find the line of best fit, we minimize the sum of squared errors, \\(\\sum(y_i - \\hat y_i)^2\\), where \\(\\hat y_i\\) is the height of the line that we get if we plug the \\(x\\) value into the model. The fact that we minimize the squares is not important, but it is important that it’s based on the quantity \\(y_i - \\hat y_i\\), called the residuals.\nFor example, consider the penguins data that we looked at earlier. In these data, we’re trying to predict the body mass of a penguin based on their flipper length. This is useful to field researchers, since measuring flipper length is much easier than weighing a penguin and still gives them some idea of how much that penguin might weigh.\n\n\n\n\n\nThe blue line is the line of best fit, which was estimated as: \\[\nbody\\_mass_i = -6787.28 + 54.62*flipper\\_length_i\n\\]\nThe slope is 54.62, meaning that the weight of the penguin increases, on average, by 54.62 grams for each 1mm increase in the flipper length. In other words, if we look at all pairs of penguins that had flipper lengths that were 1mm apart, the average difference in their body masses would be something like 54.62.1\nThe slope was calculated using our formulas from before. The correlation between flipper length and body mass is 0.7027, the standard deviation of flipper length is 6.4850, and the sd of body mass is 504.1162. Putting these together, the slope is \\(0.7072 * 504.1162 / 6.4850 = 54.9747\\). This is slightly off because of rounding - I calculated this one by hand, but the slope in the equation above was calculated with R.\nThe intercept of this model is -6787.28, which could be interpreted as saying that a penguin with a flipper length of 0 should have a body mass of about -7kg, but this isn’t how we should interpret this value.2 This intercept simply exists to shift the line up or down in order to best fit the cloud of points.\nThis interpretation of the intercept as a “nuisance” parameter3 can be seen in the way we calculate it. The calculation is \\(a = \\bar y - b\\bar x\\), i.e., the intercept is calculated to ensure we have a line with slope \\(b\\) that goes through the point \\((\\bar x, \\bar y)\\) on the plot.\nThe red line represents the residual for one of the penguins. This particular penguin had a flipper length of 207, leading to a predicted body mass of \\(-6787.28 + 54.62*207 = 4519.06\\). This particular penguin had an actual body mass of 5050, giving a residual of \\(5050 - 4519.06 = 530.94\\)."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#inference-for-the-slope-parameter",
    "href": "L22-Inference_for_Regression.html#inference-for-the-slope-parameter",
    "title": "19  Inference for Regression",
    "section": "19.3 Inference for the Slope Parameter",
    "text": "19.3 Inference for the Slope Parameter\nYou may have noticed a word show up several times in that recap: “average”. The intercept passes through the average of x and the average of y, and the slope is the average increase in \\(y\\) for a one-unit increase in \\(x\\). Linear regression is essentially just a 2 dimensional average!\nAs you might guess from this fact, we’re going back to t-tests! We still have the exact same test statistic: \\[\nt_{obs} = \\frac{\\text{Observed} - \\text{Hypothesized}}{\\text{Standard Error}}\n\\] We just have to decide on the hypotheses and calculate the standard error!\nAs noted above,4, the slope is 0 when the correlation is 0. In general, we are checking the hypotheses: \\[\\begin{align*}\nH_0: \\beta = 0\\text{ vs. }H_a: \\beta\\ne 0\\\\\n\\end{align*}\\] We are now equipped to fill out the test statistic: \\[\nt_{obs} = \\frac{b - 0}{\\text{Standard Error}}\n\\] We can just plug that into a calculator and get a result, right? Wait, something might be missing.\nIn this class, we won’t even write out the equation of the standard error. This is the sort of thing software was designed to do for you. By now, you should be able to explain the concept of the standard error to a grandparent; it’s a central part of everything we’ve done since the midterm. You should also know why it decreases with a larger sample size, and how this affects the test statistic and p-value! However, it’s fine to skip over the actual value for now and simply trust that statisticians are smart.\nWith the Standard Error being calculated by software, this hypothesis test works exactly the same as the test for a mean.\n\nAssumptions\nAs always, statistics is built on making some assumptions about the population that allow us to make inferences from a sample. The assumptions should be pretty obvious.\n\n\n\n\n\n\nLinear Regression Assumptions\n\n\n\n\nThere is some true relationship \\(y = \\alpha + \\beta x\\)\n\nThat is, the model is actually a straight line relationship with no curves.\n\nThe deviations above and below this line are normally distributed.\n\nThat is, the height of the line at a given \\(x\\) value is normal, with the mean being the height of the line.\nPut another way: The residuals are normal.\n\nThe individuals are independent of each other.\nThe variance above and below the line doesn’t depend on the \\(x\\) value.\n\n\n\n\nViolating Assumption 1\nThere is one way for a line to be straight, and an infinite number of curved lines. Basically, the plot of \\(y\\) against \\(x\\) should look linear.\n\n\n\n\n\nIn the plot above, there is clearly not a linear relationship. The math works out just fine and we can calculate a straight line that minimizes the sum of squared error, but it doesn’t tell us anything about the population.\n\n\nViolating Assumptions 2 - 4\nAgain, there are many ways to violate these assumption. A good example might be the stock price of Apple Computers (or any stock).\n\nThe price on one day is going to be close to the price the day before.\n\nNot independent!\n\nWhen Apple holds a press conference, there will be a lot more variability in the stock price depending on what they announce.\n\nThe variance depends on the \\(x\\) value!\nThis also violates the assumption of normality. Large stock price changes are to be expected, but the normal distribution doesn’t allow for this!"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#regression-in-r",
    "href": "L22-Inference_for_Regression.html#regression-in-r",
    "title": "19  Inference for Regression",
    "section": "19.4 Regression in R",
    "text": "19.4 Regression in R\nCalculating things by hand helps you conceptualize what’s going on, but it’s impractical for actual practice. As a “Statistical Programming Language”, R has so many useful functions built in.\nFor this example, we’ll use the mtcars data that is built into R, so we don’t have to worry about loading in new data. These data include various measurements of a sample of cars in the 1970s. For our purposes, we’re going to determine the relationship between fuel efficiency (\\(y\\)) as measured in miles per gallon (mpg), and weight (\\(x\\)) measured in units of 1,000 lbs.\nWe’ll start by checking a plot. I sometimes use “base R” plotting, and sometimes use “ggplot”. Neither will be tested on the final exam, but I like pointing out the distinction. Base R plotting has notation that matches the syntax of linear modelling, so it’s useful to include here.\n\ndata(mtcars) # built-in data in R\n\n# Base R plot: y ~ x, data = ...\nplot(mpg ~ wt, data = mtcars)\n\n# Linear Model (lm): y ~ x, data = ...\nmylm &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Add the line to the existing plot\nabline(mylm)\n\n\n\n\nAs always: check assumptions first!\n\nThe plot above looks pretty linear.\n\nThere might be a slight curve to the line, though. The points at the left and right are mostly above the line, but the points in the middle are mostly below the line. This doesn’t appear to be a strong pattern, but it’s something to note when making a conclusion.\n\nExtrapolation is definitely not possible, but a linear model might explain the data in this range.\n\n\nThere aren’t any obvious outliers, but we’ll need to look at a different plot to really check this assumption.\nFrom the sampling strategy, I feel comfortable saying that the observations are independent.\n\nThere’s no possible test for this, it’s all about having a good sampling strategy!\n\nThis is probably fine, but again we should check other plots before we make a conclusion.\n\nThere are two assumptions that we were not able to test by looking at a plot of mpg against weight. R has some built-in plotting methods that help us with these assumptions.\n\n# Create a plotting space with 2 rows and 2 columns\n# \"mfrow\" = Multiple Figures, filled in ROW-wise\npar(mfrow = c(2,2))\n\n# The basic plot function for the output of lm\n# makes 4 different plots.\nplot(mylm)\n\n\n\n\nSome notes on these plots:\n\nResiduals versus Fitted: This is usually better to look at than the \\(y\\) versus \\(x\\). When you get into more than one \\(x\\) variable, it can be difficult to look at all of the plots, and this tells us more information anyway.\n\nFor this example, we can see the pattern again: points above the line on the left and right, and below the line in the middle. The red line helps highlight this.\n\nNormal Q-Q: This plot checks whether the residuals are normal. We’ll skip the details of how this plot is made, but it’s useful to have an intuition about these plots. Essentially, if the residuals are normal then everything should fall exactly on the dotted line. Due to random sampling it won’t, so we’re mainly looking for systematic deviations from the line. I’ve added some code at the end of this lesson for you to check this.\n\nThese data look okay, but not perfect. The residuals are possibly heavy tailed.5\n\nScale-Location: This is essentially the absolute value of the residuals, which shows whether the variance is the same for all values of \\(x\\). We want this to look like there is no pattern.\n\nThe red line wiggles a bit, but this is to be expected. It looks pretty good to me!\n\nResiduals versus Leverage: This plot is awkward to read, but shows us if any of the points are affecting the line by a lot. Essentially, we’re looking for any points on the wrong side of the dotted lines (“Cook’s Distance”). Above the 0.5 dotted line is something to look into, and something above 1 is bad for the model. Again, I’ve added an appendix about “leverage”.6\n\nNothing outside of that 0.5 dotted line, so this should be good!\n\n\nNow that we’ve looked at the plots to check our assumptions7, we can look at our estimates and our p-values.\nThe output of the lm() function isn’t very user-friendly, but the summary() function makes it nicer.\n\nsummary(mylm)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nLet’s walk through this output!\n\nThe Call: is the R code we used to make this model.\nThe Residuals show the five number summary for the residuals. If they’re normal with a mean of 0 as we assume, then they should have a minimum that is the negative of the maximum, a Q1 that is the negative of the Q3, and a median of 0.\n\nThis is a quick check to see whether the residuals are symmetric.\nFor this example, these results aren’t ideal but we also have a small data set so we can be a little forgiving.\n\nThe Coefficients table is where the magic happens.\n\n(Intercept) is our estimate of \\(a\\). Again, this is a nuisance parameter that we’re not super interested in right now.\nwt corresponds to our estimate of \\(b\\). The slope is estimated as \\(b=-5.3445\\), and it provides the standard error and the t test statistic for us!\n\nThe test statistics is (Estimate - 0)/Std. Error, where the “- 0” comes from the null hypothesis that \\(\\beta = 0\\).\nThe three stars at the end of the line show significance level. *** means significant at the 0.1% level, ** is significant at the 1% level, * is 5%, and . is 10%. You should always set your significance level before looking at this table, but it gives a nice quick visual check for significance.8\n\n\nThe last block of text shows some important quantities.\n\nThe “Multiple R-squared” value is what we learned previously, whereas the “Adjusted R-squared” is something you will need to learn about when moving into multiple linear regression. For practice, try to figure out which (if any) is equal to the square of the correlation between mpg and wt!\nThe F-statistic row is also going to be very important when you move into multiple linear regression.\n\n\nLooking at this output, we can see that the slope parameter associated with wt, the weight of the car, is significantly different from 0. This means that there is statistically significant correlation between the fuel efficiency of a car and it’s weight. This isn’t surprising, but it’s always nice to have a scientific confirmation of what we hypothesized to be true."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#confidence-intervals",
    "href": "L22-Inference_for_Regression.html#confidence-intervals",
    "title": "19  Inference for Regression",
    "section": "19.5 Confidence Intervals",
    "text": "19.5 Confidence Intervals\nSince we know it’s a t-test, we’re still looking at confidence intervals of the form: \\[\n\\text{point estimate}\\pm\\text{critical value}*\\text{standard error}\n\\] where we use the SE given in the table. However, we have changed the degrees of freedom! Recall that the df can be seen as the number of parameters we can estimate from the data9, and we need to estimate the intercept. For this reason, we’ve lost another degree of freedom, so the \\(t\\) critical value is based on \\(n-2\\) degrees of freedom.\nA 95% CI for the slope in the mtcars example can be calculated as follows. We know:\n\nThe point estimate is -5.3445.\nWe’re finding a 95% CI, so we use qt((1 - 0.95)/2).\nThe SE is given from the R output as 0.5591\n\nWe just use this value - we don’t have to divide by \\(\\sqrt{n}\\) or anything like that!\n\n\nThe following R code uses + c(1, -1) to add and subtract (\\(\\pm\\)).\n\n-5.3445 + c(1, -1) * qt((1 - 0.95)/2, 32 - 2) * 0.5591\n\n[1] -6.486335 -4.202665\n\n\nSo a 95% CI for the slope is (-6.49, -4.20). This has the usual10 interpretation that, if we repeated this study many many times, then 95% of the intervals that we construct this way would contain the true population slope.\nUnlike the tests for proportions, this CI is back to having the interpretation of “contains every hypothesized value that would not be rejected by a two-sided hypothesis test”. We already did a test for \\(H_0:\\beta = 0\\) versus \\(H_a:\\beta \\ne 0\\) in which the null was rejected, and indeed 0 is not in this interval."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#conclusion",
    "href": "L22-Inference_for_Regression.html#conclusion",
    "title": "19  Inference for Regression",
    "section": "19.6 Conclusion",
    "text": "19.6 Conclusion\nSo that’s it! It’s a t-test based on a standard error that we’re not going to calculate by hand. Other than the interpretation of the slope and one less degree of freedom, this is basically inference for a single mean!\nYou will still need to keep all of the assumptions of regression in mind. We need a linear relationship, independence, normality of the residuals, and constant variance across values of \\(x\\) for this test to be valid. We must check these assumptions by looking at some plots and critiquing the data collection.\nFor the exam, you’ll be expected to know:\n\nThe assumptions, and how to check them.\n\nIncluding interpreting QQ plots to say whether they’re good or bad, and interpreting whether a point looks like it might be high leverage. (In other words, the Appendices in this chapter are not just optional bonus topics.)\n\nThe interpretation of the hypothesis test and the CI\n\nStated in the context of the problem.\n\n\nA nice exam question might show you the results of plot(mylm) and summary(mylm) and ask you to make a conclusion in the context of the study.11"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#crowdsourced-questions",
    "href": "L22-Inference_for_Regression.html#crowdsourced-questions",
    "title": "19  Inference for Regression",
    "section": "19.7 Crowdsourced Questions",
    "text": "19.7 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nAfter analyzing the results of an experiment, we found a correlation coefficient of 0.75. At the 5% level, does this represent a statistically significant correlation?\n\nWe neglected to establish the significance level beforehand, hence, careful interpretation is needed in relation to the correlation’s significance\nYes, since the correlation coefficient is greater than 0.5\nNo, since the correlation coefficient is greater than 0.5\nNo, since the correlation coefficient is less than 1\nNot enough information for a valid answer.\n\n\n\n\nSolution\n\nThe correlation is not a p-value, so nothing in the question tells us whether this was significant. While a correlation of 0.75 may feel strong, this can happen with a small sample size, and thus it still is not statistically significant.\nRemember that there are no definite “strong” or “weak” correlations - they must all be interpreted in context!"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#appendix---leverage",
    "href": "L22-Inference_for_Regression.html#appendix---leverage",
    "title": "19  Inference for Regression",
    "section": "19.8 Appendix - Leverage",
    "text": "19.8 Appendix - Leverage\nThe word “leverage” comes from the word “lever”, which is intentional. If you think of the line of best fit as a see-saw, a high leverage point is a point that either pushes the see-saw down or pulls it up.\nAn outlier is a point that doesn’t really fit into the pattern. There isn’t a single way to define what an “outlier” is in 2 dimensions12, so we have to be smart about it. Usually, we refer to an outlier as a point that’s far from the mean of x and y.\nNot all outliers are high leverage, though! The following plots demonstrate this idea. Both plots show the same data, but with an extra outlier. The first plot has an outlier at an x value of \\(\\bar x - 6\\) and a y value at \\(\\bar y + 15\\). The second plot has an outlier with the same \\(x\\) value, but the \\(y\\) value is \\(\\bar y - 15\\).\nThese two potential outliers are the exact same distance from the mean of \\(x\\) and the mean of \\(y\\), but have very different effects on the line! Including the red point changes the line a little, while the green point changes the line a lot! Even though they’re the same distance from the mean, the green point has higher leverage.\nThe definition of leverage is much more well-defined than the definition of an outlier. The leverage of a point is a measure of how much the line of best fit would change if that point were not in the data.13 It is possible to have outliers with low leverage. Outliers are points that are far from your data; leverage provides a measure of how well a point fits into the pattern.\n\nlm0 &lt;- lm(y ~ x)\n\nx1 &lt;- c(x, mean(x) - 6)\ny1 &lt;- c(y, mean(y) + 15)\nlm1 &lt;- lm(y1 ~ x1)\n\nx2 &lt;- c(x, mean(x) - 6)\ny2 &lt;- c(y, mean(y) - 15)\nlm2 &lt;- lm(y2 ~ x2)\n\nn1s &lt;- rep(1, n)\n\npar(mfrow = c(1,2))\nplot(x1, y1, col = c(n1s, 2), \n    pch = 16, cex = c(n1s, 2))\nabline(h = mean(y), col = \"grey\")\nabline(v = mean(x), col = \"grey\")\naxis(2, at = mean(y), labels = expression(bar(y)), las = 1)\naxis(1, at = mean(x), labels = expression(bar(x)), las = 1)\nabline(lm0)\nabline(lm1, col = 2)\nlegend(\"topright\", legend = c(\"Without Outlier\", \"With Outlier\"), col = 1:2, lty = 1)\n\nplot(x2, y2, col = c(n1s, 3), \n    pch = 16, cex = c(n1s, 2))\nabline(h = mean(y), col = \"grey\")\nabline(v = mean(x), col = \"grey\")\naxis(2, at = mean(y), labels = expression(bar(y)), las = 1)\naxis(1, at = mean(x), labels = expression(bar(x)), las = 1)\nabline(lm0)\nabline(lm2, col = 3)\nlegend(\"topright\", legend = c(\"Without Outlier\", \"With Outlier\"), col = c(1,3), lty = 1)"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#appendix---interpreting-qq-norm",
    "href": "L22-Inference_for_Regression.html#appendix---interpreting-qq-norm",
    "title": "19  Inference for Regression",
    "section": "19.9 Appendix - Interpreting QQ norm",
    "text": "19.9 Appendix - Interpreting QQ norm\nCopy the following code and past it into RStudio. Run it over and over again to get a feel for QQ plots. Change n as indicated.\nSome things you might notice:\n\nTruly normal data is never perfectly on the line!\n\nWhen interpreting QQ, it’s okay to allow for a little bit of variance!\nMost methods are fairly robust to deviations from normality.\n\nHeavy tailed data (e.g., more variance than expected by the normal distribution) result in one kind of shape, right tailed data result in a different shape.\n\nWhat do you expect left tailed data to look like?\n\n\n\n# Change this to 20, 100, 500, and 10000 to see how much changes\nn &lt;- 500\n\nnormal_sample1 &lt;- rnorm(n, mean = 0, sd = 1)\nnormal_sample2 &lt;- rnorm(n, mean = 0, sd = 1)\nt_sample &lt;- rt(n, df = 5) # Low df to highlight difference from normal\nchisq_sample &lt;- rchisq(n, df = 5)\n\npar(mfrow = c(2,2))\nqqnorm(normal_sample1, main = \"Normal Data 1\")\nqqline(normal_sample1)\nqqnorm(normal_sample2, main = \"Normal Data 2\")\nqqline(normal_sample2)\nqqnorm(t_sample, main = \"t Data - Heavy Tailed\")\nqqline(t_sample)\nqqnorm(chisq_sample, main = \"Chi Square - Right-Tailed\")\nqqline(chisq_sample)"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#footnotes",
    "href": "L22-Inference_for_Regression.html#footnotes",
    "title": "19  Inference for Regression",
    "section": "",
    "text": "This isn’t exactly how it works, but it’s a useful analogy.↩︎\nEvaluating the height of the line at an x-value that is outside the range of our observations is called extrapolation, and should generally be avoided.↩︎\nA nuisance parameter is something we must calculate in order for the model to work but something we’re not planning on interpreting.↩︎\nAnd many times in previous lectures.↩︎\nYou’re not expected to be able to guess this on the exam.↩︎\nThis concept will be on the exam, but not the calculations.↩︎\nNotice how we have to fit the model before we can check the assumptions. The p-values are already calculated, but you should be very careful not to think about them before you’ve checked the assumptions!↩︎\nBad statisticians who violate the issues in the “Inference Cautions” lecture are accused of “chasing stars”.↩︎\nWith one data point, we can estimate the mean but not sd. With two, we can calculate the mean which we need for the sd. And so on.↩︎\nHighly specific, and wrong if you miss any part of it.↩︎\nPossibly with one of the assumptions violated, which you’ll have to catch on your own!↩︎\nThere’s no way to do Q1 - 1.5IQR in both \\(x\\) and \\(y\\).↩︎\nThere is an exact calculation, but we’re just concerned with the concept for now.↩︎"
  },
  {
    "objectID": "L23-ANOVA-oi.html#analysis-of-variance-anova-and-the-f-test",
    "href": "L23-ANOVA-oi.html#analysis-of-variance-anova-and-the-f-test",
    "title": "20  Comparing means with ANOVA",
    "section": "20.1 Analysis of variance (ANOVA) and the \\(F\\)-test",
    "text": "20.1 Analysis of variance (ANOVA) and the \\(F\\)-test\nThe famuss dataset was introduced in Chapter 1, Section 1.2.2. In the FAMuSS study, researchers examined the relationship between muscle strength and genotype at a location on the ACTN3 gene. The measure for muscle strength is percent change in strength in the non-dominant arm (). Is there a difference in muscle strength across the three genotype categories (CC, CT, TT)?\n\nHypotheses\nThe null hypothesis under consideration is the following: \\(\\mu_{\\texttt{CC}} = \\mu_{\\texttt{CT}} = \\mu_{\\texttt{TT}}\\). Write the null and corresponding alternative hypotheses in plain language.\n\n\nSolution\n\n\\(H_0\\): The average percent change in non-dominant arm strength is equal across the three genotypes. \\(H_A\\): The average percent change in non-dominant arm strength varies across some (or all) groups.]\n\n\n\nAssumption Checking\nThe table below provides summary statistics for each group. A side-by-side boxplot for the change in non-dominant arm strength is shown in Figure 5.24; Figure 5.25 shows the Q-Q plots by each genotype.2\n\nIt is reasonable to assume that the observations are independent within and across groups; it is unlikely that participants in the study were related, or that data collection was carried out in a way that one participant’s change in arm strength could influence another’s.\nBased on the Q-Q plots, there is evidence of moderate right skew; the data do not follow a normal distribution very closely, but could be considered to ‘loosely’ follow a normal distribution.3\nNotice from the table that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied for using ANOVA.\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\nSample size (\\(n_i\\))\n173\n261\n161\n\n\nSample mean (\\(\\bar{x}_i\\))\n48.89\n53.25\n58.08\n\n\nSample SD (\\(s_i\\))\n29.96\n33.23\n35.69\n\n\n\n\n\n\n\n\nFigure 2.24: Side-by-side box plot of the change in non-dominant arm strength for 595 participants across three groups.\n\n\n\n\n\n\n\n\n\nFigure 5.25: Q-Q plots of the change in non-dominant arm strength for 595 participants across three groups.\n\n\n\n\n\n\nDifference in Means\nThe largest difference between the sample means is between the and groups. Consider again the original hypotheses:\n\n\\(H_0\\): \\(\\mu_{\\texttt{CC}} = \\mu_{\\texttt{CT}} = \\mu_{\\texttt{TT}}\\)\n\\(H_A\\): The average percent change in non-dominant arm strength (\\(\\mu_i\\)) varies across some (or all) groups.\n\nWhy might it be inappropriate to run the test by simply estimating whether the difference of \\(\\mu_{\\texttt{CC}}\\) and \\(\\mu_{\\texttt{TT}}\\) is statistically significant at a 0.05 significance level?\n\n\nSolution\n\nIt is inappropriate to informally examine the data and decide which groups to formally test. This is a form of **data fishing*; choosing the groups with the largest differences for the formal test will lead to an increased chance of incorrectly rejecting the null hypothesis (i.e., an inflation in the Type~I error rate). Instead, all the groups should be tested using a single hypothesis test.\n\n\n\nMean Squared Error - Within and Between\nAnalysis of variance focuses on answering one question: is the variability in the sample means large enough that it seems unlikely to be from chance alone? The variation between groups is referred to as the mean square between groups (\\(MSG\\)); the \\(MSG\\) is a measure of how much each group mean varies from the overall mean. Let:\n\n\\(\\overline{x}\\) represent the mean of outcomes across all groups\n\nIn other words, it’s the mean of the data when it’s not split into groups.\n\n\\(\\overline{x}_i\\) is the mean of outcomes in a particular group \\(i\\)\n\\(n_i\\) is the sample size of group \\(i\\).\n\nThe mean square between groups is: \\[\nMSG = \\frac{1}{k-1}\\sum_{i=1}^{k} n_{i}\\left(\\overline{x}_{i} - \\overline{x}\\right)^{2} = \\frac{1}{df_{G}}SSG,\n\\] where \\(SSG\\) is the sum of squares between groups, \\(\\sum_{i=1}^{k} n_{i}\\left(\\overline{x}_{i} - \\overline{x}\\right)^{2}\\), and \\(df_{G}=k-1\\) is the degrees of freedom associated with the \\(MSG\\) when there are \\(k\\) groups.\nNotice what’s happening here: \\(MSG\\) is kind of like a variance, but not the variance of individual values.\n\n\n\n\n\n\nMSG is the variance of the means\n\n\n\nInstead of individual data, we’re looking at the mean of each group. With 3 groups, we have 3 different means. The \\(MSG\\) is like calculating the variance based on these 3 values (disregarding the variation within each group).\n\n\nUnder the null hypothesis, there is no real difference between the groups. In other words, the null hypothesis assumes that the groupings are non-informative, such that all observations can be thought of as belonging to a single group. If the null is true, then it the variability between the group means should be equal to the variability observed within a single group. The mean square error (\\(MSE\\)) is a pooled variance estimate with associated degrees of freedom \\(df_E=n-k\\) that provides a measure of variability within the groups. The mean square error is computed as: \\[\nMSE = \\frac{1}{n-k}\\sum_{i=1}^{k} (n_i-1)s_i^{2} = \\frac{1}{df_{E}}SSE,\n\\] where the \\(SSE\\) is the sum of squared errors, \\(n_i\\) is the sample size of group \\(i\\), and \\(s_i\\) is the standard deviation of group \\(i\\).\n\n\n\n\n\n\nThe MSE is the variance as if there are no groups\n\n\n\nThe MSE is almost the same as the variance of the data, assuming we lumped everything together and forgot about the groupings. The only difference is the degrees of freedom!\n\n\nUnder the null hypothesis that all the group means are equal, any differences among the sample means are only due to chance; thus, the \\(MSG\\) and \\(MSE\\) should also be equal. ANOVA is based on comparing the \\(MSG\\) and \\(MSE\\). The test statistic for ANOVA, the F-statistic, is the ratio of the between-group variability to the within-group variability: \\[\nF = \\frac{MSG}{MSE}\n\\]\nThis is a new distribution that we’re not going to talk about too much. The main thing to note is that i’s similar to the Chi-Square distribution: The test statistic is based on adding squared things, so we’re only interested in a right-tailed test.\n\n\n\n\n\n\nThe \\(F\\)-Statistic\n\n\n\nThe F-statistic can be seen as the ratio of the variance between groups and the variance within groups. A large \\(F\\)-stat means that there’s a lot of variance between groups compared to the variance of the data.\nPut another way, the data tells us how much variance to expect so that we have context for the variance of the group means.\n\n\n\n\nCalculating the \\(F\\)-statistic\nCalculate the \\(F\\)-statistic for the famuss data summarized in Figure 5.23. The overall mean \\(\\overline{x}\\) across all observations is 53.29.\n\n\nSolution: By hand\n\nFirst, calculate the \\(MSG\\) and \\(MSE\\). \\[\\begin{align*}\nMSG =& \\frac{1}{k-1}\\sum_{i=1}^{k} n_{i}\\left(\\bar{x}_{i} - \\bar{x}\\right)^{2} \\\\\n=& \\frac{1}{3-1} [(173)(48.89 - 53.29)^{2} \\\\&+ (261)(53.25 - 53.29)^{2} + (161)(58.08 - 53.29)^{2} ]\\\\\n=& 3521.69\n\\end{align*}\\]\nNotice how the MSG is essentially the variance of three observations.\n\\[\\begin{align*}\nMSE =& \\frac{1}{n-k}\\sum_{i=1}^{k} (n_i-1)s_i^{2} \\\\\n=& \\frac{1}{595-3}[(173-1)(29.96^2) + (261-1)(33.23^2) \\\\&+ (161-1)(35.69^2)] \\\\\n=& 1090.02\n\\end{align*}\\]\nThe MSE is almost the same as the variance of all observations if we were to ignore which group they were in!\nThe \\(F\\)-statistic is the ratio: \\[\ndfrac{MSG}{MSE} = dfrac{3521.69}{1090.02} = 3.23\n\\]\n\n\n\nSolution: R\n\nThe following R code will save us many calculations.\nAs in the linear regression section, I use the summary() function to get nicer output.\n\nlibrary(oibiostat)\ndata(famuss)\n\n# The data have uninformative names.\n# ndrm.ch is percentage change in non-dominant arm strength\n# actn3.r577x is the grouping variable (genotype)\nsummary(aov(ndrm.ch ~ actn3.r577x, data = famuss))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nactn3.r577x   2   7043    3522   3.231 0.0402 *\nResiduals   592 645293    1090                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice how the R output givus us - \\(df_G = 592\\), \\(MSG = 3522\\) (rounded from 3521.69) - \\(df_E = 2\\), \\(MSE = 1090\\) (rounded from 1090.02) - \\(MSG / MSE = 3.231\\)\nI will not expect you to do the calculations by hand on any test or assignment.\n\n\n\np-values\nA \\(p\\)-value can be computed from the \\(F\\)-statistic using an \\(F\\)-distribution, which has two associated parameters: \\(df_{1}\\) and \\(df_{2}\\). For the \\(F\\)-statistic in ANOVA, \\(df_{1} = df_{G}\\) and \\(df_{2}= df_{E}\\). An \\(F\\) distribution with 2 and 592 degrees of freedom corresponds to the \\(F\\)-statistic for the genotype and muscle strength hypothesis test,.\nThe larger the observed variability in the sample means (\\(MSG\\)) relative to the within-group variability (\\(MSE\\)), the larger \\(F\\) will be. Larger values of \\(F\\) represent stronger evidence against the null hypothesis. The upper tail of the distribution is used to compute a \\(p\\)-value, which is typically done using statistical software.\n\n\nConclusion\nFrom the R output, the \\(p\\)-value corresponding to the test statistic is equal to about 0.04. Does this provide strong evidence against the null hypothesis at significance level \\(\\alpha = 0.05\\)?\n\n\nSolution\n\nThe \\(p\\)-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis at a significance level of 0.05. The data suggest that average change in strength in the non-dominant arm varies by participant genotype.\n\n\n\n\n\n\n\nThe \\(F\\)-statistic and the \\(F\\)-test\n\n\n\nAnalysis of variance (ANOVA) is used to test whether the mean outcome differs across two or more groups. ANOVA uses a test statistic \\(F\\), which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If \\(H_0\\) is true and the model assumptions are satisfied, the statistic \\(F\\) follows an \\(F\\) distribution with parameters \\(df_{1}=k-1\\) and \\(df_{2}=n-k\\). The upper tail of the \\(F\\)-distribution is used to calculate the \\(p\\)-value.\nThe null hypothesis is false if at least of the means is sufficiently different from the others, relative to the variance in the data."
  },
  {
    "objectID": "L23-ANOVA-oi.html#multiple-comparisons-and-controlling-type-1-error-rate",
    "href": "L23-ANOVA-oi.html#multiple-comparisons-and-controlling-type-1-error-rate",
    "title": "20  Comparing means with ANOVA",
    "section": "20.2 Multiple comparisons and controlling Type 1 Error rate",
    "text": "20.2 Multiple comparisons and controlling Type 1 Error rate\nRejecting the null hypothesis in an ANOVA analysis only allows for a conclusion that there is evidence for a difference in group means. In order to identify the groups with different means, it is necessary to perform further testing. For example, in the famuss analysis, there are three comparisons to make: \\(\\texttt{CC}\\) to \\(\\texttt{CT}\\), \\(\\texttt{CC}\\) to \\(\\texttt{TT}\\), and \\(\\texttt{CT}\\) to \\(\\texttt{TT}\\). While these comparisons can be made using two sample \\(t\\)-tests, it is important to control the Type 1 error rate. One of the simplest ways to reduce the overall probability of identifying a significant difference by chance in a multiple comparisons setting is to use the Bonferroni correction procedure.\nIn the Bonferroni correction procedure, the \\(p\\)-value from a two-sample \\(t\\)-test is compared to a modified significance level, \\(\\alpha^\\star\\); \\(\\alpha^\\star = \\alpha/K\\), where \\(K\\) is the total number of comparisons being considered. For \\(k\\) groups, \\(K=\\frac{k(k-1)}{2}\\). When calculating the \\(t\\)-statistic, use the pooled estimate of standard deviation between groups (which equals \\(\\sqrt{MSE}\\)); to calculate the \\(p\\)-value, use a \\(t\\)-distribution with \\(df_2\\). It is typically more convenient to do these calculations using software.\n\n\n\n\n\n\nBonferroni correction\n\n\n\nThe Bonferroni correction suggests that a more stringent significance level is appropriate when conducting multiple tests: \\[\\begin{align*}\n\\alpha^\\star = \\alpha / K\n\\end{align*}\\] where \\(K\\) is the number of comparisons being considered. For \\(k\\) groups, \\(K=\\frac{k(k-1)}{2}\\).\n\n\n\nBut which group is different?\nThe ANOVA conducted on the famuss dataset showed strong evidence of differences in the mean strength change in the non-dominant arm between the three genotypes. Complete the three possible pairwise comparisons using the Bonferroni correction and report any differences.\nUse a modified significance level of \\(\\alpha^\\star = 0.05/3 = 0.0167\\). The pooled estimate of the standard deviation is \\(\\sqrt{MSE} = \\sqrt{1090.02} = 33.02\\).\nGenotype CC versus Genotype CT: \\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{48.89 - 53.25}{33.02 \\sqrt{\\frac{1}{173} + \\frac{1}{261}}} = -1.35\n\\] This results in a \\(p\\)-value of 0.18 on \\(df =592\\). This \\(p\\)-value is larger than \\(\\alpha^\\star = 0.0167\\), so there is not evidence of a difference in the means of genotypes CC and CT.\nGenotype CC versus Genotype TT: \\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{48.89 - 58.08}{33.02 \\sqrt{\\frac{1}{173} + \\frac{1}{161}}} = -2.54.\n\\]\nThis results in a \\(p\\)-value of 0.01 on \\(df =592\\). This \\(p\\)-value is smaller than \\(\\alpha^\\star = 0.0167\\), so there is evidence of a difference in the means of genotypes CC and TT.\nGenotype CT versus Genotype TT:\n\\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{53.25 - 58.08}{33.02 \\sqrt{\\frac{1}{261} + \\frac{1}{161}}} = -1.46\n\\]\nThis results in a \\(p\\)-value of 0.14 on \\(df =592\\). This \\(p\\)-value is larger than \\(\\alpha^\\star = 0.0167\\), so there is not evidence of a difference in the means of genotypes CT and TT.\nIn R, these can be calculated using the pairwise.t.test() function as follows:\n\npairwise.t.test(famuss$ndrm.ch, famuss$actn3.r577x,\n    p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  famuss$ndrm.ch and famuss$actn3.r577x \n\n   CC    CT   \nCT 0.537 -    \nTT 0.034 0.433\n\nP value adjustment method: bonferroni \n\n\nAs you can guess from the command and its output:\n\nThere is more than one way of adjusting p-values! There is some debate about the best way, but the important thing is to recognize when they’re necessary and use them!\nThis test is using a pooled standard deviation. Thast is, it calculates the sd as if all of the data have the same variance.\n\nFor the t-test, it’s easy enough to use different variances for each group. For multiple comparisons, using the pooled variance makes a little bit more sense because it will tend to be larger, and therefore we’re adding extra variance and not being overconfident in our results.\n\n\nFrom these results, we can see that CC and TT are significantly different from each other. CC and CT are not, and neither are CT and TT.4"
  },
  {
    "objectID": "L23-ANOVA-oi.html#summary",
    "href": "L23-ANOVA-oi.html#summary",
    "title": "20  Comparing means with ANOVA",
    "section": "20.3 Summary",
    "text": "20.3 Summary\nIn summary, the mean percent strength change in the non-dominant arm for genotype CT individuals is not statistically distinguishable from those of genotype CC and TT individuals. However, there is evidence that mean percent strength change in the non-dominant arm differs between individuals of genotype CC and TT are different."
  },
  {
    "objectID": "L23-ANOVA-oi.html#crowdsourced-questions",
    "href": "L23-ANOVA-oi.html#crowdsourced-questions",
    "title": "20  Comparing means with ANOVA",
    "section": "20.4 Crowdsourced Questions",
    "text": "20.4 Crowdsourced Questions\nThe following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).\n\nWhy is ANOVA preferred over multiple t-tests when comparing the means of three or more groups?\n\nBecause it can analyze more than two groups at once with a single test, reducing the risk of Type I errors.\nBecause it provides a direct comparison of variances, which t-tests cannot do.\nBecause ANOVA requires less data to be effective.\nBecause it is less computationally intensive than performing multiple t-tests.\n\n\n\n\nSolution\n\nANOVA is preferred because it allows for the analysis of more than two groups at once with a single test, which reduces the risk of Type I errors (falsely rejecting the null hypothesis) associated with conducting multiple t-tests."
  },
  {
    "objectID": "L23-ANOVA-oi.html#footnotes",
    "href": "L23-ANOVA-oi.html#footnotes",
    "title": "20  Comparing means with ANOVA",
    "section": "",
    "text": "This is 1 minus the probability of no significant results in three tests. The probability of a result that is not significant is \\(1-0.05\\), so we do this three times and then do 1 minus the result. This is similar to the probability of not getting a 6 in 3 dice rolls.↩︎\nThe previous lesson has some code to help you interpret Q-Q plots.↩︎\nIn a more advanced course, it can be shown that the ANOVA procedure still holds with deviations from normality when sample sizes are moderately large. Additionally, a more advanced course would discuss appropriate transformations to induce normality.↩︎\nNote that this is not transitive! CC and CT are not, CT and TT are not, but CC and TT are different!↩︎"
  },
  {
    "objectID": "L24-Review.html#the-broad-topics",
    "href": "L24-Review.html#the-broad-topics",
    "title": "21  Post-Midterm Review",
    "section": "21.1 The Broad Topics",
    "text": "21.1 The Broad Topics\nThe following topics have applied to almost everything since the midterm.\n\nStandard Errors\n\nSince our sample was random, we could have gotten different data. With different data, we’d get a different mean and sd.\nThe standard error of the mean represents uncertainty around the mean. With more data, we are more certain about the value.\n\nEffects of outliers diminish, and the overall variation “averages out”.\nThe same thing happens to the sd, but it’s not a normal distribution.\n\nBe ready to explain how standard errors relate to various concepts.\n\nFor example, could you explain why test statistics and p-values have a standard error (even though you don’t know the formula for it)?\n\n\n\n\nAssumption Checking\n\nAll of the methods we use require assumptions about the population.\nThere’s always some sort of independence assumption, which can be satisfied by having a good sampling strategy.\nThere’s usually either an assumption of normality, or some conditions for which the normal approximation applies.\n\nFor means, we need a big enough sample for the Central Limit Theorem to apply (\\(n &gt; 30\\) or \\(n&gt;60\\) or something like that). This applies to all of the groups for t-tests and ANOVA.\nFor proportions, we need some form of \\(np&gt;10\\) and \\(n(1-p)&gt;10\\) so that the normal approximation applies. Be careful whether you need the observed proportion, hypothesized proportion, or pooled proportion!\nFor regression, we assume that the residuals are normal, and we can assess this with the QQ Plot.\n\nThere are other assumptions that may be method-specific.\n\nFor example, linear regression requires that the plot looks linear!\n\nOn the exam, there will be questions that ask you to interpret the results of a study. You must check the assumptions before making a conclusion, regardless of whether you’re explicitly told to!\n\nIn a job, your boss might just ask for the results, but they’ll expect you to make sure the results are valid before sharing them!\n\n\n\n\nHypothesis Testing\n\nNull and Alternative hypotheses: Convert the word problem into math.\n\nNull: Nothing is going on. Very often, this means \\(\\mu = 0\\) or \\(p = 0.5\\) or \\(\\mu_1 - \\mu_0 = 0\\). However, we could also test things like \\(p_1 - p_2 = 0.1\\), i.e., whether \\(p_1\\) is 10 percentage points higher than \\(p_2\\).\nAlternative: generally involves a \\(\\ne\\), \\(&gt;\\), or \\(&lt;\\). The wording of the statement will indicate whether it’s a two sided (\\(\\ne\\)), right-tailed (\\(&gt;\\)), or left-tailed (\\(&lt;\\)) p-value.\n\nSet the significance level.\n\nThis determines how “strong” the evidence must be in order for us to reject the null. \\(\\alpha = 0.1\\) means we’ll accept weak evidence, whereas \\(\\alpha = 0.001\\) means we need to be very sure of our results before the null is rejected.\n\nTest statistic: measures the distance between our observed data and the hypothesized value, relative to the standard error.\n\nRelies on the value from the null hypothesis, as well as the sample size!\n\np-value: ASSUMING THE NULL HYPOTHESIS IS TRUE, the p-value measures the probability of getting data at least as extreme as the data we got.\n\n“At least as extreme” = this far away from the null values or further. The direction is determined by the alternative hypothesis.\nA small p-value is strong evidence. If \\(p &lt; \\alpha\\), we reject the null and claim that our result is “statistically significant”.\nA small p-value does not mean that there’s a large difference in the observed and hypothesized! It’s a large difference relative to the standard error, which decreases with better study designs and larger sample sizes. For example, a 0.01% increase in cancer risk is not something that a person really needs to worry about, but a large enough sample might find a statistically significant result!\n\n\n\n\nConfidence Intervals\n\nIf we were to repeat the exact same study many, many times with new samples each time, \\((1-\\alpha)\\)% of the intervals we create will contain the true population parameter.\nGenerally, a CI has the form “point estimate \\(\\pm\\) critical value * standard error”\n\nPoint estimate: the mean, for example.\nCritical value: the value from the relevant distribution that makes it a \\((1-\\alpha)\\)% interval. Essentially, this controls the width of the interval so that the interval actually does contain the true parameter as often as we claim that it does.\nStandard error: see above.\n\nA 95% CI is essentially the middle 95% of the sampling distribution, if it were centered on the observed sample mean.\n\nIn other words, we use the standard error to determine an interval that we hope covers the middle 95% of all possible mean values.\n\nFor a 95% CI, we want the middle 95%. This means we want 2.5% on either side, which is why we see \\(\\alpha/2\\) a lot when dealing with confidence intervals.\n\n\n\nConclusion in the context of the problem\nThe beauty of stats is that it’s rigorous math with applications to the real world - always remember that the data come from somewhere and the results might be meaningful to real people!\n\n\nType 1 and 2 Error\n\nType 1 Error: rejecting a null when it’s true.\n\nWe reject if \\(p &lt; \\alpha\\) because our p-value is “too unlikely” under the null. However, unlikely things still happen!\nIf \\(\\alpha=0.05\\), then we reject anything with a p-value less than 5%. However, things with a p-value of 5% still happen 5% of the time.\nP(Type 1 Error) = \\(\\alpha\\).\n\nIn other words, we control P(Type 1 Error) when we choose a significance level.\n\n\nType 2 Error: Fail to reject the null when it’s false\n\nEven when the null is actually false, we may not have strong enough evidence to reject it.\n\nBetter evidence comes from either a better study design or a larger sample size.\n\nPower: 1 - P(Type 2 Error), which is generally difficult to calculate.\n\nIt is not \\(1-\\alpha\\).\n\nA study with low power might have p-values larger than \\(\\alpha\\), but the authors would not be able to say whether the null is false.\n\n\n\n\nMultiple Comparisons Problem\n\nSuppose the null is true. We still have a 5% chance of rejecting a true null, and therefore a 95% chance of correctly not rejecting it.\nIf we have two nulls, both of which are true, then the probability that we correctly don’t reject either is 0.95*0.95 = 0.9025. This means we have a 9.75% chance of rejecting at least one of them, even though they’re both true.\nThe multiple comparisons problem states that, when we check a lot of p-values, the Type 1 Error increases."
  },
  {
    "objectID": "L24-Review.html#means",
    "href": "L24-Review.html#means",
    "title": "21  Post-Midterm Review",
    "section": "21.2 Means",
    "text": "21.2 Means\n\nOne-sample t-tests and CIs for a Mean\n\nAssumptions: The population is normal, or that the sample is large enough for the CLT to apply. Independence among observations.\nNull Hypothesis: \\(H_0:\\mu = \\mu_0\\), where \\(\\mu_0\\) is the hypothesized value given in the question.\nTest Statistic: \\(t_{obs} = \\dfrac{\\bar x - \\mu_0}{s/\\sqrt{n}}\\)\np-value:\n\npt(\\(t_{obs}\\)) for left-tailed (\\(&lt;\\))\n1 - pt(\\(t_{obs}\\)) for right-tailed (\\(&gt;\\))\n2*(1 - pt(|\\(t_{obs}\\)|)) for two-tailed (\\(\\ne\\))\n\nCI: \\(\\bar x \\pm t_{n-1}^*\\dfrac{s}{\\sqrt{n}}\\)\n\nFor a \\((1-\\alpha)\\)% interval, \\(t^*_{n-1}\\) comes from qt(alpha/2)\n\nE.g., for a 95% CI, \\(\\alpha = 0.05\\) and we would use qt(0.025).\nThis could also be written as qt((1 - 0.95)/2).\n\n\n\nExample: The EPA claims that the average fuel mileage of cars is 19 mpg. Does the mtcars data set support this claim?\nA small difference from 19 would not be important to us, so we’re only going to reject this claim if there is strong evidence. For this reason, we’ll use a significance level of 0.01.\n\n\nThe plot below does not quite look normal, but for this size sample it is potentially normal “enough”.\n\n\n\n\n\n\nThe question asks whether our data support the claim, but does not ask if the true mpg is larger than or less than. We use a two-sided hypothesis test.\n\\[\nH_0: \\mu = 19\\text{ vs. }H_A:\\mu \\ne 19\n\\]\nWe’ll let R do the calculations for us.\n\n\n\nt.test(mtcars$mpg, mu = 19, \n    alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 1.0237, df = 31, p-value = 0.3139\nalternative hypothesis: true mean is not equal to 19\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n\nOur p-value is 0.31, which is much larger than our significance level of 0.05. We conclude that we cannot reject the EPA’s claim.1\n\n\nSecond Example\n\nThe ggplot2 package also loads in a data set called mpg, do these data agree with our previous conclusion?\n\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\n\nt.test(mpg$overall, mu = 19)\n\n\n    One Sample t-test\n\ndata:  mpg$overall\nt = 2.528, df = 233, p-value = 0.01213\nalternative hypothesis: true mean is not equal to 19\n95 percent confidence interval:\n 19.18104 20.45998\nsample estimates:\nmean of x \n 19.82051 \n\n\nThe p-value for this test is 0.012, which is above our significance level of 0.01. However, it’s only slightly above!\nIf these are two well-collected samples from the population, then there’s a \\(1 - (1 - 0.01)^2 = 0.0199\\approx 2\\%\\) chance of at least one of them being significant by chance.\nAs mentioned in class, the mtcars data set is from 1973. This data set is from 1999-2008, so they’re not exactly separate samples from the same population!\n\n\n\nMatched Pairs t-test for a Mean Difference\nA matched pairs study design is one where each subject is matched with another, only one of which gets the treatment and the other gets the control. See the oitment example from the class.\nA matched pairs test is actually just a one-sample t-test for the differences. In other words, we treat each difference as if it’s the value we’re calculating. The one-sample t-test assumptions and conclusions apply.\n\n\nTwo-Sample t-tests\n\nAssumptions: Both populations are normal, or both the samples are large enough for the CLT to apply. Independence among observations within and between groups.2\nNull Hypothesis: Generally, we have the null hypothesis \\(H_0:\\mu_{diff} = 0\\), where \\(\\mu_{diff}\\) is the difference in the means (either \\(\\mu-1 - \\mu_2\\) or \\(\\mu_2 - \\mu_1\\)3).\nTest Statistic: \\(t_{obs} = \\dfrac{\\bar x_1 - \\bar x_2}{SE}\\)\n\n\\(SE\\) comes from the square root of the sum of their variances.\n\n\\(SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{2_s^2}{n_2}}\\)\n\n\np-value: Same as 1-sample, where we use a \\(t\\)-distirbution.\nCI: \\(\\bar x1 - \\bar x_2 \\pm t_{n-1}^*SE\\)\n\nExample: In the 1-sample procedure, I included a second example using a different sample of cars. The first example used a dataset called mtcars, which measured a sample of cars from 1973-1974, while the second used a data set called mpg which measures a sample of cars from 1999-2008. In this example, let’s test whether the fuel efficiency of cars has improved (i.e., the mpg has gone up).4\nIn symbols, our hypotheses are \\(H_0:\\mu_{mtcars} - \\mu_{mpg} = 0\\) versus \\(H_A: \\mu_{diff} &lt; 0\\).5\n\n\n\n\n\n\n\nFrom these two plots, it looks like there is a possible slight difference.\n\n\nWe are assuming that both data sets are based on good samples, although this may not actually be appropriate and we should acknowledge this in our conclusions.\nIt is clear that there is not dependence between these two data sets - they were collected completely separately!\n\n\n\n\nt.test(mtcars$mpg, mpg$overall, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  mtcars$mpg and mpg$overall\nt = 0.24252, df = 36.979, p-value = 0.5951\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 2.149167\nsample estimates:\nmean of x mean of y \n 20.09062  19.82051 \n\n\nIt looks like there is not a significant difference.\nI’m always a little bit skeptical when something is doing calculations for me, so I just want to double check whether R is doing mtcars minus mpg or the other way around.\n\nmean(mtcars$mpg); mean(mpg$overall)\n\n[1] 20.09062\n\n\n[1] 19.82051\n\n\nSo it looks like R is labelling “x” as mtcars and “y” as mpg, and doing mtcars minus mpg. This means that we’re correct in using “alernative = \"less\".\nNow that we’ve double checked that the calculations were correct, we can make our conclusion. It does not appear that the two data sets have significantly different fuel efficiencies. However, this conclusion is highly sensitive to whether the data sets have “good” sampling strategies.\nThe mtcars data were sample by Motor Trend Magazine based on what they thought their audience would like, whereas the mpg data were taken from the Environmental Protection Agency without any preference. It is very important to note that neither data set represents the average mpg of cars on the road. For instance, the mpg data contain one row for the manual Chevrolet Corvette and one row for the automatic Corvette, as well as one for the Honda Civic (automatic and manual). This is not representative of cars in general, as well as not representative of the actual cars on the road.\n\n\nANOVA for Multiple Means\n\nAssumptions: The population is normal, or that the sample is large enough for the CLT to apply. Independence among observations. Independence between groups. Groups have the same variance.\nNull: All means are equal, i.e. \\(\\mu_1 = \\mu_2 = ... = \\mu_k = 0\\).\n\nThis is false if any or all of the means differ; ANOVA does not tell us which mean is significantly different from the others.\n\nTest Statistic: \\(MSG/MSE\\), which is interpreted as the variance of the group means (considering their sample sizes) divided by the variance in the data if we were to ignore the groups.\np-value: Always right-tailed, since we’re only testing whether the variance of group means is too large relative to the data varaince. The p-value is calculated from an F distribution.\nConclusions and further steps: If we reject the null and conclude that at least one group is statistically significant, we can then do a post-hoc analysis to determine which mean(s) is(are) statistically significantly different from the others.\n\nWe use special techniques to control the Type 1 error!\n\n\nExample: The EPA’s claim of 19 mpg as the average does not take into account the number of cylinders. When asked, the spokesperson said that the average mpg is the same regardless of the number of cylinders that a car has.\nLet’s test this claim! Again, we want strong evidence before we reject this claim, so we’ll set the significance level to 0.01.\n\n\nFrom the plot below, it looks like there will almost certainly be a difference!\n\n\n\n\n\n\nThe table shows that one of the assumptions of ANOVA is not satisfied (guess which before looking at the footnote6), so we should be careful when interpreting the results.\n\n\n\n\n\ncyl\nmean\nsd\nsize\n\n\n\n\n4\n26.66\n4.51\n11\n\n\n6\n19.74\n1.45\n7\n\n\n8\n15.10\n2.56\n14\n\n\n\n\n\n\n\n\nsummary(aov(mpg ~ factor(cyl), data = mtcars))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals   29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "L24-Review.html#regression",
    "href": "L24-Review.html#regression",
    "title": "21  Post-Midterm Review",
    "section": "21.3 Regression",
    "text": "21.3 Regression\n\nAssumptions: Independence, no patterns in the residual plots\n\nResiduals vs. Fitted should be a straight line with no patterns\nQQ plot should look like a line\nScale-location should be a straight line\nNothing outise the dotted lines in the Leverage plot.\n\nHypothesis: The slope is 0 (which implies that the correlation is 0)\nTest Statistic: Like a test for a one-sample t-test, but with a more complicated standard error.\n\nR will give the standard error.\nDegrees of freedom is \\(n-2\\).\n\np-value: Generally two sided, but not always\n\nWe may be asking about a positive association, e.g. \\(H_A:\\beta &gt; 0\\).\n\nConfidence Intervals: See one-sample t-test.\n\nExample: Is there a linear relationship between the fuel efficiency of the car and the car’s weight? Test at the 5% level.\nTo test the assumptions, we have to actually fit the model first! This is a good test of self control - the p-values are just sitting there waiting to be interpreted, but we’ve got a lot of work before we should even look at them!\n\nmtcars_lm &lt;- lm(mpg ~ wt, data = mtcars)\n\npar(mfrow = c(2, 2)) # sets up plotting region for 4 plots\nplot(mtcars_lm)\n\n\n\n\n\nResiduals vs. Fitted: There’s a slight “U”-shaped pattern. This is not good!\nNormal Q-Q: Doesn’t look too bad.\nScale-location: A bit of a pattern, but overall not too bad.\nResiduals vs. Leverage: One point outside the “0.5” dotted line, which should be investigated but isn’t too bad.\n\nSo the assumptions are not really satisfied, mainly because of the “U”-shaped pattern in the first plot.\nWe’ll look at the output just for practice, but the actual pattern might not be linear.\n\nsummary(mtcars_lm)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nAccording to the output, the slope is significant. There appears to be a significant correlation between mpg and weight of the car, but the actual pattern might not be linear."
  },
  {
    "objectID": "L24-Review.html#proportions",
    "href": "L24-Review.html#proportions",
    "title": "21  Post-Midterm Review",
    "section": "21.4 Proportions",
    "text": "21.4 Proportions\n\nOne-Sample Test for Proportions\n\nAssumptions: \\(np &gt; 10\\) and \\(n(1 - p)&gt;10\\) so that the normal approximation applies. Indpedence/good sampling.\n\nFor hypothesis tests we use \\(p_0\\) (the hypothesized proportion), for confidence intervals we use \\(\\hat p\\) (the estimated proportion).\n\nHypotheses: \\(p = p_0\\).\nTest Statistic: We assume normality, and under the null we have a standard error of \\(\\sqrt{p_0(1 - p_0)/n}\\). The test statistic is \\(z = (\\hat p - p_0)/SE\\)\np-value: Since we’re not estimating the standard deviation, we don’t get a \\(t\\) distribution. The p-value comes from the normal distribution.\nConfidence Intervals: For a CI, we don’t have a hypothesis and so we use \\(\\hat p\\) in the standard error.\n\n\\(100(1-\\alpha)\\%CI: \\hat p \\pm z^*\\sqrt{\\hat p(1 - \\hat p)/n}\\)\n\n\nExample: It is suggested that, if male penguins are more likely to be the hunters, then there should be more females than males (the hunters will get hunted by orcas). Assuming that the Palmer Penguins data are a random sample, do we have evidence at the 5% level that males are the primary hunters?\n\n\nWe are told to assume it’s a random sample, and we can see from the plots that \\(n\\) is large enough for \\(np_0&gt;10\\) and \\(n(1-p_0)&gt;10\\).\nOur hypothesis is that the proportion of males is less than 0.5: \\[\nH_0: p = 0.5\\text{ vs. }p &lt; 0.05\n\\]\n\n\n\n\n\n\n\n\nWe can look at the data as follows:\n\ntable(penguins$sex == \"male\")\n\n\nFALSE  TRUE \n  165   168 \n\nprop.test(table(penguins$sex == \"male\"), p = 0.5, alternative = \"less\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(penguins$sex == \"male\"), null probability 0.5\nX-squared = 0.012012, df = 1, p-value = 0.4564\nalternative hypothesis: true p is less than 0.5\n95 percent confidence interval:\n 0.0000000 0.5419071\nsample estimates:\n        p \n0.4954955 \n\n\nSince the p-value is 0.4564, we do not reject the null hypothesis. There is no evidence to suggest that there are fewer males than females, which means that males do not appear to be predated at a higher rate.\n\n\nTwo-Sample Test for Proportions\n\nAssumptions: \\(n_1\\hat p &gt; 10\\) and \\(n_1(1 - \\hat p)&gt;10\\), similar \\(n_2\\), so that the normal approximation applies. Indpedence/good sampling.\n\nFor hypothesis tests, \\(\\hat p\\) is the pooled proportion. For confidence intervals, we require that \\(n_i\\hat p_i&gt;10\\) and \\(n_i(1-\\hat p_i)&gt;10\\) for \\(i=1\\) and \\(i=2\\).\n\nHypotheses: Generally, we’re testing \\(p_1 = p_2\\), which amounts to testing \\(p_{diff} = 0\\), where \\(p_{diff} = p_2 - p_1\\) of \\(p_1 - p_2\\).\n\nIt is also possible to test, e.g., whether \\(p_2\\) is 10 percentages higher than \\(p_1\\), which would be \\(H_0:p_{diff} = 0.1\\).\n\nTest Statistic: Since this is based on a normal approximation, this works almost exactly the same as the two-sample t-test approach.\n\n\\(z = \\frac{\\hat p_{diff} - p_{diff}}{SE(\\hat p_{diff})}\\).\n\np-value: See two-sample t-tests.\nCI: See two-sample t-tests.\n\nExample: In the penguins data, penguins are sampled from three different islands: Biscoe, Dream, and Torgersen. For this example, suppose a researcher is interested in whether the islands Biscoe and Dream have the same proportion of males and females. We’ll test this at the 10% level.\nWe can treat the penguins from Biscoe as one sample, find the proportion of males, and do the same for Dream.\n\n# Code to specify the islands\ndb &lt;- penguins[penguins$island %in% c(\"Dream\", \"Biscoe\"),]\n# Ensure that R ignores the third island)\ndb$island &lt;- factor(db$island)\n# Create a two-way table\ntable(db$island, db$sex)\n\n        \n         female male\n  Biscoe     80   83\n  Dream      61   62\n\n\nFrom the two-way table, I’m guessing that we won’t reject the null! Those proportions look pretty close, and the sample size is pretty small.\nWe can use prop.test() to do the calculations for us. If we don’t specify the alternative hypothesis, it will assume two sided. This is appropriate, since we’re just looking for a difference in proportions.\n\nprop.test(table(db$island, db$sex))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(db$island, db$sex)\nX-squared = 1.7776e-30, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.1273096  0.1170348\nsample estimates:\n   prop 1    prop 2 \n0.4907975 0.4959350 \n\n\nThe p-value for the two-sided alternated hypothesis is 1! This is definitely larger than our significance level, so we absolutely cannot reject the null. We conclude that there is no evidence of a difference in the proportion of males to females.\nNotice that the proportion estimates are 80/(80 + 83) and 61/(61 + 63), i.e. the proportion of female penguuins on Briscoe Island and the proportion of female pengiuns on Dream Island (note that these are conditional probabilities). If we had done it the other way:\n\nprop.test(table(db$sex, db$island))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(db$sex, db$island)\nX-squared = 2.8876e-30, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.1248439  0.1147680\nsample estimates:\n   prop 1    prop 2 \n0.5673759 0.5724138 \n\n\nwe get the proportion of Briscoe among females and the proportion of Dream among females, but the exact same p-value!\n\n\nChi-Square Test for Two-Way Tables\n\nAssumptions: Independence among individuals, large enough samples for the normal approximation to apply.\nHypotheses: The Column and the Row variable are independent.\n\nIn other words, P(A | X) = P(A) for all rows A and columns X, and vice versa.\n\nTest Statistic: The squared difference between the observed count and the expected count.\n\nThe expected count is what we would get if the rows and columns were independent.\n\np-value: Always right-tailed, since the test stat is a squared difference.\n\nExample: Instead of just looking at Dream and Biscoe, let’s look at all three islands.\n\ntable(penguins$island, penguins$sex)\n\n           \n            female male\n  Biscoe        80   83\n  Dream         61   62\n  Torgersen     24   23\n\n\n\nchisq.test(table(penguins$island, penguins$sex))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$island, penguins$sex)\nX-squared = 0.057599, df = 2, p-value = 0.9716\n\n\nAs we might have expected, there’s no evidence that there’s a difference in the “sex” variable in different islands.\nOne way to interpret this result is that P(Male | Briscoe) = P(Male), and similarly for both biological sexes and for all three islands. That is, knowing which island the penguin was sampled from does not give us more information about the penguin’s biological sex. Conversely, if I told you that it was a Male penguin then you would have no more information about which island it was sampled from.\n\n\nSecond Example\n\nWhat about a difference in the “island” variable for different sexes?\n\nchisq.test(table(penguins$sex, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$sex, penguins$island)\nX-squared = 0.057599, df = 2, p-value = 0.9716\n\n\nThe p-value is identical! If one is independent of the other, then the other is independent of the one!\n\n\n\nChi-Square Test for Goodness of Fit\n\nAssumptions: Independence between individuals.\nHypothesis: The distribution matches the hypothesized distribution.\nTest Statistic: The squared difference between observed and expected, where expected is defined in the hypothesis.\np-value: Always right-tailed.\n\nExample: A researcher claimed that they saw 50% of the penguins on the Briscoe island, 30% on Dream, and 20% on Torgersen. Is this hypothesis compatible with the data?\nThe penguins data looks like this:\n\ntable(penguins$island)\n\n\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n\n\nchisq.test(table(penguins$island), p = c(0.5, 0.3, 0.2))\n\n\n    Chi-squared test for given probabilities\n\ndata:  table(penguins$island)\nX-squared = 8.3876, df = 2, p-value = 0.01509\n\n\nIt looks like the answer is no! The p-value is less than 0.05, so we conclude that the distribution of penguins across the islands is different from the claimed 0.5, 0.3, and 0.2. Let’s see the actual proportions to see why:\n\ntable(penguins$island) / nrow(penguins)\n\n\n   Biscoe     Dream Torgersen \n0.4883721 0.3604651 0.1511628 \n\n\nFrom the data, it looks like there were more on Dream than the researcher suggested, and fewer on Torgersen.\nNotice how the goodness-of-fit test works like a one-sample Chi-Square test. That’s neat!"
  },
  {
    "objectID": "L24-Review.html#practice-problems",
    "href": "L24-Review.html#practice-problems",
    "title": "21  Post-Midterm Review",
    "section": "21.5 Practice Problems",
    "text": "21.5 Practice Problems\n\nfamuss Study\nIn the famuss study, we focused on the ndrm.ch(non-dominant arm percent change after 12 weeks of strength training) and the actn3.r577x (genotype at a particular position on the genome). We used this for the lecture on Chi-Square test for proportions.\nHere are the other variables available in the data set:\n\n# Only need to run these lines of code *once*!\ninstall.packages(\"devtools\")\ndevtools::install_github(\"OI-Biostat/oi_biostat_data\")\n\n\nlibrary(oibiostat)\n\ndata(famuss)\nhead(famuss)\n\n  ndrm.ch drm.ch    sex age      race height weight actn3.r577x    bmi\n1      40     40 Female  27 Caucasian   65.0    199          CC 33.112\n2      25      0   Male  36 Caucasian   71.7    189          CT 25.845\n3      40      0 Female  24 Caucasian   65.0    134          CT 22.296\n4     125      0 Female  40 Caucasian   68.0    171          CT 25.998\n5      40     20 Female  32 Caucasian   61.0    118          CC 22.293\n6      75      0 Female  24  Hispanic   62.2    120          CT 21.805\n\n\nThese data are not necessarily a random sample of the population, so questions like “does weight change with age?” won’t result in conclusions that apply to the general population. However, you can do a couple of tests just to practice your skills.\nFor each example below, answer all of these questions:\n\nTest yourself on the assumptions we’re making.\n\nAre they satisfied in the example?\n\nWrite the hypotheses in the appropriate symbols.\n\nDo the statements in the examples make sense?\n\nExplain why the test is appropriate for the data.\nInterpret the conclusions in the context of the problem.\n\nI’ve provided the research questions and the code to calculate the p-value, as well as some plots to help you address the assumptions. Try to run the code yourself so that you can play around with things (try different hypotheses, verify that values in a confidence interval will be rejected, etc.), but I’ve also included the output below.\n\nAre 50% of the people in this study female?\n\nsum(famuss$sex == \"Female\")\nnrow(famuss)\nprop.test(sum(famuss$sex == \"Female\"), nrow(famuss), p = 0.5)\n\nIs the change in dominant arm strength larger than 0?\n\nmean(famuss$drm.ch)\nt.test(famuss$drm.ch, mu = 0, alternative = \"greater\")\n\nThe data contain a column for non-dominant and dominant arm strength. Are the means of these columns different?7\n\nt.test(famuss$drm.ch - famuss$drm.ch, mu = 0, alternative = \"two.sided\")\nAlternative question: is the mean of ndrm.ch larger than ndrm.ch by more 50? This is the same as asking if the dominant arm strength is 50 percentage points higher than the non-dominant by over 50% percentages.8 This is a small change to the code, but it adds a lot to the interpretation!\nInterpret the confidence interval.\n\nIs there a difference between change in non-dominant arm strength for men and women in this study?\n\nt.test(ndrm.ch ~ sex, data = famuss, alternative = \"two.sided\")\n\nIs there an association between race and genotype?\n\ntable(famuss$race, famuss$actn3.r577x)\nchisq.test(table(famuss$race, famuss$actn3.r577x))\n\nInterpreting the results here must be done carefully, but is important!\n\nIf the results are significant, how might you figure out which races are different?\n\nIs the mean change in non-dominant arm strength the same across races?\n\nanova(aov(ndrm.ch ~ race, data = famuss))\n\nIs there a correlation between dominant and non-dominant arm strength?9\n\nsummary(lm(drm.ch ~ ndrm.ch, data = famuss))\nInterpret the slope in the context of the problem, being careful to refer to any shortcomings of the problem.\n\nAlso, did it matter which I used as the \\(y\\) variable?\n\nIdentify any potentially influential outliers.\n\n\n\n\nRelevant Plots\n\n\n\n\n\n\n\n\n\n\nOutput of Code\nTry these youself, changing the alternative hypothesis and hypothesized values!\n\n# 1. Are 50% of the people in this study female?\nsum(famuss$sex == \"Female\")\n\n[1] 353\n\nnrow(famuss)\n\n[1] 595\n\nprop.test(sum(famuss$sex == \"Female\"), nrow(famuss), p = 0.5)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  sum(famuss$sex == \"Female\") out of nrow(famuss), null probability 0.5\nX-squared = 20.336, df = 1, p-value = 6.496e-06\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.5524836 0.6328489\nsample estimates:\n        p \n0.5932773 \n\n# 2. Is the change in dominant arm strength larger than 0?\nmean(famuss$drm.ch)\n\n[1] 10.35025\n\nt.test(famuss$drm.ch, mu = 0, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  famuss$drm.ch\nt = 13.995, df = 594, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is greater than 0\n95 percent confidence interval:\n 9.131896      Inf\nsample estimates:\nmean of x \n 10.35025 \n\n# 3. Are the means of the non-dominant and dominant columns different?7\nt.test(famuss$drm.ch - famuss$drm.ch, mu = 0, alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  famuss$drm.ch - famuss$drm.ch\nt = NaN, df = 594, p-value = NA\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n NaN NaN\nsample estimates:\nmean of x \n        0 \n\n# 4. Is there a difference between change in non-dominant arm strength for men and women in this study?\nt.test(ndrm.ch ~ sex, data = famuss, alternative = \"two.sided\")\n\n\n    Welch Two Sample t-test\n\ndata:  ndrm.ch by sex\nt = 10.073, df = 574.01, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 19.07240 28.31175\nsample estimates:\nmean in group Female   mean in group Male \n            62.92720             39.23512 \n\n# 5. Is there an association between race and genotype?\ntable(famuss$race, famuss$actn3.r577x)\n\n            \n              CC  CT  TT\n  African Am  16   6   5\n  Asian       21  18  16\n  Caucasian  125 216 126\n  Hispanic     4  10   9\n  Other        7  11   5\n\nchisq.test(table(famuss$race, famuss$actn3.r577x))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(famuss$race, famuss$actn3.r577x)\nX-squared = 19.4, df = 8, p-value = 0.01286\n\n# 6. Is the mean change in non-dominant arm strength the same across races?\nanova(aov(ndrm.ch ~ race, data = famuss))\n\nAnalysis of Variance Table\n\nResponse: ndrm.ch\n           Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nrace        4  11524  2881.1  2.6526 0.03233 *\nResiduals 590 640812  1086.1                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# 7. Is there a correlation between dominant and non-dominant arm strength?\nsummary(lm(drm.ch ~ ndrm.ch, data = famuss))\n\n\nCall:\nlm(formula = drm.ch ~ ndrm.ch, data = famuss)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.004  -9.691  -1.391   7.953  86.965 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.32170    1.30415  -0.247    0.805    \nndrm.ch      0.20026    0.02079   9.634   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.79 on 593 degrees of freedom\nMultiple R-squared:  0.1353,    Adjusted R-squared:  0.1339 \nF-statistic: 92.81 on 1 and 593 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "L24-Review.html#footnotes",
    "href": "L24-Review.html#footnotes",
    "title": "21  Post-Midterm Review",
    "section": "",
    "text": "This does not mean that 19 is the true value, we just don’t have evidence against this value. We don’t confirm the null; it’s set up in a such a way that we seek evidence against it, not for it.↩︎\nThere is a version of the two-sample t-test that also assumes equality of variances in the two groups, but it’s generally best to avoid that assumption unless it’s abundantly clear that it holds.↩︎\nEither is fine, but you must be careful about using right or left-sided p-values↩︎\nThis example cannot be seen as a matched pairs procedure - there’s nothing to match!↩︎\nTake a moment to ensure that these make sense to you!↩︎\nEqual variance within groups↩︎\nJustify why this is a matched pairs test.↩︎\nTechnical note: we are not testing if it’s double, e.g. \\(\\mu_{dom} = 2*\\mu_{non}\\); we have not learned the machinery for this. in particular, there are some extra steps for the standard error.↩︎\nDefinitely check the plots for this one!!!↩︎"
  },
  {
    "objectID": "L15-CI_for_Means.html#self-study-questions",
    "href": "L15-CI_for_Means.html#self-study-questions",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.8 Self-Study Questions",
    "text": "12.8 Self-Study Questions\n\nExplain the confidence interval that we found in the basketball player example.\n\nFor feedback, try the following in ChatGPT, Google Gemini, or Bing Chat: “In a sample of basketball players, a 95%CI was calculated as (169.5, 174.9). My explanation is: !!!!!put your explanation here!!!!!. Please rate my explanation as if it’s an answer on an exam in an introductory statistics course.”\n\nExplain very clearly why we checked whether 162.3 was in our interval.\nRepeat the example of creating a CI for basketball players, except for hockey players. The code below creates the data for hockey players. If you’d prefer to use a calculator, I also provide the mean and sd (you’ll still need to calculate critical value from the \\(t\\) distribution).\n\n\npucks &lt;- stats$Height[stats$Sport == \"Hockey\"]\nmean(pucks)\n\n[1] 165.2128\n\nsd(pucks)\n\n[1] 4.55038\n\n\nA plot of the values is below to help with your interpretation:\n\n\n\n\n\n\nIn 2014, Dr. S. Hooker published a paper claiming to find a link between vaccines and autism, but only for African American boys. In the paper, Dr. Hooker tested 35 p-values and found 2 were less than the significance level of 0.05.\n\nUsing the binomial distribution, find the probability of at least 2 successes in 35 trials when the probability is 0.05.\nInterpret the probability in part (a) in terms of the vaccines study.\nThe 35 trials came from not just testing for a link, but splitting up the data by race, gender, and vaccine timing so that there were more hypothesis tests. This is known as “p-hacking”. Explain why this will often lead to statistically significant results, even when the null hypothesis is true.\n\n\n\n\nQ4 Solution\n\n\n1 - pbinom(1, size = 35, prob = 0.05)\n\n[1] 0.5279735\n\n\nThis calculation assumes that all of the null hypotheses are true (as we should do when calculating p-values). Under this assumption, there’s about a 50% chance of getting 2 or more significant results, even if there’s nothing going on. This is vastly different than the “5% risk of rejecting a true null” that we get when setting our significance level.\nUse this app to play around with p-hacking and see why it’s so dangerous!"
  }
]