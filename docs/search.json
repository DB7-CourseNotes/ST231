[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Notes for ST231",
    "section": "",
    "text": "Introduction\nThese course notes structured to coincide with Baldi and Moore’s The Basic Practice of Statistics in the Life Sciences, 4th edition. However, I am working towards making them self-contained in order to provide these notes as an Open Educational Resources. In doing so, I will also borrow from OpenIntro Statistics for the Biomedical and Life Sciences, with attribution. As such, these resources have the same license (see the end of this page), except for those elements that are still present from Baldi & Moore (mainly exercises, which are being removed as I go)."
  },
  {
    "objectID": "index.html#completion-progress",
    "href": "index.html#completion-progress",
    "title": "Course Notes for ST231",
    "section": "Completion Progress",
    "text": "Completion Progress\nThese notes are still in progress.\n\nCh01 has been edited since the version posted in class.\nCh02-Ch09 are the versions posted to MyLS for this semester.\n\nThere may be missing images due to copyright concerns.\n\nCh11 is nearly fully updated for this semester.\nCh12+ are direct copies of old notes and need editing, which may involve addition of new material.\n\nIf you read ahead, you may need to re-read later."
  },
  {
    "objectID": "index.html#helpful-information",
    "href": "index.html#helpful-information",
    "title": "Course Notes for ST231",
    "section": "Helpful Information",
    "text": "Helpful Information\n\nText: Bald & Moore: The practice of statistics in the life sciences.\n\nRecommended, not required.\nCopy available in the library. Old versions work fine.\n\nAlternate (free) text (trialing):\n\nOpenIntro Statistics for Life Sciences\n\n\n\nThe textbook for this course is recommended but not required. We will not be using questions directly from the textbook so it’s fine if you have an old version. Instead, you can just use it as a reference for extra information about any of the course topics.\nI’ve also included an alternate textbook. If you use this, please share your experience with me so that I know whether I could switch to this free textbook!."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Course Notes for ST231",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCritically appraise published articles in health sciences research.\nUse industry standard tools to apply basic statistical concepts to real-world problems.\nUnderstand the use and application of statistical techniques such as descriptive and inferential statistics."
  },
  {
    "objectID": "index.html#accessing-materials",
    "href": "index.html#accessing-materials",
    "title": "Course Notes for ST231",
    "section": "Accessing Materials",
    "text": "Accessing Materials\n\nLectures posted on MyLS\nRStudio\n\nFree, open-source interface to the R programming language.\n\nSyzygy Jupyter (JuPyteR) Notebooks\n\nFree, web-based service for WLU students\nNo need to install R on your own computer\n\n\nYou can use either RStudio or syzygy for this course, RStudio has many fantastic bells and whistles that help you produce results and reports, whereas syzygy has an online interface and makes it easy to use without installing R on your own computer.\nFor lectures, I will be using VSCode, which is the main program that I use because it works well with python and R, as well as other languages that I need. I will switch to RStudio for many demonstrations just to show you how it works because this is the program that most people who do statistics will use. I will occasionally demonstrate some concepts using Jupyter notebooks because this is another common way that people do statistics and data science. You will not be tested on the features of Rtudio, VSCode or Jupyter notebooks, but mastery of RStudio will be extremely helpful for all future data analysis tasks beyond this course.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#introduction",
    "href": "L01-Intro_PicturingGraphs.html#introduction",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nDefining “Statistics”\nMy definition: Statistics is the study of variance (or uncertainty).\n\nThe big question: is 1 statistically different from 100?\n\n1 vs. 100 apples? Yes.\n1 vs. 100 atoms in an apple? No.\n\n\n\nMany definitions of “statistics” are something along the lines of “methods for dealing with data”. This completely misses out on theoretical statistics, and makes it seem like statistics is a collection of recipes stating “if you have this data, use this method”. I think a better definition of statistics is that it’s the study of variance, whether that means studying the theoretical properties of variance or trying to explain variance in a particular data set.\nI like to ask the question: “is 1 statistically different from 100”. It may seem like they are obviously different numbers, but we can’t know that without the context. If you’re comparing numbers of apples, then yes, one apple is very different from 100 apples. However, if we’re looking at numbers of of atoms per apple, then one and 100 are both imperceptible numbers of atoms and thus we might say they’re practically the same. The difference in these two examples is the scale, and variance is a fantastic way to measure the scales of things. In my opinion, the main thing we will learn in this course is how to tell whether to numbers are different, given the scale of those two numbers. Another popular definition for statistics is “putting numbers in context”, and by “in context” they mean “relative to their variance”.\nIn my lecture notes, I use bold font for anything that you will be expected to be able to explain or define. You won’t necessarily see a full definition the first time you see a word in bold, but by the midterm/final it is something I expect you to know. A good way to study in this course is to keep a glossary of all of the words I’ve put in bold, with a definition file/note that you update as we learn more about that concept. And, of course, write a description as if you’re teaching someone else!\n\n\n\nWhy study variance?\n\nGive context to different numbers.\n\nThe size of the difference depends on the context.\n\nWe need to know how and why we were wrong.\n\nHow: What is the magnitude of the difference?\nWhy: Are we missing relationships? Bad sampling? Fundamental randomness?\n\n\nVariance is information!\n\nVariance comes from many sources. We might just be doing something wrong and missing out on important feature of our data, we might be collecting the data in a biased or incorrect way, or there might be some fundamental part of the problem that we will never be able to measure perfectly, and so the variance that we calculated may already be the smallest possible variance for this problem.\nI like to say that variance is information. Suppose we’re trying to figure out the heights of undergraduate students. We can calculate an average height, but it is entirely possible that nobody in our data set has a height that is exactly equally to the average height. We would want to quantify how much variation there is around that average height. If our sample includes a basketball team, then, this is variation due to some thing that we could have measured. In other words, the variation in our data can be explained by including another perspective. Once we have that perspective and we were included in our analysis are variance should be smaller, and therefore we have extracted more information. As we gain more information about our data, the variance in our estimate goes down - this is why I say that variance is information! Or, rather, variance is hiding information.\n\n\n\nDescriptive Versus Inferential Statistics\n\nDescriptive statistics are used to explore the data.\n\nGraphs/figures\nNumbers\n\nInferential statistics relate our data to the population.\n\nMust have a good sample first!\nOur sample has a mean. The population has a mean. How different do we expect them to be?\n\nhow different\n\n\n\n\nIn this course, we will learn about two classes of statistics. Descriptive statistics are the ones that we used to describe the sample that we obtained. This can include things like the mean/median/mode, the variance or the interquartile range, as well as bar charts, histograms box, plots, etc.\nInferential statistics are numbers that we calculate because we think have a relationship to the population. For instance, if we calculate the mean of our data, and we trust that our sample is good, then we expect this sample mean to be somewhat close to the population mean. Any time in this course I talk about the difference between two things, I will always mean “with reference to a measure of variance”. In this example, we have a sample mean as well as some measure of its variance, and this variance tells us how similar we expect the sample mean to be to the population mean. If we have a small sample variance, it means that we have a lot of information about the population mean. Variance contains information that we haven’t learned yet!1\nIn this course, we’re going to start by talking about descriptive statistics and work our way to inferential statistics."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "href": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.2 Descriptive Statistics: Plots and Graphs",
    "text": "1.2 Descriptive Statistics: Plots and Graphs\n\nThe Palmer Penguins Data\nLet me introduce you to a dataset that I’m going to be returning to throughout the semester. This is called the Palmer penguins data, end it contains information on penguins from several islands in Antarctica. In the slides, I use some fancier code to only show some of the data, but I’ll display the full data set with simpler code below:\n\nlibrary(palmerpenguins)\nhead(as.data.frame(penguins))\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\n\nSpecies can be Adelie, Gentoo, or Chinstrap\nSex can be male or female2\nBill Length is measured in millimetres\nBody Mass is measured in grams\nNot shown: Island (one of three), bill depth (mm), flipper length (mm), year.\n\nThese data are really nice for teaching statistics because we can look at what factors contribute to body mass (since it’s easier to measure the length of a beak than it is to weigh a live penguin), or we can try to determine biosex using these measurements (since penguins normally have no external genitalia, but have other morphological differences). We can also fit a model to determine the species of each penguin so that we can get a full picture of what makes each species unique! All of these are examples of statistical analyses that we will cover in this course.\n\n\nTypes of Variables\n\n\n\npenguins[, c(\"species\", \"sex\", \"body_mass_g\")] |&gt;\n    head() |&gt;\n    knitr::kable()\n\n\n\n\nspecies\nsex\nbody_mass_g\n\n\n\n\nAdelie\nmale\n3750\n\n\nAdelie\nfemale\n3800\n\n\nAdelie\nfemale\n3250\n\n\nAdelie\nNA\nNA\n\n\nAdelie\nfemale\n3450\n\n\nAdelie\nmale\n3650\n\n\n\n\n\n\n\nSpecies is categorical\n\nMutually exclusive categories.\n\nSex is binary3\n\nOne or the other\nSpecial case of categorical with 2 possibilities\n\nBody mass is quantitative\n\nIt’s a number (quantity)\n\n\n\n\n\nThe type of variable is extremely important for choosing the right summary of the data.\nCategorical variables consist of two or more mutually exclusive categories, that is, each observation has a label and nothing has more than one label. Categorical variables may be ordered (such as “low”, “medium”, and “high”) or unordered (such as names or student numbers; putting names in alphabetical order is not usually meaningful for summarising the data or doing the analysis).\nThis distinction of “ordered” versus “unordered” matters to determine what visualization or model you might want to use to compare data within categories. For instance, we might want to see the change in response to a “low” treatment compared to a “medium” or “high” treatment, where we fully expect the response to the treatment to be lower for “low” treatment and higher for “high”. In contrast, suppose we knew the patients’ occupations. We have no expectation that the treatment reponse in the “electrician” group is lower or higher than any other group, because there’s no logical ordering to occupation.4\nBinary variables are a special case of categorical variables, which only have two categories. In this case, the ordering is rarely important, and thus we don’t really make the distinction between ordered binary and unordered binary. It’s a single difference either way, so we can just look at the differences within the categories.5\nQuantitative variables are those that are measured with numbers. Unlike “low” to “medium” versus “medium” to “high”, we know how big a step it is from 0 to 1 and then 1 to 2. Unlike categorical variables, quantitative variables have a concept of “in-between”; we have nothing between “medium” and “high”, but there are infinite numbers between 1 and 2.\nThere are, however, cases where there aren’t possible measurements between 1 and 2. These are called “discrete”6 For instance, the number of children that some has is either 0, 1, 2, etc. Discrete variables are somewhere in between categorical and continuous variables (variables that can take be number). Consider the following two examples of discrete variables: the number of children that someone has, and the number of cents in their bank account. In a study, we might want to look at everyone who has no children, everyone who has 1 child, everyone who has 2, etc. However, we would not want to compare everyon who has 0 cents in their bank account, everyone who has 1 cent in their account, 2 cents, 3, cents, etc. In terms of modelling and visualization, we will sometimes treat discrete variables as categorical and sometimes as continuous.7\nThe distinction between categorical and quantitative isn’t always this obvious, but can be very helpful for choosing the right kind of plot or numerical summary.\n\n\n\nGrey Areas\n\n\n\n\n\n\nStudent Numbers\n\n\n\nA student number looks like a quantitative variable, but it’s actually just a name (category)!\n\n\n\nThere are a couple gray areas when talking about variable types. The distinction between categorical and quantitative isn’t always perfectly clear. For example, student numbers are names, but they are completely made up of numbers. However, you wouldn’t treat these numbers as if one student number comes after another in the same way that you wouldn’t put students in an order based on their names. You technically can do this if you do it alphabetically or order the student numbers in order, but this isn’t a meaningful ordering. It’s not like one student number is larger than another student number, and taking the meaning of student numbers wouldn’t make any sense.\n\n\n\nIndividuals (Subjects)\nIndividual: the unit of study.\nIn the Palmer Penguins data set, a penguin is an individual.\n\n\n\n\n\n\nCO\\(_2\\) Measurements\n\n\n\nMonthly measurements of CO\\(_2\\) - the months are the individuals?\n\n\n\n\n\n\n\n\nPaired Observations\n\n\n\n\nSpousal pairs - the pairs are the individuals?\nBefore/after (e.g., weight loss) - people are individuals.\n\n\n\n\nAll of the variables we just talked about are measured on individuals. That is to say, an individual is what you are measuring when collecting data. This can take the form of a single penguin, in which case it’s obvious that this body mass belongs to this penguin; we have measured this body mass on this.\nThere are some gray areas to this as well, though. For example, if we’re measuring carbon dioxide every month, then we’re actually measuring carbon dioxide as the variable and months as the individuals, and we can measure other things on those individuals if needed.\nWe will also encounter pared observations in this course, which are measurements on two things at once. For example, we might be looking at spousal pairs, perhaps measuring the difference in height per pair. The variable we are measuring is the difference, and so we only have one observation per individual, which means that the individuals must be the pairs of people. A slightly more obvious example is something like a weight loss study were observing a change in weight for a certain person, even though we have two observations the individual is still the person who we’re measuring.\n\n\n\nExample: What are the Individuals? What are Variables?\n\nmtcars[1:6, 1:5]\n\n                   mpg cyl disp  hp drat\nMazda RX4         21.0   6  160 110 3.90\nMazda RX4 Wag     21.0   6  160 110 3.90\nDatsun 710        22.8   4  108  93 3.85\nHornet 4 Drive    21.4   6  258 110 3.08\nHornet Sportabout 18.7   8  360 175 3.15\nValiant           18.1   6  225 105 2.76\n\n\n\n\nIndividuals: Each car (not brand)\nVariables: wt, mpg, cyl, am, etc.\n\nwt: Quantitative; the weight of the car.\nam: Binary (categorical); whether the car is automatic or manual.\ncyl: It’s the number of cylinders, so it can be considered a number. However, there are only three possible values: 4, 6, or 8. We could consider these numbers, but it is likely more useful to think of 4 cylinder cars in one category, 6 cylinder in another category, and 8 cylinder in the last category. This way, when we do analysis, we are just comparing categories rather than see what happens when we add cylinders (“adding cylinders” makes it sound like we might add 1 cylinder to a 4 cylinder car, or like we might make predictions about what would happen with a 2 cylinder car)."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "href": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.3 Graphs/Charts/Plots",
    "text": "1.3 Graphs/Charts/Plots\n\nPie Charts\n\nThe wedges must sum to 1.\n\nIf “Adelie” makes up 44% of the data, it should be 44% of the pie chart.\nAll penguins are either Adelie, Gentoo, or Chinstrap; no penguins have more than one species.\n\nIn contrast,\n\n\nMainly good for emphasizing one wedge\n\nEmphasizing can easily mean misrepresenting, whether accidentally or on purpose!\n\n\nhttps://www.darkhorseanalytics.com/blog/salvaging-the-pie\n\nIn this course, I will not be providing you the code required to make a pie chart. However, you should understand what a pie chart is, what data it works for (categories), and how they relate to bar charts.\n\n\n\nBar Charts: Categories\nBar charts are similar to pie charts, but better in practically every way.\nEach bar represents a category, and the height represents the number of observations in that category.\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(mtcars) +\n    aes(x = factor(am)) +\n    geom_bar() +\n    labs(x = \"Transmission Type\",\n        y = \"Count\")\n\n\n\n\n\nCompare this to the output of table(mtcars$am):\n\ntable(mtcars$am)\n\n\n 0  1 \n19 13 \n\n\n\nBar charts are primarily used to compare categories. The most common use of a bar chart is to count the number of observations in each category, and create a bar with a corresponding height. In this example, we see, automatic and manual transmissions, with automatic labelled as zero and manual labelled as one. We can see that approximately 22 cars are automatic and 13 cars are manual. Unlike a pie chart, we can read these numbers off of the plot and it’s easy to compare these two categories.\nThe code required to create the bar chart can be shown by clicking the “&gt;Code” icon. The data set is built into our so we don’t need to load anything to put this data. We do need to load in the ggplot library, though. In my lecture notes, you’ll see lots of code that looks like this, but you will not be tested on your ability to re-create this code. For those interested, here’s a quick breakdown of the functions I used:\n\nThe ggplot function tells R what data we will be using.\nThe aes() function sets up the plot “aesthetics”, such as what variable goes on the x-axis, what variable goes on the y-axis, what variable is assigned to a colour, what variable determines the shapes of points, etc.\ngeom_bar() actually draws the bar plot using the data set that ggplot() set up and the aesthetics, that aes() set up.\n\nTry running the code without this line and see what happens!8\n\nThe labs() function simply adds labels to the plot to make it look nicer.\n\nExercises\nThe following exercises are based on the exercises in Chapter 1 of Baldi & Moore, 4th ed.\n\nChildren’s food choices. Does the presence of popular cartoon characters on food packages influence children’s food choices? A study asked 40 young children (ages four to six) to taste two small pieces of Graham Crackers coming from a package with and a package without a popular cartoon character, and to indicate whether the two foods tasted the same or one tasted better. Unknown to the children, the crackers were the same both times. Here are the findings:\n\n\n\n\nTaste Preference\nNumber of Children\nPercent\n\n\n\n\nTastes better without character\n3\n\n\n\nTaste the same\n15\n\n\n\nTastes better with character\n22\n\n\n\n\n\nIdentify the individuals and the variable or variables in the study.\nPresent these data in a well-labeled bar graph.\nWould it also be correct to present these data in a single pie chart? Explain your reasoning.\nWhat do the data suggest about the influence of cartoon characters on Graham Cracker preference in young children\n\n\nThe study in Exercise 1 also asked the 40 children to taste small pieces of gummy fruit snacks and baby carrots presented in packages with and in packages without a popular cartoon character. For each food type, the children indicated which of the two options they would prefer to eat for a snack. (Note that this is a different question from the one asked in Exercise 1) The number and percent of children choosing the version with a cartoon on the package are displayed in the following table:\n\n\n\n\n\n\n\n\n\nFood Item\nNumber of children choosing the cartooon version\nPercent choosing the cartoon\n\n\n\n\nGraham Crackers\n35\n\n\n\nGummy and fruit snacks\n34\n\n\n\nBaby carrots\n29\n\n\n\n\n\nIdentify the individuals and the variable or variables in the study.\nMake a well-labeled bar graph of the data.\nWould it be correct to present these data in a single pie chart? Explain your reasoning.\nWhat can you conclude from these findings?\n\n\n\n\nOrdered and Unordered\nWhether the categorical variable is ordered or unordered affects the way we make the plot:\n\nOrdered: put the bars in order\nUnordered: put it in an arbitrary order\n\nAlternative: order according to largest to smallest.\n\n\n\n\nCode\nlibrary(palmerpenguins)\nggplot(penguins) +\n    aes(x = species) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories\")\n\n\n\n\n\n\n\nCode\nlibrary(forcats) # For rearranging \"factors\", aka. categorical variables\nggplot(penguins) +\n    aes(x = fct_infreq(species)) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories, Ordered by Count\")\n\n\n\n\n\nThe bar chart is the de-facto standard for categorical variables, whether binary or otherwise. For quantitative, variables, we need other options.\n\n\nQuantitative Variables\nRecall the distinction between discrete and continuous:\n\nDiscrete (whole numbers)\n\nEx. Number of students in a classroom.\n\nContinuous (could be measured with more precision)\n\nEx. height\n\n\n\n\n\n\n\n\nGrey Area\n\n\n\nWhat type of variable is “dose level”, defined as either no dose, half dose, or full dose? They aren’t whole numbers, but we can’t measure them with greater precision!\n\n\n\nQuantitative variables are split into discrete and continuous variables. Discrete variables are generally represented by whole numbers, for example, the number of students in a given classroom.\nIn contrast, continuous numbers could be anything! I like to think of them as numbers that could’ve been measured with more precision if we had better tools. For example, peoples Heights could be measured to infinite precision if we had perfect tools, whereas we don’t need better tools to measure the number of children and family more precisely.\nOf course, as with all things, there is a gray area here. Many studies will choose to give their subjects either no dose, a half dose or a full dose. These are obviously numbers and it is very likely that the response for a 0.75 dose is somewhere in between the half dose and the full dose. However, we chose these numbers and thus there are only three possible numbers. No amount of measuring is going to give us something other than a half dose (any deviation in administration of the dose can hopefully be ignored for the purpose of the study). In the definitions we’ve used it is neither a whole number, nor cannot be measured with higher precision. For the purposes of visualization, we might actually want to use a bar chart as if this were a categorical variable. If the dose had more categories and we expected the response to have a smooth trend across different dose levels, then we might use visualizations meant for discrete data. If the dose could have been any number between zero and one then we might use visualization meant for continuous data.\n\n\n\nPlotting Quantitative Variables\nHere are the lengths of sharks:\n 9.4 12.1 12.2 12.3 12.4 12.6 13.2 13.2 13.2 13.2 13.5\n13.6 13.6 13.8 14.3 14.6 14.7 14.9 15.2 15.3 15.7 15.7\n15.8 15.8 16.1 16.2 16.2 16.4 16.4 16.6 16.7 16.8 16.8\n17.6 17.8 17.8 18.2 18.3 18.6 18.7 18.7 19.1 19.7 22.8\n\nWe could have measured more precisely!\n\nThis is a continuous variable.\n\nCan’t just draw a bar chart with all sharks that were 9.4, all that were 12.1, …\n\n\nHow many we display this collection of shark lengths? It is clear that there are many different values that we could’ve gotten for the length and so we might not want to use something like a bar chart. Let’s try it anyway.\n\n\n\nQuantitative Variables as a Bar Chart\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\nsharks &lt;-  c(9.4, 12.1, 12.2, 12.3, 12.4, 12.6, 13.2, 13.2, 13.2, 13.2, 13.5,\n13.6, 13.6, 13.8, 14.3, 14.6, 14.7, 14.9, 15.2, 15.3, 15.7, 15.7,\n15.8, 15.8, 16.1, 16.2, 16.2, 16.4, 16.4, 16.6, 16.7, 16.8, 16.8,\n17.6, 17.8, 17.8, 18.2, 18.3, 18.6, 18.7, 18.7, 19.1, 19.7, 22.8)\n\nggplot() + aes(x = sharks) + geom_bar()\n\n\n\n\n\n\nThis plot demonstrates why bar chart isn’t appropriate for these data. We can see that each data point essentially gets its own bar, and so the heights are no longer meaningful. The exception is that these data are rounded to one decimal place, and so some lengths end up in the same bar. Knowing that some of our data are rounded to the same value is not necessarily meaningful for any analyses that we might want to do. Instead, we would like a chart that shows us where most of the data are, and whether or not they are clear patterns in these data.\n\n\n\nHistograms: Put observations into bins\nThe steps in building a histogram:\n\nChoose the bins.\n\ne.g. (0,10], (10,20], (20, 30], etc.\n\nThe notation (a, b] means that “a” is not included in the interval, but “b” is. We have no sharks that have a length of 0, but a shark with a recorded length of exactly 10 would be in the first bin, labelled (0, 10], not the second bin that is labelled (10, 20].\n\n\nCount the number of obs. in each bin.\nDraw a bar chart as if the bins are categories.\n\nBars should touch since there’s nothing in between.\n\n\n\n\nCode\n## Note that I've manually chosen the bin widths and centers.\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\nIn this histogram, the bins are (-1, 1], (1,3], (3,5]…\nNotice how the y-axis is still “Counts” (like a bar chart).\n\nMost of the time we will probably want to use a histogram to display quantitative, continuous data. A histogram is very much like putting continuous numbers into discrete bins, and then showing it as a bar chart. In this example, I chose bins from 1 to 3, then 3 to 5, then 5 to 7, and so on. For the bar on this histogram centred at a shark length of 12 we can see that there were five observations between 11 and 13. Note that the definitions of bins has a round bracket on the left side and the square bracket on the right side, this is to say that the left end point is not included but the right end point is included. This is just to account for cases where X may fall directly on the border between two bins, and we have to choose which bin. The actual bin we choose is arbitrary, kind of like driving in the left or the right. You will not be tested on whether you can remember which endpoint is inclusive.\nFrom the plot, we can see that most of the sharks are around 16 feet in length with sun going down to 10 feet and some as long as around 22 feet. The plot has a nice bell shape.\n\n\n\nHistograms: Bin Width Matters!\nThese histograms are showing the same data!\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=1.5, \n        center = 0.75, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nIn the previous graph, it looked like the distribution of sharks followed a nice bell-shaped curve. However, if we use bins that are 1.5 units wide we get a plot that looks fairly different. It still looks like most sharks are around 16 feet and some go down to 10 and some go as high is 22 or 23, But we see a large bar that covers 12 to 13.5.\nWith histograms the bins that you choose are extremely important. Most software have default values that are generally reasonable, But it’s always always always worth investigating other bins.\nA simple version of the plot can be made as follows, where ggplot chooses the bins automatically. Note that this is rarely desireable, and you should almost always choose the bins yourself.\n\nggplot() + \n    aes(x = sharks) +\n    geom_histogram()\n\n\n\n\nBelow is an app to visualize the difference that the binwidth can make!\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/DensHist\")\n\n\n\n\nDescribing Distributions\nWhen you’re asked to comment on a histogram, always mention the following:\n\nShape: Unimodal/bimodal and skewness\n\nSkewness: put a glob of peanut butter on toast, “skew” it to one side.\n\nCenter: midpoint (mean/median)\n\nMode depends on the bin!\nSkewness shows up in the relation between mean and median: “Mean less (than median) means left (skew).”\n\nSpread: the range/variance/IQR\n\nMore on IQR later!\n\nOutliers: points that don’t fit the shape\n\nMore on outliers when we cover IQR!\n\n\n\nThere are many shapes that a histogram can show. A distribution can be skewed (or “heavy-tailed”), which means that it looks like a bell curve but one side has a longer/thicker tail. We also want to know about several measures of the center of the distribution, as well as how spread out it is. Outliers are also something interesting to note; outliers are something that are not part of the shape (so you wouldn’t consider them when evaluating the skewness of a distribution).\nTry drawing out each of these shapes/patterns!\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis represents a bimodal distribution because it has two peaks (the word “mode” can refer to the category with the most observations, but it can also refer to the top of a peak). This would be described as a bimodal distribution with centres around 190 and 215, ranges around 195 to 205 and 205 to 235, with both peaks being symmetric and without any outliers.\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis is the classic sort of gotcha question that I like to use. This is actually a bar chart that I modified so that the bars have no space in-between - the x-labels are categories, not ranges! It may look somewhat symmetric and unimodal without any outliers, but the x-labels are out of order. These numbers are just numerical encodings of species names - 1 refers to Adelie penguins, 2 refers to Chinstrap, and 3 refers to Gentoo. These numbers were applied alphabetically because there isn’t really a logical way to order these species: they’re unordered categories!\nSo, basically, it does not make sense to talk about shape in a bar graph where the labels could have been put in any order!\n\n\n\nPurely Pedagogical: Stem-and-Leaf plots.\nConsider the data\n12, 43, 12, 32, 53, 66, 78, 25, 36, 12, 26,\n34, 98, 39, 44, 23, 15, 67, 1,  4,  54, 21\n\n\n\n\nStem-and-Leaf\n0  | 14\n10 | 2225\n20 | 1356\n30 | 2469\n40 | 34\n50 | 34\n60 | 67\n70 | 8\n80 |\n90 | 8\n\nSideways Histogram\n\nmydata &lt;- c(12, 43, 12, 32, 53, 66, 78, 25, 36, 12, 26,\n    34, 98, 39, 44, 23, 15, 67, 1,  4,  54, 21)\n\nggplot() + theme_minimal() +\n    aes(x = mydata) +\n    geom_histogram(binwidth = 10, center = 5,\n        fill = \"lightgrey\", colour = \"black\") +\n    scale_x_continuous(breaks = (0:10)*10, trans = \"reverse\", expand = c(0,0)) +\n    coord_flip() + \n    labs(x = \"Value\", y = NULL)\n\n\n\n\n\n\n\nThis visualization technique is shown purely for pedagogical reasons. A stem and leaf plot is like a histogram where the bins are all powers of 10. It isisplayed using the stem, which is the first digit and the leaf which is the second digit. For example, the number 78 has a 7 in the tens place (and were using the tens place as the stem) and an 8 in the ones place (and we using the ones place as the leaf). In the stem and leaf plot, 78 goes on the stem labelled 70 and it gets a leaf of eight. Going the other way, we can see a stem labelled 90 and a leaf labelled eight which corresponds to the number 98. For the stem labelled zero we have the numbers one and four, for the stem label 10 we have the numbers 12, 12, 12, and 15, and so on.\nEssentially, this is just a histogram. We’re instead of drawing a bar that corresponds to the number of observations in that bin, we are just listing the observations in that bin. Compare the stem and leaf plot to the sideways histogram on the right: in the first bin from 0 to 10 (not including 10) there are two numbers, one and four, and the length of the bin is two. For the stem label 20, we have the numbers 21, 23, 25 and 26, and this is displayed as a bar with link for in the histogram.\nThe main reason for showing this visualization technique is that it can be very useful for tests and quizzes, because it allows you to create a histogram without software. It also allows easy computation of the median since all of the leaves are in order. These are not used in practice, because in practice, you will have software to create histograms and find the median!\nNote that the shape of the distribution can be seen from the stem and leaf plot. It is a unimodal distribution that is right skewed and likely does not have an outlier (there is one point that appears to be separate from the others, but this is most likely due to bin choice).\n\n\n\nSummary\n\nIndividuals are what we make measurements on\n\nCan be pairs, dates, or people\n\nVariables are what we measure\n\nCan be derived from other measurements\n\nYou will not be asked to do anything with pie charts in this course.\nBar charts show counts of categories.\n\nCan optionally sum to 1 (like a pie chart).\n\nHistograms are like bar charts for binned data.\n\nBins matter.\nMust interpret shape."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#participation-questions",
    "href": "L01-Intro_PicturingGraphs.html#participation-questions",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.4 Participation Questions",
    "text": "1.4 Participation Questions\n\nQ1\nWhich of the following visualizations would be least appropriate for discrete data?\n\nBar Chart\nHistogram\nPie Chart\nStem-and-Leaf plot\n\n\n\nQ2\n\n\nWhat shape is the distribution on the right?\n\nSymmetric\nLeft Skewed\nRight Skewed\n\n\n\n\n\n\n\n\n\n\n\nQ3\nAge was collected as the number of days since birth. What kind of variable is this?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ4\nAge was collected as 0-17, 18-25, 25-34, or 35+. What kind of variable is this?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ5\nAge was collected as age in years, then rounded to the nearest 10. What kind of variable is the rounded value of age?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ6\n\n\nApproximately what proportion of the data is larger than 0? Note that there are 40 data points.\n\n50%\n25%\n75%\n100%\n\n\n\n\n\n\n\n\n\nAnswers: 334133\nExercises\n\nPrescriptions of opioid pain relievers. Opioid pain relievers are prescribed at a higher rate in the United States than in any other nation, even though abuse of these medications can result in addiction and fatal overdoses. The CDC examined opioid pain reliever prescriptions in each state to find out how variable prescription rates are across the nation. Here are the 2012 state prescription rates, in number of prescriptions per 100 people, listed in increasing order:\n\nopiods &lt;- c(52.0, 57.0, 59.5, 61.6, 62.9, 65.1, 66.5, 67.4, 67.9, 69.6, 70.8, 71.2, 71.7, 72.4, 72.7, 72.8, 73.8, 74.3, 74.3, 74.7, 76.1, 77.3, 77.5, 79.4, 82.0, 82.4, 85.1, 85.6, 85.8, 88.2, 89.2, 89.6, 90.7, 90.8, 93.8, 94.1, 94.8, 96.6, 100.1, 101.8, 107.0, 109.1, 115.8, 118.0, 120.3, 127.8, 128.4, 137.6, 142.8, 142.9)\n\nMake a histogram of the state opioid pain reliever prescription rates using classes of width 10 starting at 50.0 prescriptions per 100 people. e.g. (50, 60]. Do this by hand first, then using R.\nWould you say that the distribution is single-peaked or multiple-peaked? Is it roughly symmetric or skewed?\n\nThe Statistical Abstract of the United States, prepared by the Census Bureau, provides the number of single-organ transplants for the year 2010, by organ. The following two exercises are based on this table:\n\n\n\nDisease\nCount\n\n\n\n\nHeart\n2,333\n\n\nLung\n1,770\n\n\nLiver\n6,291\n\n\nKidney\n16,898\n\n\nPancreas\n350\n\n\nIntestine\n151\n\n\n\n\n(1.14 in BM) The data on single-organ transplants can be displayed in\n\n\na pie chart but not a bar graph.\na bar graph but not a pie chart.\neither a pie chart or a bar graph.\n\n\n(1.15 in BM) Kidney transplants represented what percent of single- organ transplants in 2010?\n\n\nNearly 61%.\nOne-sixth (nearly 17%).\nThis percent cannot be calculated from the information provided in the table.\n\nSee also: OpenIntro Textbook problems relating to visualizations that we have learned, especially 1.30, 1.36, 1.37, 1.39, 1.40, 1.47."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#footnotes",
    "href": "L01-Intro_PicturingGraphs.html#footnotes",
    "title": "1  Picturing Distributions with Graphs",
    "section": "",
    "text": "It is worth noting that there is also variance that isn’t information, and there’s information that we’ll never have access to. Variance is an opportunity to learn, but there’s almost always a limit to how much we can learn.↩︎\nA note on gender/sexuality/biology: penguins, especially Chinstrap and Gentoo penguins, don’t have particularly strong gender roles, and often form same-sex couples. In this course, I will use the term “sex” to mean “biosex”, rather than “gender”, to indicate that we’re looking at morphological differences due to XX and XY chromosomes.↩︎\nPossibly categorical if there are any penguins with chromosomal abnormalities.↩︎\nWe may, however, know that factory workers have more exposure to a pathogen than those who work from home, but we would likely want to measure this directly rather than measuring it by proxy with occupation.↩︎\nWe often encode on of the categories as 0 and the other as 1, but this is usually either clear (0 = no treatment, 1 = treatment) or completely arbitrary (0 = femala, 1 = male) and this arbitrariness is acknowledged. Neither of these cases affect the way we make plots or run analyses that are based on binary variables.↩︎\nNot discreet.↩︎\nThere are methods/visualizations that are specific to discrete variables, but they only apply in very specific circumstances and will not be taught in this course.↩︎\nIt will still create a plot with the correct x and y axes, but won’t draw the bars.↩︎"
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "href": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.1 Measures of Spread",
    "text": "2.1 Measures of Spread\n\nWhich has more variance?\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nAll have the same range (max - min).\n\n\nThese three data sets all have the same mean and median, but just looking at them shows that they are differnt collections of numbers. The first said, only has two unique values put those values are relatively far away from each other compared to the other sets. The second set is a more even spread from one to five. The third set has for value is equal to the mean and two values that may be outliers.\nTo me, the first set looks like it’s the most variable because all of the values are very far away from either the mean or the median. The second set has a smaller variance, because there are values closer to the mean. And the last set I expect to have the lowest variance, because most values are actually equal to the mean.\nThe formula for the variance, which will introduce next, matches this intuition. However, many students have different intuitions about which has the most variance and those are valid as well but are harder to quantify.\n\n\n\nMeasure of Spread: The Variance\nConsider set 1, which has a mean of 3:\n1 1 1 5 5 5\n\nThe distances to the mean are all -2 or 2\n\nIf we found the mean, we’d get 0! We need to make sure they’re all positive.\n\nAlternative 1: Absolute value. The average absolute distance to the mean is 2.\nAlternative 2: Squared value.\n\nThe average squared distance to the mean is 4\n\nImportant: This is not the actual variance calculation!\n\n\n\nThe variance is the average squared distance to the mean!\n(We use squared because math - more on this later).\n\nWe are basically looking at the average deviation from the mean. We want that deviation to be positive and there are several ways to do this. We have settled on squaring the numbers for the same reason we drive on the right side of the road: it’s just convention. There are benefits to using the absolute distance from the mean, but there are many mathematical advantages to squaring the values first.\n\n\n\nVariance Formula\n\n\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2\n\\]\nWe use \\(n-1\\) because of math reasons.\n\nThe easiest way to calculate this is to put it in a table:\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n1\n-2\n4\n\n\n2\n1\n-2\n4\n\n\n3\n1\n-2\n4\n\n\n4\n5\n2\n4\n\n\n5\n5\n2\n4\n\n\n6\n5\n2\n4\n\n\n\\(\\sum\\)\n18\n0\n24\n\n\n\nThe mean is 3, and the variance is 24/5 = 4.8.\n\n\n\nIn the table above, as before, the subscript \\(i\\) is just used to denote different observations. For example \\(x_1\\) is the first observation, \\(x_2\\) is the second observation in our data, and so on (this ordering is arbitrary).\nIn order to calculate the variance, we must first know the mean, and so it’s convenient to put this at the bottom of the table. We then squared the deviations from the mean and divided by \\(n-1\\). There are very good technical reasons why we divide by (n-1) that we won’t get into here. Come to my office and chat if you’d like to know more, or just ask ChatGPT!\n\n\n\n\n\n\n\\(n-1\\) in the denominator\n\n\n\nAs a quick explanation for \\(n-1\\), consider the variance of a single observation. It doesn’t vary! There’s not enough information to see how much variance there is. There isn’t enough information in our data. The \\(n-1\\) in the denominator enforces this - we can’t calculate the variance of one observation.\n\n\nNote that the variance can be calculated in R as follows:\n\nmy_values &lt;- c(1, 2, 2, 3, 4, 5, 6.3212, 3)\nvar(my_values)\n\n[1] 3.050982\n\n\n\n\n\nThe Variance and the Standard Deviation\n\\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x - \\bar x)^2}\n\\]\nThe standard deviation (or sd) is just the square root of the variance.\n\nThis makes it have the same units as the original data.\n\n\nIn addition, if we have two data sets and the variance of one is larger than the other, then the standard deviation is also larger. They’re the same thing, just in different units!\nI will often refer to one when I mean the other. When I’m comparing standard deviations, I may call them variances because the same patterns will be there.\nHere’s the R code:\n\nsd(my_values)\n\n[1] 1.746706\n\nsd(my_values)^2 # the sd is the square root of the variance\n\n[1] 3.050982\n\n\n\n\n\nExercise: Variation of the three sets\n\n\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nDraw out bar plots.\nSet up the table and find the variance.\nCompare the standard deviations; make a conclusion.\n\n\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n\\(\\sum\\)\n\n\n\n\n\n\n\n\n\nFill out the table above yourself!\n\n\n\nMeasure of Spread 2: The IQR\nThe IQR is very closely related to the median. But first, we will learn what quartiles are.\nConsider the data:\n1 2 3 4 5 6 7 8\nThe median of these data is 5; 50% of the data are to the left of this point.\n\n“Quartile”: Split the data into four.\n\nQ1: 25% of the data are to the left.\nQ2: 50% of the data are to the left (the median).\nQ3: 75% of the data are to the left.\n\n\n\n\nFinding Quartiles\n1 2 3 4 5 6 7 8\n\nFind the median\n\nIt’s 4.5. Cool.\n\nQ1 is just half of 50% - we’re finding a median again!\n\nQ1 is the median of everything to the left of the median.\nIn this case, 1 2 3 4 are the numbers to the left of the median, and so Q1 is 2.5.\n\nBy a similar argument, Q3 is 6.5.\n\n\nQ0 is where 0% of the data are to the left. In other words, it’s the minimum value in the data! Similarly, Q4 is the maximum value in the data.\n\n\n\n\n\n\nWarning\n\n\n\nThe algorithm we just used for computing the quartiles is not the only one! In R, there are NINE different ways to calculate the quartiles. You should stick to doing this by hand if you want to get the WeBWork answers right.\n\n\n\n\n\nThe five number summary\nLet’s use the folowing example:\n1, 3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 8, 9, 10, 10, 11, 12\nThe quartiles give an excellent way to summarise data:\n\n\n\nQ0 (min)\nQ1\nQ2 (median)\nQ3\nQ4 (max)\n\n\n\n\n1\n4.5\n7\n8.5\n12\n\n\n\n\nThe five number summary just shows all five of the quartiles. Note that there are five quartiles, because zero is also one of them.\nFor practice, make sure you can calculate the median, and then the median of all the values to the left of it!\n\n\n\nVisualizing the five number summary: the boxplot\n\n\nThe plot on the right shows the body masses for the Palmer Penguins.\n\nThe lowest point is Q0\nThe left of the box is Q1\nThe thick line in the box is Q2 (the median)\nThe right of the box is Q3\nThe highest point is Q4\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(y = body_mass_g) +\n    geom_boxplot() +\n    labs(y = \"Body Mass (g)\") +\n    coord_flip()\n\n\n\n\n\nggplot(penguins) + \n    aes(x = body_mass_g) +\n    geom_histogram(colour = 1, fill = \"lightgrey\") +\n    labs(x = \"Body Mass (g)\")\n\n\n\n\n\n\n\nThe boxplot and the histogram both demonstrate the right skew of the data, but the boxplot is much more compact!\nTake a moment to compare the two plots and make sure you can explain the skewness. Remember that 25% of the data are in each interval shown in the box plot!\n\n\n\nMeasure 2: The Inter-Quartile Range (IQR)\nThe IQR is defined as: Q3 - Q1.\n\nSame units as the original data\nRobust to outliers (unlike the sd)!\n\n\nThis is the second measure of spread that we will learn. The IQR is commonly used when we have highly skewed data or data with outliers. The sd measures the average squared deviation from the mean, whereas the IQR measures the middle 50% of the data.\nNotice how this is not centered on the median. Consider the following data:\n\nmy_values &lt;- c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 10)\nlength(my_values)\n\n[1] 20\n\nhist(my_values)\n\n\n\n\nThe median of these should be at position \\((n+1)/2 = (20 + 1)/2 = 10.5\\) (I used the length() function in R to count the number of observations for me). This value is halfway between 3 and 3, meaning it’s 3. Q1 is the median of the first 10 data points, which is at position 5.5, giving us a value of 2. Q3 is 5.5 positions from the end, which is 4.5. Thus the IQR is \\(4.5 - 2 = 2.5\\).\nFirst, does this make sense to you? Does 2.5 sound like a reasonable width for the middle 50%?\nNow consider that the distribution is clearly skewed to the right. This affects the variance a lot, but the IQR would have been the same no matter what the first 4 or last 4 values were.\n\n\n\nIQR for Outliers\nIn this class, we use a rule of thumb for calculating outliers. Anything that is…\n\nBelow the median minus 1.5*IQR, or\nabove the median plus 1.5*IQR\n\nis considered an outlier.\n\nThis rule of thumb is not based on any mathematical derivations, it just seems to work in most situations.\nThe idea is that the IQR gives a measure of spread, and the median gives the measure of the centre, so anything too far from the centre is an outlier. We use the spread to figure out how far away from the centre we are willing to accept. This will show up several times in this course. We’ve seen it in this example for the IQR and median because this is simple and easy to interpret.\nMost of the rest of this course will be spent looking at something similar for the mean. We will still use this idea of the centre plus or minus some measure of the spread, but will incorporate information about the sample and assumptions about the population that allow us to make much stronger conclusions beyond simply checking if something is an outlier.\n\n\n\nSummary\n\nThe “centre” is trying to measure the most common value.\n\nOften, this is our best prediction.\n\nThe “spread” is trying to measure the scale, or variation.\n\nGives context to the centre.\n\nThe mean and variance\n\nInterpretations and formulas are important.\n\nThe median and IQR\n\nCalculations, interpretations, five-number-summary, outliers, and boxplots are all important.\n\n\n\nWe saw the same thing a couple of times throughout this lesson. We saw measures of centre that try to describe the middle of a distribution and centres of spread that tell us how spread out the data are. The mean and the sd are intrinsically linked, and the median and IQR are intrinsically linked.\nWe also saw the rule-of-thumb to use IQR for finding outliers by using the median plus-an-minus some number times the spread. You better believe that this idea will show up again later in this course!\nBoxplots are a visual representation of the five number summary. These can be very small while still showing the shape of our data. However, these only work for unimodal data - there isn’t a good way to show a bimodal distribution on a boxplot. Also, it is very easy to plot two boxplots for two different data sets in order to compare the distributions.\nFor assignments and exams, be ready to calculate any of these values and compare the mean/median and sd/IQR. Also be ready to compare the five number summary to a boxplot.\nExercises\n\nSpider Silk. Spider silk is the strongest known material, natural or man-made, on a weight basis. A study examined the mechanical properties of spider silk using 21 female golden orb weavers, Nephila clavipes. Here are data on silk yield stress, which represents the amount of force per unit area needed to reach permanent deformation of the silk strand. The data are expressed in megapascals (MPa):\n\n164.00 173.00 176.10 236.10 251.30 270.50 270.50\n272.40 282.20 288.80 290.70 300.60 327.20 329.00\n332.10 351.70 358.20 362.00 448.90 478.70 740.20\n\nDescribe the shape, centre, and spread of the data using a histogram (code below).\nFind the mean and median yield stress. Compare these two values. Referring to the histogram produced by the code below, what general fact does your comparison illustrate?\nRe-run the code using different values of breaks. What do you see? (Note that this example uses base R rather than ggplot2 because it has simpler code - ggplot2 has more flexibility, but that flexibility isn’t necessary here.)\nUse the boxplot() function to create a boxplot (you do not need the breaks=10 part of the code). Compare this to the histogram. Also comment on any points that stand out (when there are outliers, R shows \\(Q2\\pm 1.5IQR\\) rather than Q0 and Q4).\nUse the \\(Q2\\pm 1.5IQR\\) formula by hand to find the outliers, and verify your calculations with the R plot.\n\n\nsilk_stress &lt;- c(164.00, 173.00, 176.10, 236.10, 251.30, 270.50, 270.50,\n    272.40, 282.20, 288.80, 290.70, 300.60, 327.20, 329.00,\n    332.10, 351.70, 358.20, 362.00, 448.90, 478.70, 740.20)\nhist(silk_stress, breaks = 10)\n\n\n\n\n\nDeep-sea sediments. Phytopigments are markers of the amount of organic matter that settles in sediments on the ocean floor. Phytopigment concentrations in deep-sea sediments collected worldwide showed a very strong right-skew. Of two summary statistics, 0.015 and 0.009 gram per square meter of bottom surface, which one is the mean and which one is the median? Explain your reasoning.\nGlucose levels. People with diabetes must monitor and control their blood glucose level. The goal is to maintain a “fasting plasma glucose” between approximately 90 and 130 milligrams per deciliter (mg/dl). The data tables below give the fasting plasma glucose levels for two groups of diabetics five months after they received either group instruction or individual instruction on glucose control.\n\nI provide the data as vectors in R, but you don’t need R for this question (it’s good practice to do it both ways).\n\ngroup &lt;- c(78.00, 95.00, 96.00, 103.00, 112.00, 134.00, 141.00, 145.00, 147.00,\n    148.00, 153.00, 158.00, 172.00, 172.00, 200.00, 255.00, 271.00, 359.00)\n\nindividual &lt;- c(128.00, 128.00, 158.00, 159.00, 160.00, 163.00, 164.00, 188.00, 195.00,\n    198.00, 220.00, 221.00, 223.00, 226.00, 227.00, 283.00)\n\n\nCalculate the five-number summary for each of the two data sets.\nMake side-by-side boxplots comparing the two groups. What can you say from this graph about the differences between the two diabetes control instruction methods? (Hint, you can create side-by-side boxplot using the code boxplot(variable_1, variable_2).)\nObtain the mean and standard deviation for each sample. Does this information give any clue about the shape of the two distributions?\nAdd to the historgrams a symbol representing the mean of each group and error bars representing one standard deviation above and below the mean. (You can do this by hand.) Compare this graphical summary with the boxplot display you also created.\nUse the 1.5 × IQR rule to identify any suspected outliers. Then look at the raw data to determine if unusually high or low values in either data set actually are outliers."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#relationships",
    "href": "L03-Scatterplots_Correlation.html#relationships",
    "title": "3  Scatterplots and Correlation",
    "section": "3.1 Relationships",
    "text": "3.1 Relationships\n\nExplanatory and Response Variables\n\nResponse: Responds to the explanatory variable.\n\nAlso called dependent variable.\n\nExplanatory: Explains the response variable.\n\nAlso called independent variable.\n\n\nKnowledge about explanatory tells us about the response.\n\nWe are not assuming the explanatory causes the response. We will not be covering causality in this course.\nWe are discovering tendencies, not rules.\n\n\nI just want to make this very clear: we are not looking for a causation. Instead, we’re just looking at whether or not to variables are related, and we think that measurements of one will be enough to tell us about measurements of the other. For example, if we think one variable is easy to measure and another is harder to measure, then we might want to set the easy to measure variable as the explanatory variable and see if it “explains” the harder to measure variable. This has nothing to do with the easy to measure variable causing the hard to measure one.\n\n\n\nExamples\n\nBlood alcohol content affects reflex time. – Some individuals may be more or less affected.\nSmoking cigarettes is associated with increased risk of lung cancer, and mortality. – Some heavy smokers may live to age 90\nAs height increases, weight tends to increase.\n\nHeight does cause weight, but there are other explanations.\n\n\n\nIn these examples, we carefully use words like “affects”, “associated with”, and “tends to”. For all of these examples we would expect a relationship of some sort, but the causality is not necessarily obvious.\nWe obviously expect the blood alcohol contact to affect reflex time. We expect this to be a causal relationship.\nIn the mid-1900s, it was hypothesized by cigarette companies that, rather than cigarettes causing cancer, people who were at increased risk of lung cancer with the sorts of people who also tended to smoke. Finding a relationship was not enough to convince people that it was cigarettes causing lung cancer. Even though we know that there’s a relationship between cigarettes and lung cancer, the techniques we learn in this course are not enough to conclude causality.\nHeight and weight are an example of how are the knowledge of one variable tells us about the other, without there being any causal relationship. We expect that taller people will have more mass, but there are also other reasons why somebody might have more mass that or not captured by their height."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#scatterplots",
    "href": "L03-Scatterplots_Correlation.html#scatterplots",
    "title": "3  Scatterplots and Correlation",
    "section": "3.2 Scatterplots",
    "text": "3.2 Scatterplots\n\nExample\n\n\nIn the data frame above, we have an observation of the number of power boats registered in each year, as well as the number of manatees that died in a collision with a powerboat in that year. The table shown above is only a small part of the data.\nThe rest of the data are shown in the plot. To create this plot, we put the number of powerboats registered on the X axis, and the number of manatee deaths on the Y axis. The annotation on the plot demonstrates how the points were added. One of the columns in the data is labelled 1977 and in that year they were to 755,000 powerboats registered and they were 54 manatee deaths that year. Because these two numbers are measured on the same individual (with the individual being the year in this example), we know that those two numbers go together. If we had a collection of peoples heights and a separate collection of peoples weights, but no knowledge of which individual each was collected on, then we would not be able to make a scatterplot. In order to make a scatterplot, we have to know which observation on the X axis is associated with which observation on the Y axis.\n\n\n\nWhat to look for\n\n\n\nOverall pattern\n\nLinear, curved, etc.\nDirection (increasing/positive, decreasing/negative)\nConstant variability\n\nDeviations from the pattern\n\nE.g., linear only in a small range\n\nOutliers\n\nAs before, discuss outliers separately from the pattern.\n\n\n\n\n\n\n\nIn general for this course were looking for a linear pattern. There are other models out there that fit nonlinear patterns, but we do not cover them in this course. There’s one way for things to be linear, and there are an infinite number of ways for things to be nonlinear. However, there are many common ways to account for non-linearity while still using a linear model.\nRegardless of whether something is linear or has some sort of curve, we are very interested in how strong of a pattern there is. For a linear model this means we want the points to be very close to the line, whereas for non-linear models we want the pattern to be very clear. We generally want patterns to pass the “facial impact test”, were the pattern is so obvious that it might as well be slapping you in the face (this is not an official test).\nAs with describing the shape of histograms, we treat outliers as something that are not part of the shape. We can have a clear linear pattern that happens to have an outlier.\nThe plot of manatees versus powerboats above would be described as a strong linear pattern, perhaps with some extra variation at larger X values.\n\n\n\nPenguins!\n\n\nWhat pattern is this?\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() + \n    geom_smooth(formula = y~x, method = \"lm\", se = FALSE) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\n\nThe plot above shows a clear linear pattern. There is still some variation above and below the lines, but the pattern is still clear. It kinda looks like there may be two clusters; there’s a space between the two groups in the center of the X axis.\n\n\n\nAdding a Categorical Variable\n\n\nEach point has an \\(x\\) coordinate, \\(y\\) coordinate, and some other information.\nWe can encode that information with a colour!\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g,\n        colour = species) +\n    geom_point() +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\n\nFrom this plot, we can see that the three species in these data all have a similar relationship, but still it might be worth separating out the groups and seeing what happens!\n\n\n\nThe Importance of Plotting: Anscombe’s Quartet\n\ndata.frame(\n    variable = names(anscombe),\n    mean = apply(anscombe, 2, mean),\n    sd = apply(anscombe, 2, sd)\n) |&gt; knitr::kable(row.names = FALSE)\n\n\n\n\nvariable\nmean\nsd\n\n\n\n\nx1\n9.000000\n3.316625\n\n\nx2\n9.000000\n3.316625\n\n\nx3\n9.000000\n3.316625\n\n\nx4\n9.000000\n3.316625\n\n\ny1\n7.500909\n2.031568\n\n\ny2\n7.500909\n2.031657\n\n\ny3\n7.500000\n2.030424\n\n\ny4\n7.500909\n2.030578\n\n\n\n\n\n\nIn this lecture were introducing plots before we talk about numerical summaries of two variables for a very good reason. The date is it displayed above is a well-known dataset called Anscombes quartet. Up to the first two decimal places, all of the variables in the data have the same mean and standard deviation. If this were all of the information you had, you might expect the plots to look similar.\n\n\n\nAnscombe’s Quartet\n\npar(mfrow = c(2,2), mar = c(3,3,2,1))\nplot(y1 ~ x1, data = anscombe)\nabline(lm(y1 ~ x1, data = anscombe))\n\nplot(y2 ~ x2, data = anscombe)\nabline(lm(y2 ~ x2, data = anscombe))\n\nplot(y3 ~ x3, data = anscombe)\nabline(lm(y3 ~ x3, data = anscombe))\n\nplot(y4 ~ x4, data = anscombe)\nabline(lm(y4 ~ x4, data = anscombe))\n\n\n\n\n\nClearly, there’s a very different pattern in each plot.\n\nThe first plot looks relatively linear with a little bit of random variation. For this data set a linear model does seem appropriate.\nThe plot at the top right she was a very clear pattern that is not linear, so we may be able to fit a model that accounts for this non-linearity.\nThe plot at the bottom left is almost a perfect line, but with an outlier. This outlier makes it so that the line that I have added to the plot doesn’t actually go through the perfect pattern that we can see if that outlier weren’t there.\nThe bottom right plot is a mess. If it weren’t for the outlier, the X values would all be identical! In this case, a scatterplot would not be appropriate. If I saw this while analysing my data, I would have assumed that X was supposed to be either constant (e.g., all X values should have been 8) or categorical. In both cases, a scatterplot would not be appropriate.\n\nDespite all of these wildly different shapes, all of these data sets have the same summary statistics.\n\n\n\nSummarizing Plots\n\nEach data point has an \\(x\\) and a \\(y\\). We plot \\(y\\) against \\(x\\).\n\n\\(y\\) is the response, \\(x\\) is the explanatory variable.\n\nWe’re looking to see if it’s linear. Linear models are something we know how to deal with!\n\nDeviations from linearity are noteworthy.\nOutliers are noteworthy.\n\nWe can incorporate more information in a scatterplot, especially categorical variables."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#correlation",
    "href": "L03-Scatterplots_Correlation.html#correlation",
    "title": "3  Scatterplots and Correlation",
    "section": "3.3 Correlation",
    "text": "3.3 Correlation\n\nMeasuring Strength of Linearity\n\n\nFrom plots, we can sorta see that one looks more linear than another.\nIt would be splendid if we could have a way to quantify this…\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nx &lt;- runif(100, 0, 10)\ny1 &lt;- 2 + 3*x + rnorm(100, 0, 4)\ny2 &lt;- 2 + 3*x + rnorm(100, 0, 1)\n\ng1 &lt;- ggplot() + aes(x = x, y = y1) + geom_point() +\n    labs(title = \"Strong correlation\")\ng2 &lt;- ggplot() + aes(x = x, y = y2) + geom_point() +\n    labs(title = \"Stronger correlation\")\ng1 / g2\n\n\n\n\n\n\n\nFrom this point on, we’re focusing on linear relationships. The plots above both demonstrate the same linear relationship, but with different “strength”s. Let’s measure that!\n\n\n\nThe correlation coefficient \\(r\\)\nRecall the formula for the variance: \\[\ns_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)(x_i - \\bar x)\n\\]\nThe correlation coefficient is defined as: \\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\] where \\(s_x\\) is the s.d. of \\(x\\) and \\(s_y\\) is the s.d. of \\(y\\).\nIt’s like a variance for two variables at once!\n\nThis explanation might not stick for those of you who aren’t a fan of formulas, but I think this demonstrates an important aspect of the correlation coefficient. The formula for the standard deviation includes \\((x_i - \\bar x)(x_i - \\bar x)\\). If we replaced one of those with \\(y\\), we’d get \\((x_i - \\bar x)(y_i - \\bar y)\\), which is one step closer to the correlation coefficient. In other words, the correlation is a measure of how two (quantitative) variables vary together!\nLet’s try another approach. \\(x\\) has variance. \\(y\\) has variance. They also have variance with each other. This is measured by the correlation!\nIf neither of these explanations make sense, don’t worry! We’ll see plenty of correlations and get an intuition for how correlations are different with different data.\n\n\n\nThe range of \\(r\\)\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\n\\(s_x\\) and \\(s_y\\) are positive\n\\(s_x &gt; \\sum_{i=1}^n(x_i - \\bar x)\\), similar for \\(s_y\\)\n\nThis can’t be larger than 1\n\n\\(x_i - \\bar x\\) can be negative (same with \\((y_i-\\bar y)\\)).\n\nThe correlation coefficient can be anything from -1 to 1, with 0 representing no correlation and -1 and 1 representing perfect correlation.\n\nThe fact that the correlation can be negative is important. A correlation coefficient of -1 looks like a perfect downward slope.\n\n\n\nInterpreting correlation\n\n1 and -1 are perfect correlation.\n0.8 is a strong correlation (depending on context)\n\nPhysics: 0.8 is very very weak.\nSocial science: 0.8 is very very strong.\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/ScatterCorr\")\n\n\nThe app above shows data that start uncorrelated, then are slowly transformed into perfect correlation. If you hav R installed on your computer it should run just fine (you may need to run install.packages(\"shiny\") for the shiny package, and possibly install.packages(\"ggplot2\") if you haven’t already).\nFor more examples (and more info on the correlation coefficient in general), see the OpenIntro Textbook and let me know what you think of that textbook!\n\n\n\nComments on the correlation\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\nThe order of \\(x\\) and \\(y\\) can be switched\n\n2 times 3 is the same as 3 times 2.\n\nSince we’re subtracting the mean and dividing by the s.d., the units don’t matter!\n\nSwitching from kg to lbs has no effect on the correlation.\n\n\\(r&gt;0\\) means the line goes up. \\(r &lt; 0\\) means the line goes down.\nQuantitative only\nLinear only\nNot robust to outliers.\n\n\nLet’s explore some of these points with code!\n\nplot(y1 ~ x1, data = anscombe)\n\n\n\n\nIt looks relatively linear. Take a moment to think of how correlated these two variables are, and assign it a value between 0 and 1. This is how you would guess the correlation coefficient\nOn exams, you will be expected to differentiate between “not correlated” (about 0), “slightly correlated” (0.2 to 0.4), “very correlated” (0.6 to 0.8), and “near perfect correlation (almost exactly 1)”, or the negatives of these values; you won’t need to guess whether the correlation is 0.55 or 0.6.\nIn R, we calculate the \\(r\\) with the cor() function.\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nDoes this number make sense to you? It seems fairly high to me, but with small amounts of data it’s not that surprising. Think of it this way: if you removed a quarter of the data at random, would you still be able to see the pattern? If so, then it’s probably “very correlated”!\nThe first point states that the order doesn’t matter:\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nThe units don’t matter:\n\ncor(anscombe$y1*5 + 1, anscombe$x1)\n\n[1] 0.8164205\n\n\nHowever, it does matter if we do a non-linear transformation, such as squaring the values. The correlation is a measure of linear association, so making things non-linear will affect it.\n\nplot(y1^2 ~ x1, data = anscombe)\n\n\n\ncor(anscombe$x1, anscombe$y1^2)\n\n[1] 0.7992029\n\n\nFor these data, squaring didn’t have much of an effect (as we can see in the plot), but we still saw a change in \\(r\\)! Notice that a unit change had absolutely no effect on \\(r\\). In general, we either expect things to be exactly the same or they can be completely different; very few things are “almost equal” in the general case (they may be almost equal with one set of data, but that means nothing for completely different sets of data).\n\n\n\n\\(r\\) measures linear correlation\n\n\nEnzymatic activity is known to be affected by temperature. A study examined the activity rate (in micromoles per second, μmol/s) of the digestive enzyme acid phosphatase in vitro at varying temperatures (measured in kelvins, K). The findings are displayed in the following table.\n\nDescribe the relationship\nExplain why it doesn’t make sense to describe this as “positively associated” or “negatively associated”.\nIs this a strong or a weak relationship? Explain.\n\n\n\n\n\n\nSolutions:\n\nThe relationship increases with an upward curve from temperatures of 300K to 340K, when it turns downward sharply and decreases to 355K.\nThe association is different for different X values. This is not a linear relationship, which means we have to do extra work to make sure that we cover all the non-linearities.\nThis is a very strong relationship. The pattern clearly passes the facial impact test that we discussed before. It is far from a linear relationship, but it’s clearly noticable.\n\n\n\n\nAgain, always plot your data!!!\n\n\nAll of the plots in the Anscombe quartet have the same correlation coefficient.\n\\(r\\) is a measure of linear association - if it’s not linear, \\(r\\) can’t be interpreted!!!\n\n\n\n\n\n\n\n\n\nIt’s important to note that \\(r\\) can always be calculated for numeric data. If we had student numbers as well as a categorical variable that used 0 to represent black, 1 to represent asian, etc., then we could technically calculate the correlation coefficient. This would be utterly meaningless!!!!!\n\n\n\nExample: Penguins\n\n\n\n\n\n\nThis is an example of something called Simpson’s Paradox: If we don’t account for the sub-groups, we get the opposite affect! As we can see in the plot, if we have all the groups together than it looks like a negative correlation, but once we separate groups each individual group has a positive correlation.\n(Note that I hid the code for this plot - the code I used to ensure the colours matched and I got the right layout is pretty advanced, and I also used some tricks along the way.)\n\n\n\nCorrelation Summary\n\n\\(r\\) is a measure of linear association\n\nI’ve said it plenty, I’ll say it again: \\(r\\) does not apply to non-linear patterns!\nAlways plot your data before calculating \\(r\\).\n\n\\(r\\) is like a measure of how two variables vary together.\n\nFormula is similar to the variance formula!\n\n\\(r\\) is a number between -1 and 1, with 0 meaning no correlation and 1 or -1 meaning perfect correlation.\n\nA negative \\(r\\) means a negative relationship (i.e. a line that goes down).\n\nEverything on the “Comments” slide is fair game for test questions."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#participation-questions",
    "href": "L03-Scatterplots_Correlation.html#participation-questions",
    "title": "3  Scatterplots and Correlation",
    "section": "3.4 Participation Questions",
    "text": "3.4 Participation Questions\n\nQ01\nThe correlation coefficient measures the strength of the correlation.\n\nTrue\nFalse\n\n\n\nQ02\nIf \\(r &lt; 0\\), there is a negative linear correlation.\n\nTrue\nFalse\n\n\n\nQ03\nWhich of the following represents the strongest correlation?\n\n0.4\n0.7\n-0.8\n0\n\n\n\nQ04\n\n\nWhat is the best description of the plot on the right?\n\nNo correlation, has an outlier.\nStrong correlation, has an outlier\nSlight negative correlation\nShapeless\n\n\n\nx &lt;- c(runif(99, 0, 10), 11)\ny &lt;- c(rnorm(99), 20)\n\nggplot() + aes(x = x, y = y) + geom_point() #+ \n\n\n\n    #geom_smooth(method = \"lm\", formula = y~x, se = FALSE)\n\n\n\n\n\nExtra context: Fitting a Line\n\n\nA line would try and fit the outlier, which misleads us into thinking there might be a correlation!\n\n\nggplot() + aes(x = x, y = y) + geom_point() + \n    geom_smooth(method = \"lm\", formula = y~x, se = FALSE)\n\n\n\n\n\n\n\n\nQ05\nWhich statement is true.\n\nThe explanatory variable causes the response.\nThe response must be something measured after the explanatory variable.\nWe use the explanatory variable to explain the response, without assuming causality.\nThe correlation between the explanatory and response variable will be positive if the explanatory causes the response, negative if the response causes the explanatory.\n\n\n\nQ06\nWe can add colour to a plot using what type of variable?\n\nCategorical\nQuantitative\n\n\nExercises:\n\nThe following code will draw a plot and calculate the correlation coefficient. Currently, it’s doing this for the column mpg (response) versus the column wt (“weight”, explanatory) in the mtcars data which is built in to R.\n\nRe-run the code, but replace wt with disp (engine displacement), hp (horsepower), drat (rear axle ratio, although I couldn’t explain this further), and qsec (quarter mile time, in seconds). Comment on the apparent pattern and the magnitude of the correlation.\nChange wt tocyl, the number of cylinders. What do you notice about the plot, and how does this affect your interpretation of the correlation between mpg and cyl? Explain why cyl might be better incorporated as a categorical variable, even though it is indeed numeric.\nRepeat part (b) for am, which is “0” for automatic transmission and “1” for manual transmission.\n\n\n\nplot(mpg ~ wt, data = mtcars)\n\n\n\ncor(mtcars$mpg, mtcars$wt)\n\n[1] -0.8676594\n\n\n\nThe following figure comes from the article “Shared neural representations and temporal segmentation of political content predict ideological similarity” by De Brujin et al., published in 2023 (link to aricle here). The star on the plot indicates that they have found a statistically significant relationship (more on this next week). Is this a strong correlation?\n\n\n\nThe following figure comes from the article “Effect on Blood Pressure of Daily Lemon Ingestion and Walking” by Kato et al., published in 2013 (link to article here). Comment on the shape of this relationship. Recall how we described a “strong” shape as a shape that remains even if some of the data points were removed.\n\n\nExercises from OpenIntro Biotatistics textbook\nQuestions 1.35, 1.36, 1.37.\nFor further R practice and case studies, see the labs page for the OpenIntro textbook."
  },
  {
    "objectID": "L04-Regression.html#introduction",
    "href": "L04-Regression.html#introduction",
    "title": "4  Regression",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThese notes are based on Chapter 4 of Baldi & Moore and Chapter 6.1 to 6.3 in OpenIntro Biostats.\nIn linear modelling, we have a collection of pairs \\(x_i\\) and \\(y_i\\). We think that there’s some sort of relationship between \\(x\\) and \\(y\\), and we think that a line is an adequate way to characterize that relationship1.\nJust like we assume that there’s a “true” population mean, there is also a “true” slope and intercept for the line that characterizes the relationship between \\(x\\) and \\(y\\). In the plot below, the green line represents the “true” relationship between \\(x\\) and \\(y\\), and the data are random values above and below that line2.\n\n\n\n\n\nIn high school, you may have learned a line as \\(y = mx + b\\). In statistics, we often use latin letters for estimates and greek letters for population parameters3. The population line is thus:\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\n\n\\(\\alpha\\) is the intercept.\n\\(\\beta\\) is the slope.\n\nA 1 unit increase in \\(x\\) corresponds to a \\(\\beta\\) increase in \\(y\\).\n\n\\(\\epsilon_i\\) is random noise (\\(N(0,\\sigma)\\)).\n\nAgain, we think of \\(x\\) as being fixed. The random noise is above and below the line, not side to side.\n\nThe formula implies that \\(y_i \\sim N(\\alpha + \\beta x_i, \\sigma)\\), since \\(y_i\\) is centered at \\(\\alpha + \\beta x_i\\) but randomly varies above and below the line with variance \\(\\sigma^2\\).\n\nThe word “regression” means to go backward. I like to think that we are “going backward” to the population numbers from the sample values4. Any situation where you are estimating a population parameter is technically a regression, but this terminology is not useful for this class.\nTo regress, we estimate the parameters using sample statistics. For linear regression, we use regular old latin letters instead of the fancy greek ones. \\(a\\) is the estimate for \\(\\alpha\\), \\(b\\) for \\(\\beta\\), and \\(e\\) for \\(\\epsilon\\). In order to do find these sample statistics, we minimize the squared error between the line and the data:\n\\[e_i^2 = (y_i - a - b x_i)^2\\]\nIn other words, we find \\(a\\) (for \\(\\alpha\\)) and \\(b\\) (for \\(\\beta\\)) that make the sum of the squared errors \\(e_i\\) as small as possible. We use the squared errors for the same reason we use squared deviations in the forumla for the variance: so that positive and negative values do not cancel out5.\nThe estimates \\(a\\) and \\(b\\) are as follows:\n\\[\\begin{align*}\nb &= rs_y/s_x\\\\\na &= \\bar y - b\\bar x\n\\end{align*}\\]\nThese are called the least squares estimates6. The equation for \\(b\\) is especially important!\nIn R, these can be calculated as follows. The mtcars data set is a collection of measurements made on various cars. In this example, we’ll regress the fuel efficiency (in miles per gallon, or mpg) against the weight of the car.\n\n## Load a built-in data set\ndata(mtcars) \n\n## Define which variables are x and y.\n## This isn't necessary, but helps with teaching\nx &lt;- mtcars$wt\ny &lt;- mtcars$mpg\n\n## Calculate the estimates by hand\nb &lt;- cor(x, y) * sd(y) / sd(x)\na &lt;- mean(y) - b * mean(x)\n\n## Print the estimates \nc(a, b)\n\n[1] 37.285126 -5.344472\n\n## Use the built-in functions\nsummary(lm(y ~ x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nx            -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom this line, we can make predictions about new points by simply plugging in the \\(x\\) value. For example, let’s say we wanted to guess the mpg of a car that weighs 3,000 lbs. In the data, the units for weight are 1000 lbs, so this means plugging a value of wt=3 into the data.\n\na + b*3\n\n[1] 21.25171\n\n\nSo we would guess that a 3 ton car would have a fuel efficiency of 21.25 miles per gallon. Let’s look at this on a plot:\n\nplot(y ~ x)\npoints(3, a + b*3, col = \"red\", pch = 16)\n\n\n\n\nIt looks like this is somewhere around where we would expect.\nIf we repeat this for every possible \\(x\\) value, we get the regression line below:\n\nplot(y ~ x)\npoints(3, a + b*3, col = \"red\", pch = 16)\n## abline adds a line with slope b and intercept a to a plot.\nabline(a = a, b = b, col = \"red\")\n\n\n\n\nWe cal also see the values of \\(e\\), the residuals.\n\ne &lt;- y - (a + b*x)\nplot(e ~ x, main = \"Plot of the Residuals\")\n## abline can also draw a line with slope 0 (horizontal)\nabline(h = 0, col = \"grey\")"
  },
  {
    "objectID": "L04-Regression.html#regression-facts",
    "href": "L04-Regression.html#regression-facts",
    "title": "4  Regression",
    "section": "4.2 Regression Facts",
    "text": "4.2 Regression Facts\nHere are some facts about the least squares regression line:\n\nThe point \\((\\bar x, \\bar y)\\) is always on the line.\n\nLeast squares regression can be seen as putting a line through \\((\\bar x, \\bar y)\\) and rotating it until the squared error is the smallest.\n\n\\(s_y\\ge 0\\) and \\(s_x\\ge 0\\), so whenever \\(r &gt; 0\\), we know that \\(b &gt; 0\\).\n\nThe slope has the same sign as the correlation. Otherwise, the slope could be pretty much any number, regardless of the correlation.\nIf \\(r = 0\\), then \\(b = 0\\), and vice versa.\nOther than the sign and the special case of \\(r=0\\), there is no way to tell the value of \\(r\\) if all you know is \\(b\\).\n\nFor \\(r\\), the distinction between \\(y\\) and \\(x\\) doesn’t matter.\n\nFor the regression line, it absolutely matters!\n\nThe sum of the errors is 0.\n\n\n## The prediction at mean(x) is equal to mean(y)\n## In other words, (mean(x), mean(y)) is a point on the line\na + b * mean(x)\n\n[1] 20.09062\n\nmean(y)\n\n[1] 20.09062\n\n## Correlation doesn't care about order\ncor(x, y)\n\n[1] -0.8676594\n\ncor(y, x)\n\n[1] -0.8676594\n\n## Theoretically 0, but computers aren't perfectly precise\n## Note: e-14 refers to 10^-14, or 14 zeroes before the first digit\n    # So, pretty close to 0.\nsum(e) \n\n[1] 1.065814e-14"
  },
  {
    "objectID": "L04-Regression.html#percent-of-variation-explained",
    "href": "L04-Regression.html#percent-of-variation-explained",
    "title": "4  Regression",
    "section": "4.3 Percent of Variation Explained",
    "text": "4.3 Percent of Variation Explained\nBecause of some mathematical quirks, \\(r^2\\), the squared value of \\(r\\), can be interpreted as:\n\nThe percent of variation in \\(y\\) that can be “explained” by the linear model.7\n\nThe value of \\(r^2\\) can be calculated as: \\[\nR^2 = r^2 = \\frac{\\text{Variance of the predicted }y\\text{-values}}{\\text{Variance of the observed }y\\text{-values}}\n\\]\nI’ll explain this in steps. The first plot below shows just the values in \\(y\\). This collection of values has a own mean and variance.\nThe second plot shows the change in variance that the line “explains”. Instead of deviations above and below the mean, the variance can now be characterized as the deviations above and below the regression line. This variance will always be lower than the variance of \\(y\\) without incorporating \\(x\\)8.\nThe third plot shows where this variance went. The line itself has variance; there is deviation in the line above and below the mean of \\(y\\). This is the variance that gets explained by incorporating \\(x\\)! If you consider one of the points in \\(y\\), say \\(y_1\\), the distance between \\(y_1\\) and \\(\\bar y\\) can be split up into the difference between \\(\\bar y\\) and the regression line plus the distance between the regression line and \\(y_1\\).\n\n\n\n\n\nThe rest of the variance is left unexplianed. No regression will ever be perfect unless we are studing a very very simple .\nTo see this a different way, consider what happens when \\(r = 0\\)9. This will just be a horizontal line, and none of the variance is explained. On the other had, if \\(r = 1\\) then all of the points will be exactly on the line. All of the variance in \\(y\\) has been explained by the regression against \\(x\\) - there’s no variance left to be explained!10\nNotice how the R output includes"
  },
  {
    "objectID": "L04-Regression.html#extensions-and-cautions",
    "href": "L04-Regression.html#extensions-and-cautions",
    "title": "4  Regression",
    "section": "4.4 Extensions and Cautions",
    "text": "4.4 Extensions and Cautions\n\nPrediction\nFor a new \\(x\\) value, \\[y = a + bx\\] is the predicted value of \\(y\\). That is, if we have an \\(x\\) value, we can plug it into the equation and find out what value of \\(y\\) we would expect.\nNote: There is still variance around this prediction! Our “expected” value will never be exactly equal to the truth - The value of \\(y\\) at a given value of \\(x\\) follows a normal distribution11, and the probability of a single point is 0!\n\n\nExtrapolation\nExtrapolation is what happens when prediction goes wrong. In particular, it’s what happens when we try to make a prediction at an \\(x\\) value where we don’t have any data. Usually this means we’re predicting an \\(x\\) value far above or far below the range of our data, but it can also happen if there’s a gap in the middle of our data.\nIn the plot below, the black dots are the original data, and we’re trying to predict a new value at \\(x = 25\\). The red line is the true model that I generated the data from. The black line represents a linear model. This model fits the original data quite well12, but predictions are completely inappropriate for values outside the data.\n\n\n\n\n\n\n\nLurking Variables\nThe black line in the plot below represents a regression where all of the data was lumped together. As we can see, this line does not seem to fit the data well. There is a hidden relationship in the the data - the green points and the red points should be considered separately13.\n\n\n\n\n\nA more serious consequence of a lurking variable has shown up before in the Palmer penguins data. In that example, the lurking variable actually reversed the correlation - if we lumped the groups together we got a negative correlation (and therefore negative slope), but if we looked at the groups individually we got positive associations in all of the groups! This is called Simpson’s Paradox, and basically means that we have to be very careful about interpreting correlations!"
  },
  {
    "objectID": "L04-Regression.html#footnotes",
    "href": "L04-Regression.html#footnotes",
    "title": "4  Regression",
    "section": "",
    "text": "Very few things are actually linear, but lines are fantastic approximations to many things.↩︎\nWe assume that \\(x\\) is fixed, but \\(y\\) has random noise. In other words, \\(x\\) is not a random variable but \\(y\\) is.↩︎\nBecause we think it makes us fancy. Note that the Baldi & Moore textbook uses \\(a\\), \\(b\\), and \\(e\\) for everything.↩︎\nActually, the word comes from “regressing to the mean”, which comes from how children are closer to average height than their parents - they go back toward the mean. This is not important.↩︎\nAlso, because the calculus works out so much better.↩︎\nThere are other ways to estimate these parameters, but they’re outside the scope of this course. All regression lines that you see in the textbook and the notes will be least squares regression lines.↩︎\nUsually \\(r^2\\) is labelled \\(R^2\\) for historical reasons. Capitalization matters in math; it’s just coincidence that both lower case and upper case mean the same thing here.↩︎\nExcept when \\(r=0\\), can you explain why?↩︎\nTherefore the slope will also be 0.↩︎\nStatistics is still just the study of variance.↩︎\nOur prediction is just us guessing the mean value of \\(y\\) at different values of \\(x\\).↩︎\nEven though it’s not the true relationship, it’s a reasonable approximation.↩︎\nPossibly as a blocking variable.↩︎"
  },
  {
    "objectID": "L05-Probability.html#defining-probability-with-dice",
    "href": "L05-Probability.html#defining-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.1 Defining Probability with Dice",
    "text": "5.1 Defining Probability with Dice\nI find that the easiest way is to build this up by examples. Let’s start with rolling a dice.3 Let’s say you rolled the dice, and you got a 3. This is called a simple event. The collection of all simple events is called the Sample Space, which in this case is \\(\\mathcal S\\)={1,2,3,4,5,6}.\nNow, suppose that you’re about to roll a dice. You might be curious about whether it’s a 1, 2, 3, 4, 5, or a 6, but you might only care about the event that the outcome is even. Since there are multiple simple events that make up this event, it is called a compound event.\nThere’s no way for you to know what’s going to happen, but you know all of the possibilities and you know how likely they all are.4 This is called a Probability Model: The sample space along with the probability of all of the events.\nI said “the probability of all events”, but this is more complicated than it may seem and requires some explanation. For something like rolling a dice, you only need to know the probability of each simple event. Compound events, like the probability that the outcome is even, can be determined from these simple events.\nSuppose you’re playing a game where, if the outcome of the dice is less than a certain cutoff value you get to roll again (e.g., your character has a special ability that allows re-rolling of dice, but the re-roll condition depends on the situation). You know the probability of all of the simple events, but you need to know the cutoff value to actually compute any probabilities. Without the cutoff value, you cannot define the probability model.\nFor a dice, the probability model is simply: Each number has a probability of 1 in 6. But what does this mean? There are two perspectives on what a “probability” is: The Frequentist approach and the Bayesian approach. In this class we’re only going to learn the Frequentist definition of probability, but if you’re interested in learning more I’m happy to talk.5\nProbability (Frequentist Definition): The long run frequency of observing an event. In other words, it’s the number of times an event is observed divided by the number of trials after doing a near-infinite number of trials.\nFor the dice, if we rolled the dice 60 times, we would expect 10 of those rolls to be a 1, 10 of them to be a 2, etc. Due to randomness, we won’t get exactly that, but this is what we would expect. If we rolled 600 dice, we would expect 100 to be 1, etc. As we roll more dice, we get closer to the proportion of 1/6. The plot below this demonstrates this - it is the number of times that a dice was 1 divided by the number of trials, with the number of trials being increased. Notice how it takes a while for the “empirical” probability to reach the theoretical probably; as the number of trials approaches infinity, the proportion of rolls that showed a 1 will approach 1/6."
  },
  {
    "objectID": "L05-Probability.html#calculating-probability-with-dice",
    "href": "L05-Probability.html#calculating-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.2 Calculating Probability with Dice",
    "text": "5.2 Calculating Probability with Dice\nFirst, let’s introduce some notation. I will use P(x) to mean “The probability of x”. In some cases, the context will be clear, such as:\n\n“The probability of rolling a 1” = P(1)\n“The probability of rolling an even number” = P(even)\n“The probability of not rolling a 1” = 1 - P(1)6\n\nFor this section, we’ll assume that P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=1/6.7\n\n“Or”\nWhat’s the probability that we roll an even number? The even numbers are 2, 4, and 6, so what we’re really asking is “What’s the probability that we roll a 2, 4, or a 6?” In this case, the probability is P(2 or 4 or 6) = P(2)+P(4)+P(6) = 1/6 + 1/6 + 1/6 = 3/6 = 0.5.\nWe also could have figured out this probability by noting that half of the values are even, so a probability of 0.5 makes sense. It’s a good thing when our intuition matches our answer, as we’ll see next.\nLet’s consider the probability that the dice is even^ (which we will denote P(even)) or it’s strictly larger than 3 (denoted P(&gt;3)). This means the dice is either 2, 4, or 6, or it’s 4, 5, 6. Since there are 4 different numbers (2, 4, 5, and 6) that would match the criteria, the probability is 4/6. Let’s use our “or” rules to verify this!\nThe probability that the dice is even is 1/2. The probabilty that the dice is larger than 3 is also 1/2. So, obviously, P(even or &gt;3) = P(even) + P(&gt;3) = 1/2 + 1/2 = 1.\nWait.\nThat can’t be right.\nI think you may be able to see what went wrong. The P(even) = P(2) + P(4) + P(6), and P(&gt;3) = P(4) + P(5) + P(6). When we did P(even) + P(&gt;3), we added P(4) and P(6) twice! To get the right answer, we need to fix this. Since we added them twice, we must subtract them once. This brings us to…\n\n\nThe Addition Rule for “or”\nFor any two events A and B,8 the Addition Rule states:\n\\[\\begin{align}P(A\\; or\\; B) = P(A) + P(B) - P(A\\; and\\; B).\\end{align}\\]\nFirst, note that the probability of both events is P(Even and &gt;3) = P(4) + P(6), since 4 and 6 are both even and larger than 3.\n\\[\\begin{align*}\nP(Even\\; or\\; &gt;3) & = P(Even) + P(&gt;3) - P(Even\\; and\\; &gt;3)\\\\\n& = [P(2) + P(4) + P(6)] + [P(4) + P(5) + P(6)] - [P(4) + P(6)]\\\\\n& =  P(2) + P(4) + P(5) + P(6)\\\\\n& = 1/6 + 1/6 + 1/6 + 1/6\\\\\n& = 4/6\n\\end{align*}\\] \n\n\n“and”: Part 1\nThe word “and” came up in the addition rule, and so I should give a good definition of “and”. When we talk about events A and B, P(A and B) refers to the probability that they both happen together. It’s most helpful to see this as a Venn diagram:\n\nP(A) is the area of the circle labelled A, P(B) is the area of the circle labelled B, and P(A and B) is the area of the overlap between these two circles. P(A or B) is the total shaded area, including the yellow-green, green, and dark green.\nYou can see the Addition Rule at work here. If you add the area of A (which includes P(A and B)) to the area of B (which also includes P(A and B)), you’ve added P(A and B) twice!\nThere are two formulas for P(A and B). The first one is found by rearranging the formula for P(A or B):\n\\[\\begin{align*}\n\\small P(A\\; or\\; B) &\\small = P(A) + P(B) - P(A \\;and\\; B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(A) + P(B) - P(A \\;or\\; B)\n\\end{align*}\\]\nWhen in doubt, just remember P(A_B) = P(A) + P(B) - P(A_B), then put “or” in one blank and “and” in the other.\nThis formula won’t always get you to the solution, though. There will be many times where neither “and” nor “or” will be obvious, and we’ll need to do some more work to get them. We have special formulas for “and”, so we’ll usually try to figure out the “and” and then use it to figure out the “or”9.\n\n\n“given”: Conditional Probabilities\nA condition is something that must happen before you can proceed. A conditional probability is a probability that requires something else to happen, and usually involves a more complicated setup.\nLet’s look at another scenario. Let’s say I told you that the number on the dice was larger than 3. What’s the probability that the number on the dice is a 4? Intuiutively, it’s 1/3, since there are 3 possible numbers. Our notation fails us here, P(4) denotes the probability that the dice is a 4, which we already determined was 1/6. We can’t use P(4) for two things, so we need to add some notation.\nIn this case, the solution is to use a vertical bar, “|”, which is pronounced “given”. We write “P(dice is 4 | dice is greater than 3) = 1/3”,10 which is read as “The probability that the dice is 4, given that the dice is larger than 3.”\nA very important thing happened here: when we used a conditional probability (“given that”), we restricted the sample space. When we “condition” on an event, it means that we’re only looking at cases where that event happened. “The probability that the dice is 4, given that the dice is larger than 3” is another way of saying that we’re only considering events where the dice roll is greater than 3; we don’t care about 1, 2, or 3.\nWe defined “probability” as the total number of events divided by the total number of trials. For conditional probabilities, this means that we’re only looking at some of the trials.\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nThis formula is incorrect: “The number of ways that a dice can be 4” depends on the condition. For instance, the number of ways that a dice can be 2 is 0 since we’re told it’s larger than 3. We are actually looking at the number of ways that the dice can be both 4 and greater than 3. Let’s incorporate this information:\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;\\;and\\;&gt;3}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nFor any two events A and B, conditional probabilities are defined as follows:11\n\\[\\begin{align*}\n\\small P(A | B) = \\frac{P(A\\;and\\; B)}{P(B)}\n\\end{align*}\\]\nThe equation above is a definition. It’s not the result of something else, it’s the way we define conditional probability. Rearranging it, though, gives us an important result.\n\n\n“and” Part 2: The Multiplication Rule\nFor any two events A and B, the Multiplication Rule states:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) = P(A|B)P(B)\n\\end{align*}\\]\nNote that P(B|A) = P(A and B)/P(A), so the multiplication rule can be extended:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) &\\small = P(A|B)P(B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(B|A)P(A)\n\\end{align*}\\]\nIn other words, you can write it either way as long as the event that comes after the “|” also appears on it’s own.\nLet’s use this to answer the following question: What’s the probability that a dice is larger than 3 and even? By intuition, this should be 2/6 since there are two cases where both are true, but let’s verify with math!\nFirst, recall that P(&gt;3) and P(even) are both 1/2.\nP(&gt;3 | even) means that we’re look at the number of dice rolls that are larger than 3, but we’re only considering even dice rolls. We have 3 total dice rolls that are even, and 2 of those are larger than 3, so this probability is 2/3. Using the multiplication rule, P(&gt;3 and even) = P(&gt;3 | even)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\nThe other way works out the same. Given that the roll is larger than 3, there are 2 even rolls, which means that P(even | &gt;3) = 2/3. P(&gt;3 and even) = P(even | &gt;3)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\n\n\nSpecial Cases: Independent or Disjoint\n\n\nDisjoint, a.k.a. Mutually Exclusive\nDisjoint events, also called mutually exclusive events, are events that cannot occur together. For example, the event that you roll a 4 and it’s also a 3. This simply does not work, so the probability is 0.\nMore formally, A and B are disjoint if P(A and B) = 0.\n\n\nThe Addition Rule for Disjoint Events\nIf A and B are disjoint, then P(A or B) = P(A) + P(B).12\nThis is actually why we were able to say P(even) = P(2 or 4 or 6) = P(2) + P(4) + P(6) = 3/6: the events “2”, “4”, and “6” are disjoint.\n\n\nIndependent\nTwo events are independent if the knowledge of one event tells you nothing about the other.13 For instance, if I flip two different coins and tell you that the first one was Heads, you still only have a 50/50 chance of guessing the second one.\nNotice the phrasing in the previous sentence: “If I tell you that the first one is heads…” That is, I’m restricting the sample space. Independence is all about conditional probabilities!\nFormally, A and B are independent if P(A | B) = P(A).\nThis adds further insight into conditional probabilities: P(A|B) is how likely A is, given that you know B happened. Knowledge of B changes your guess of the likelihood of A. If it doesn’t change your guess, then they are independent.14\nThe following app demonstrates this concept:\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/indep\")\n\nAnother lesson to take from the app above: Independence doesn’t look special. You can’t just tell that things are independent by looking at them.\n\n\nThe Multiplication Rule for Independent Events\nAny time I see a conditional probability, I immediately write down the formula. For dependence, we are saying:\nP(A|B) = P(A and B)/P(B)\nwhich is the same as\nP(A and B) = P(A|B)P(B)15\nIf two events are independent, then P(A|B)=P(A), therefore:\nP(A and B) \\(\\stackrel{indep}{=}\\) P(A)*P(B)16\nGet this tattood backwards on your forehead so you see it every time you look at yourself in the mirror: P(A and B) is ONLY equal to P(A)*P(B) when A and B are independent!!! Some textbooks start with this rule then move to the general rule, but far too many students start using P(A and B) = P(A)P(B) as if it’s always true. My entire thesis is based on whether you can say two things are independent, so it’s kind of a sore spot for me. DO NOT MIX THIS UP.\nFor example, are the events “even” and “&gt;3” independent? If you know that the dice roll is &gt;3, then there’s a 2/3 chance that it’s even. That is, P(even|&gt;3) = 2/3 \\(\\ne\\) 1/2 = P(even), so it’s not independent.\nAlternatively, we can calcuate P(even and &gt;3) = 2/3, but P(even)*P(&gt;3) = (1/2)*(1/2) = 1/4. Since 2/3 \\(\\ne\\) 1/4, these events are not independent.\n\n\nDisjoint means Dependent\nIndependence can be defined as “if you know that one event happened, you have no knowledge of the other event.” Disjoint can be defined as “if you know one event happened, you know for sure that the other one did not happen.” If two events are disjoint, they must be dependent. In fact, knowledge of one event means that you for sure know about the other - the exact opposite of independence!\n… except when one event is impossible. For instance, P(even and 7) = 0 since there are no dice rolls that are both even and 7, but this is also equal to P(even)*P(7) = 0 since there are no dice rolls that are 7."
  },
  {
    "objectID": "L05-Probability.html#word-problems",
    "href": "L05-Probability.html#word-problems",
    "title": "5  Probability Background",
    "section": "5.3 Word Problems",
    "text": "5.3 Word Problems\nQuestion 10.6 from the textbook:17\nThe National Survey on Drug Use and Health reports that 18.1% of all adults in the United States had a mental illness in 2014. Among adults with a substance use disorder, 39.1% had a mental illness. By comparison, only 16.2% of adults without a substance use disorder had a mental illness. The report also states that 3.3% of American adults had both a mental illness and a substance use disorder. Use the notation MI and SUD for mental illness and substance abuse disorder, respectively.\n\nExpress the four percents cited here as probabilities for a randomly selected American adult. Use proper probability notation.\nObtain the probability P(SUD|MI). Write a sentence reporting this probability in context.\n\nSolutions:\n\nThere are a couple of probabilities:\n\n“18.1% of all adults in the United States had a mental illness”: P(MI) = 18.1\n“Among adults with a substance use disorder, 39.1% had a mental illness.” The part that says “among adults with SUD” means that we’re only looking at people with SUD; we’re restricting the sample space. This is a condition, so our answer must be P(_ | SUD) = __. The blanks can be filled in as P(MI|SUD) = 0.391.\n“16.2% of adults without a substance use disorder had a mental illness.” The part that says “adults without a SUD” is also restricting the sample space, so our probability statement will be P(__ | no SUD) = __. The blanks are filled in as P(MI | no SUD) = 0.162.18\n“3.3% of American adults had both a MI and a SUD”. This clearly states and, so we are looking at P(MI and SUD) = 0.033\n\n\nPart b. is going to take a few steps. Let’s write down all the formulas that might help. Firstly. there’s no “or”, so that probably won’t do it.\n\nWant: P(SUD|MI)\n\nP(SUD|MI) = P(SUD and MI)/P(MI), so we need P(SUD and MI) and P(MI).\n\nHave:\n\nP(MI) = 0.181\nP(MI | SUD) = 0.391\nP(MI | no SUD) = 0.162\nP(MI and SUD) = 0.033\n\n\nBoth P(SUD and MI) and P(MI) are given in the question, so our answer is simply:\n\nP(SUD | MI) = P(SUD and MI)/P(MI) = 0.033/0.181 = 0.1823\n\nTherefore 18.23% of people with mental illness have substance abuse disorder.\nCompare this value to P(MI | SUD) = 0.391. In general, there is no easy relationship between P(A | B) and P(B | A). If you know what P(A | B) is, you can’t really guess at what P(B | A) is; you need a lot more information!"
  },
  {
    "objectID": "L05-Probability.html#two-way-tables",
    "href": "L05-Probability.html#two-way-tables",
    "title": "5  Probability Background",
    "section": "5.4 Two-Way Tables",
    "text": "5.4 Two-Way Tables\nI rigorously collected the following data19 on programming language usage for different disciplines using the most appropriate sampling methods.\n\n\n\n\nStats\nMath\nComp Sci\nTotal\n\n\n\n\nR\n90\n30\n40\n160\n\n\nPython\n10\n60\n100\n170\n\n\nMatLab\n15\n60\n15\n90\n\n\nJulia\n10\n10\n1\n21\n\n\nTotal\n125\n160\n156\n431\n\n\n\nFrom this table, we can calculate marginal and conditional probabilities.\nMarginal probabilities are calculated from the margins, which means that we ignore one of the variables. For example, P(Math) = 160/431 and P(Julia) = 21/431. Both of these proportions are based on the margins - they don’t take the other variable into account.\nConditional probabilities are the same idea as we saw earlier. Again, we are restricting our sample space by conditioning on another variable. For example, P(R | Stats) = 90/125, whereas P(Stats | R) = 90/160. The conditioning event determines which row/column we use. When we condition on Stats, we only look at the column labelled stats - we do not consider any of the other numbers. This is why P(R | Stats) has a numerator of 125, rather than 431.\nYou should be familiar with the following calculations:\n\nP(Stats) = 125/431\nP(Stats | Julia) = 10/21\nP(Matlab | Comp Sci) = ???20\nP(Stats and Julia) = 10/431\nP(Matlab and Stats) = 15/431\nP(Stats or Julia) = P(Stats) + P(Julia) - P(Stats and Julia) = 136/431\nP(Matlab or Stats) = 200\nP(Stats or R) = ???\nP(Stats or Math) = ??\n\nTwo-way tables can also be created in R using the table() function:\n\ndata(mtcars) # It's a very useful dataset\n\ncbind(mtcars$am, mtcars$cyl) # cbind BINDs Columns together\n\n      [,1] [,2]\n [1,]    1    6\n [2,]    1    6\n [3,]    1    4\n [4,]    0    6\n [5,]    0    8\n [6,]    0    6\n [7,]    0    8\n [8,]    0    4\n [9,]    0    4\n[10,]    0    6\n[11,]    0    6\n[12,]    0    8\n[13,]    0    8\n[14,]    0    8\n[15,]    0    8\n[16,]    0    8\n[17,]    0    8\n[18,]    1    4\n[19,]    1    4\n[20,]    1    4\n[21,]    0    4\n[22,]    0    8\n[23,]    0    8\n[24,]    0    8\n[25,]    0    8\n[26,]    1    4\n[27,]    1    4\n[28,]    1    4\n[29,]    1    8\n[30,]    1    6\n[31,]    1    8\n[32,]    1    4\n\ntable(mtcars$am, mtcars$cyl)\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nThe table above is telling us that there were 3 cars that were automatic (0) and had 4 cylinders.\n\n## Note: TRUE == 1, so the sum of a logical vector is the number of TRUEs\n## The \"&\" operator only returns true if BOTH conditions are true, i.e.\n## if mtcars$cyl == 4 AND mtcars$am == 0\nsum(mtcars$cyl == 4 & mtcars$am == 0)\n\n[1] 3"
  },
  {
    "objectID": "L05-Probability.html#self-study-questions",
    "href": "L05-Probability.html#self-study-questions",
    "title": "5  Probability Background",
    "section": "5.5 Self-Study Questions",
    "text": "5.5 Self-Study Questions\n\nExplain why P(A) + P(not A) must be 1.\nIf P(A) = 0.2, P(B) = 0.35,\n\nand P(A or B) = 0.75, find P(A and B).\nand P(A and B) = 0.15, find P(A or B).\nexplain why P(A and B) can only be as large as 0.2.\nexplain why P(A or B) must be at least 0.35.\n\nFor a 6-sided dice, show that the events “even” and “odd” are not independent.\nFor a 6-sided dice, show that the events “even” and “&gt;4” are independent.\nConsider flipping one coin and rolling one dice.\n\nList out all possible events (e.g., H1 for heads and 1, T4 for tails and a 4 on the dice).\nBased on your sample space, argue that P(T1) = 1/12.\nAre the events “coin is tails” and “dice is 1” independent? Give an intuitive and a mathematical reason.\n\nConsider a loaded dice, where the probability of 1, 2, 3, 4, and 5 are all 1/8.\n\nExplain why P(6) must be 3/8.\nWhat is P(even)?\nAre the events “even” and “&lt;3” independent?\n\n\nSolutions to Two-Way Table exercises: 3. 15/156; 8. 195/431; 9. 185/431"
  },
  {
    "objectID": "L05-Probability.html#footnotes",
    "href": "L05-Probability.html#footnotes",
    "title": "5  Probability Background",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThe singular form “die” is dieing out; the dictionary lists “dice” as singular noun, and the singular “dice” is clearer for new English speakers.↩︎\nAssuming there’s nothing unusual about your dice.↩︎\nMost of my work uses the Bayesian definition.↩︎\nThis is called a complement.↩︎\nThe sum of all probabilities must be 1.↩︎\nFor example, A = “Even”, B = “&gt;3”.↩︎\nThis lecture has some of the weirdest sentences.↩︎\nP(4 | &gt;3) just looks too confusing, so I added some words.↩︎\nTo remember this, I like to imagine the vertical bar falling on the the B and pushing it into the denominator.↩︎\nNot an important point: This is a rule, not a result. The General Addition Rule is a result of this rule, not the other way around.↩︎\nThe opposite of independence is dependence.↩︎\nJust like in correlations, dependence does not imply causation!↩︎\nThis equation is always true.↩︎\nThe “\\(indep\\)” over the equals sign is there to specify that this is only true if events are independent.↩︎\nBaldi, B. and DS. Moore. 2018. The Practice of Statistics in the Life Sciences. 4th Edition, W.H. Freeman and Company.↩︎\nThis is a great place to mention: There’s absolutely no reason why P(A|B) + P(A| not B) should add to 1.↩︎\nSource: I made it up.↩︎\nAnswer is at the end.↩︎"
  },
  {
    "objectID": "L07-Normal_Distributions.html#introduction",
    "href": "L07-Normal_Distributions.html#introduction",
    "title": "6  The Normal Distributions",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn this lecture we are looking at continuous distributions. Continuous distributions have an odd quirk. If a variable has a continuous distribution, then \\(P(X = x) = 0\\). That is, the probability of any specific value is infinitely small.\nThink of it this way: suppose that human heights go from 54 cm to 272 cm. For now, suppose all of these heights are equally likely. If we record heights to the nearest centimeter, there are 219 possible heights, so the probability that you are one of those heights is 1/219. If we round to the nearest mm, there are 21,900 different heights. As we get a more and more accurate measuring instrument, the probability of any given height goes to 0. It’s not that these heights are impossible, it’s that you’re probably not going to ever guess my exact height when we measure it with infinite accuracy.\nSo what do we do? How could we possibly calculate probabilities? Well, we measure ranges! You can’t guess my height exactly, but we can talk about the probability that my height is between 170 and 180 cm, or even the probability that my height would be 170, assuming we round to the nearest centimeter.\n\nSome Facts about Distributions\nBefore we begin, the following properties are true of any distribution, regardless of whether they are discrete or continuous.\n\nAll probabilities must be between 0 and 1.\nAll probabilities together must make 1.\n\nFor discrete, adding them all should get you to 1.\nFor continuous, the area under the density curve must be 1.1\n\nIf two events are disjoint, you must be able to add their probabilities.\n\nIt’s weird, but we have to define this as a rule first before we can calculate probabilities.\n\n\nThe first point should be obvious, and you won’t ever need to check whether the third point is true.\nThe second point is the important one: The total probability for all events must be 1. For continuous distributions like the normal distribution, that means that the area under the curve is 1.2"
  },
  {
    "objectID": "L07-Normal_Distributions.html#the-normal-distribution",
    "href": "L07-Normal_Distributions.html#the-normal-distribution",
    "title": "6  The Normal Distributions",
    "section": "6.2 The Normal Distribution",
    "text": "6.2 The Normal Distribution\nThe normal distribution is a way to define the probability of something using a function, but the function is complicated.3 Instead, we’ll jump right into how to use it and let software deal with the function.\nIn the introduction, I used the example of people’s heights. I made the assumption that all heights were equally likely, but this is just a bonkers thing to say. Instead, some heights are more likely than other heights. Of course, this doesn’t mean that, say, 170 cm is very unlikely, but 175 is likely, then 176 is unlikely, then 177 is very unlikely, then 178 is suddenly really likely again; most people have heights close to the average and heights further from the average are less likely. This is exactly what the normal distribution is for! Here’s what the normal distribution looks like:\n\nmu &lt;- 162.3\nsig &lt;- 7.11\nxseq &lt;- seq(mu-3*sig, mu + 3*sig, length.out = 300)\nyseq &lt;- dnorm(x = xseq, mean = mu, sd = sig) \n\nplot(xseq, yseq, type = \"l\",\n  main = \"Heights of Canadian Women\",\n  xlab = \"Height (cm)\", ylab = \"Prob. Density\")\n\n\n\n\nThe plot above has the highest point occurs at exactly 162.3, which is the best number I could find for the actual average height of Canadian women. This is denoted \\(\\mu\\). The width of the curve is a little trickier - how did I choose to make it go from 145 to 180? I could easily have stretched it out or squeezed it inwards in both directions. The width is defined by the standard deviation, which is denoted \\(\\sigma\\). Because of the way the normal distribution is defined, there’s nothing else we can change about it - knowing \\(\\mu\\) and \\(\\sigma\\) are enough to draw the entire curve.\n\n## Requires the \"shiny\" library\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/normShape\")\n\nIf we have a variable \\(X\\) that follows a normal distribution, we use the notation \\(X \\sim N(\\mu, \\sigma)\\).\n\n\n\n\n\n\nWarning\n\n\n\nSome textbooks use the notation \\(X \\sim N(\\mu, \\sigma^2)\\), i.e. they use \\(\\sigma^2\\) rather than \\(\\sigma\\). It’s like driving on the left or the right side of the road - both are fine, but we have to choose one and stick with it.\n\n\nThe idea that “most things are close to the center, and fewer things further away” can be very powerful. This applies to:\n\nHuman heights\nIncome for a given job position\nChange in stock price from day to day\n\nOn average the change is 0, but it does change. Small changes are much more likely than large ones, but large ones do happen.\nObviously, extreme events happen sometimes, and major changes can happen.\n\nIQ scores\nBirth weight\nHow much the prediction of a model differs from the truth"
  },
  {
    "objectID": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "href": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "title": "6  The Normal Distributions",
    "section": "6.3 Calculating Normal Probabilities - Part 1",
    "text": "6.3 Calculating Normal Probabilities - Part 1\nThe height and width of the normal distribution are determined by the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)), and only the mean and standard deviation. The mean just moves the curve left and right, the standard deviation squeezes or stretches it.\nTo highlight this, we introduce something called the Empirical Rule, a.k.a. the 68-95-99.5 Rule. No matter what the mean of the distribution is, 68% of the probability is within 1 standard deviation of the mean. To say this another way, let’s extend our notation slightly. If \\(X\\sim N(\\mu,\\sigma)\\), we can say that:\n\\[\\begin{align*}\nP(\\mu - \\sigma \\le X \\le \\mu + \\sigma) \\approx 0.68\n\\end{align*}\\]\nTo phrase this in another way, if we were to draw random numbers from the normal distribution, 68% of them would be between 1sd below the mean and 1sd above the mean.\n\nset.seed(-4) # Ensure the same random numbers every time\n\n## generate 10000 random N(0,1) values\nx &lt;- rnorm(n = 10000, mean = 0, sd = 1) \n\n## You won't need to know how to write this code:\nsum(x &gt; -1 & x &lt; 1) # x is larger than -1 AND less than 1\n\n[1] 6766\n\n\nSo out of 10,000 random numbers from a N(0,1) distribution, 6,766 (67.66%) of them were above -1 but below 1. If we change the mean and sd, we still get the same results:\n\n## Mean is 4, sd is 30, so mean - 1sd = 4 - 30\n## Change the mean and sd for yourself to see what happens!\nmu &lt;- 4\nsigma &lt;- 30\nx2 &lt;- rnorm(n = 10000, mean = mu, sd = sigma)\nsum(x2 &gt; (mu - sigma) & x2 &lt; (mu + sigma)) # Not exactly 68%, but approximate!\n\n[1] 6845\n\n\nAs you can guess from the name “68-95-99.7 Rule”, 68% being within one sd is only part of the story. The 95 refers to 95% being within 2sd of the mean, and the 99.7 refers to 99.7% being within 3sd of the mean.\n\nsum(x &gt; -2 & x &lt; 2) # within 2sd of the mean\n\n[1] 9523\n\nsum(x &gt; -3 & x &lt; 3) # within 3\n\n[1] 9970\n\n## Try this with x2 as well!\n\nSome variant of the following image appears in countless textbooks:\n\nAs a small side note, the image above uses the word “data”. By this, it means that if this is the population, then 68% of all the data that it were possible to collect would be within one standard deviation of the mean. As we saw in the simulated data above, this number is almost never going to be perfect.\n\nTrickier calculations\nIf 68% of the data is between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\), then there’s still 32% of the distribution outside this range. The normal distribution is symmetric, so this 32% gets split exactly in half and 16% of the distribution is below \\(\\mu - \\sigma\\), and 16% is above \\(\\mu + \\sigma\\).\nBased on this calculation, we can say that 84% of any normal distribution is below \\(\\mu + \\sigma\\), and 84% is above \\(\\mu - \\sigma\\). Before we move on, draw out some normal distributions to convince yourself that 97.5% of any normal distribution is less than \\(\\mu + 2\\sigma\\).4\nYou should try the following calculations yourself, all of which can be done with basic arithmetic and the 68-95-99.7 Rule:\n\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu-\\sigma\\).\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu+\\sigma\\).\nAbove \\(\\mu + 2\\sigma\\) and below \\(\\mu + 3\\sigma\\)\nAbove \\(\\mu - 3\\sigma\\) and below 0."
  },
  {
    "objectID": "L07-Normal_Distributions.html#the-standard-normal-distribution",
    "href": "L07-Normal_Distributions.html#the-standard-normal-distribution",
    "title": "6  The Normal Distributions",
    "section": "6.4 The Standard Normal Distribution",
    "text": "6.4 The Standard Normal Distribution\nWe use a special letter (Z, pronounced “zed” because we’re Canadian) to denote a standard normal distribution. In particular, \\(Z\\sim N(0, 1)\\) is a normal distribution with mean 0 and standard deviation 1. Many many many many textbooks have a table in the back of them that gives probabilities for the standard normal distribution, and they call them \\(Z\\) tables.\nAll normal distributions have the exact same shape. In order to change the mean and sd, we can simply re-write the numbers on the axes. If we want to shift the whole curve to the left by 2 units, we can re-label the numbers on the x axis. If we change the sd, the plot might get “taller” or “shorter”, but if we zoom in on the plot we can make it look exactly the same!5\n\nStandardizing a Normal Distribution\nBecause they all look the same, we might as well work with just one of them! Suppose \\(X\\sim N(\\mu,\\sigma)\\). If we shift the whole curve to the left, then the mean shifts as well and the mean is 0. In other words, \\(X-\\mu \\sim N(0,\\sigma)\\). Now that the mean is at 0, \\(\\mu + 1\\sigma\\) is simply \\(\\sigma\\), \\(\\mu-3\\sigma\\) is \\(-3\\sigma\\), and so on. If we divide all of the numbers by \\(\\sigma\\), then \\(\\sigma\\) is simply 1, \\(-3\\sigma\\) is simply -3, and so on. To formalize this, if \\(x\\sim N(\\mu,\\sigma)\\), then\n\\[\\begin{align*}\n\\frac{X-\\mu}{\\sigma} = Z \\sim N(0, 1)\n\\end{align*}\\]\nThis is called standardizing a normal distribution. The resultant value is called the z-score.\nFor example, suppose a woman is 155.19 cm tall. If the true mean height of Canadian women is 162.3 and the standard deviation is 7.11, then this particular woman is exactly one standard deviation below the mean. This is the z-score, a.k.a. the standardized value; this woman’s z-score is -1.\nNow consider a woman who is 161.22 cm tall. Her z-score would be -0.152,6 meaning that she is 0.152 standard deviations below the mean.\nLet’s return to the 155.19 cm tall woman. If you take a woman at random from the population, what is the probability that the randomly chosen woman be be shorter than 155.19 cm? Based on the 68-95-99.7 rule, 68% of women are within one standard deviation of the mean, which is a range from 155.19 to 169.41. Since 68% of the women are betwen these two numbers, 16% of them are shorter than 155.19 (it is also true that 16% are taller than 169.41, but this was not required for the question).\nNow, what’s the probability that a randomly chosen woman is, say, shorter than 160 cm? This doesn’t fit nicely in the empirical rule, so we need another way to calculate probabilities. However, it’s worth stopping and trying to make a guess! The empirical rule tells us that 16% of women are below 155.19 cm, and we also know that 50% of women are shorter than the average of 162.3 cm (since the normal distribution is symmetric), so we expect that the answer is somewhere between 16% and 50%, probably closer to 50% since 160 cm is closer to 162.3 cm than it is to 155.19 cm."
  },
  {
    "objectID": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "href": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "title": "6  The Normal Distributions",
    "section": "6.5 Calculating Normal Probabilities - Part 2",
    "text": "6.5 Calculating Normal Probabilities - Part 2\nIn general, we use the cumulative distribution function (CDF, or cdf) to calculate probabilities. As with the cumulative probability tables we saw in the probability lectures, the cumulative probability calculates the area to the left of a particular point.7 Questions about the normal distribution generally come in three flavours:\n\nFind \\(P(X \\le a)\\)\nFind \\(P(X \\ge b)\\)\nFind \\(P(c \\le X\\le d)\\)\n\nThe cdf is defined as \\(P(X\\le x)\\), which allows us to answer questions like 1. For the standard normal distribution, a table of Z probabilities can be found at the back of the textbook. I’ve added a file that demonstrates how to use the Z-table in the Lecture Materials. This is something that is crucial to know for closed-book tests since you will need to caclulate probabilities somehow, but we can’t let you have a computer to run R! Before moving on, read “Intro to Ztable.pdf”.\nIn that file, there are some practice problems. Below, you’ll find a selection of solutions using R. For your own practice, try and calculate them with the Z-table (with some good drawings) and verify your answer with R.\n\n## 1. Find the probability of a z-value less than 1.11.\npnorm(1.11)\n\n[1] 0.8665005\n\n## 2. Find the probability of a z-value greater than 1.11\n1 - pnorm(1.11)\n\n[1] 0.1334995\n\n## 3. Find the probability of a z-value greater than -2.01 but less than 1.\npnorm(1) - pnorm(-2.01)\n\n[1] 0.8191292\n\n## 4. Verify the empirical rule: 68-95-99.7\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\npnorm(2) - pnorm(-2)\n\n[1] 0.9544997\n\npnorm(3) - pnorm(-3)\n\n[1] 0.9973002\n\n\nFor questions like \\(P(X\\ge x)\\), we can simply use the fact that \\(P(X \\ge x) = 1 - P(X&lt;x)\\). Since this is a continuous distribution and \\(P(X = x)=0\\), we also know that \\(P(X\\le x) = P(X&lt;x)\\) and we can just use the cdf. The last one is a little bit trickier.\nTo calculate the probability that a randomly chosen value will be within a given range, there are a few steps. Let’s use the same example as the textbook: If \\(X\\sim N(-2, 1)\\) find \\(P(-2.5\\le X\\le -1)\\). If we want to use the cdf, we need to re-write this in terms of \\(P(X\\le x)\\).\nHere’s how we do it. If we only find \\(P(X\\le-1)\\), then we have taken too much of the distribution. Everything to the left of -2.5 was something that should not have been included. So why don’t we just remove it? By this logic, we find \\(P(-2.5\\le X \\le -1) = P(X\\le -1) - P(X \\le -2.5)\\). This is shown graphically below:\n\nReturning to the heights example, the probability of a randomly chosen woman being less than 160 cm can be calculated as: \\[\n\\frac{x - \\mu}{\\sigma} = \\frac{160 - 162.3}{7.11} = -0.323488045\n\\] We can now look up -0.323 on the normal table. Try that out, and verify that you get the same value here:\n\npnorm(-0.323488045)\n\n[1] 0.3731628\n\n\nNote that R will do the standardization for you if you ask it politely.\n\npnorm(q = 160, mean = 162.3, sd = 7.11)\n\n[1] 0.3731628\n\n\nI have created a shiny app that lets you explore these calculations8. Feel free to use this to answer the questions in this lecture, and then double check the answers with the z-table.\n\n## install.packages(\"shiny\") # Run this if you get an error about \"package not found\"\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\nExamples\n\n\nEx1: P(X &lt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value less than 0?\n\nSolution 1: Standardize and Z-table. I’ll split this up into steps:\n\nStandardize: \\((x-\\mu)/\\sigma = (0 - 4)/2 = -2\\).\nFind -2 on the Z table: -2=-2.00, so this will be in the row labelled -2.0 and the column labelled 0.00,9 which is 0.0228.\nConclude: 2.28% of the N(4,2) distribution is below 0.\n\nSolution 2: Empircal rule.\n\nBefore calculating a normal probability, try and estimate how many standard deviations away from the mean the value is. In this case, 0 is 2 standard deviations from 4. The 68-95-99.7 rule states that 95% of the distribution is outside the range from \\(\\mu - 2\\sigma\\) to \\(\\mu + 2\\sigma\\), so 5% is outside of this range. This means that 2.5% is on either side, which means that 2.5% is below 0.\n\n\n\nA short version of Solution 2: By the 68-95-99.7 Rule, 95% is between 0 and 8. Therefore, 2.5% must be less than 0.\nAs you can see, the 68-95-99.7 rule is approximate. However, I highly recommend doing many practice problems with it. On a multiple choice question, if you can figure out the answer with the Empirical Rule than you might be able to guess the correct answer much quicker. You won’t get the exact answer, but if there’s only one answer that’s close to your guess, then that’s probably it.10\nSolution 3: R.\n\n## Standardize:\npnorm((0 - 4)/2)\n\n[1] 0.02275013\n\n## Same answer, without standardizing:\npnorm(q = 0, mean = 4, sd = 2)\n\n[1] 0.02275013\n\n\n\nIf \\(X\\sim N(1234, 56)\\), what’s the probability of a number smaller than 1432.\n\nBefore we begin: What do we expect the number to be? The mean is 1234, which is smaller than 1432. Is it a little smaller, or is it a lot smaller? Compared to the standard deviation, it’s a lot smaller. By the empirical rule, the vast majority of the distribution is below \\(\\mu + 3\\sigma\\), which is approximately 1400.11 We should expect an answer close to 1, since the area under the normal distribution is 1.\nSolution 1: \\((x-\\mu)/\\sigma = (1432-1234)/56 = 3.54\\), which is not on the Z table. When this happens (and we don’t have access to technology), we simply say the answer is 1.12\nSolution 2: The value we’re interested in isn’t 1, 2, or 3 standard deviations from the mean, so the Empirical Rule doesn’t apply. However, we can guess that our probability will be close to 1 since it’s larger than 3 standard deviations away.\nSolution 3: R.\n\npnorm(1432, mean = 1234, sd = 56)\n\n[1] 0.9997967\n\n\nIdeally, you would only ever use intuition from the Empirical rule, or use R. The Z-table is super convenient for written, in-person exams. It’s also nice for situations where you don’t have a computer with R available.\n\n\nP(X &gt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value greater than 0?\n\nBefore we start: Use the empirical rule! 0 is 2sd below the mean, so the answer should be close to 97.5%\nWith the Z table: \\((x-\\mu)/\\sigma = -2\\), and we’ve already found this on the table as 0.0228. Since we’re looking at the right tail, our answer is 1 - 0.0228 = 0.9772.\nWith R:\n\n1 - pnorm(0, mean = 4, sd = 2)\n\n[1] 0.9772499\n\n\n\nSuppose \\(X\\sim N(23, 23)\\). What’s the probability of a value larger than 23?\n\nBefore we start: The normal distribution is perfectly symmetric, which we have learned means that the mean is equal to the median. The median marks the point where 50% of the distribution is smaller. So before doing any work, we know that the answer must be 50%\n\npnorm(23, 23, 23)\n\n[1] 0.5\n\n\n\n\nP(a &lt; X &lt; b)\n\nIf \\(X\\sim N(0, 1)\\), what’s the probability of a value between -1.52 and -0.5?\n\nSolution 1: We have a standard normal value, so we can look these values up directly. P(Z &lt; -1.52) = 0.064313 and P(Z &lt; -0.5) = 0.3085.14 We want the area between these two values. P(Z &lt; -0.5) contains everything from negative infinity to -0.5, but we only want values from -1.52 to -0.5. To fix this, we remove everything from negative infinity to -1.52. Our answer is 0.3085 - 0.0643 = 0.2442.\nSolution 2: R.\n\npnorm(-0.5) - pnorm(-1.52)\n\n[1] 0.2442821\n\n\nI have made a shiny app for you to visualize this:\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\n\\(X \\sim N(2,3)\\), find \\(P(-1 &lt; X &lt; 5)\\)\n\nBefore we begin: This is the empirical rule for 1sd. The answer is 68%.\nWith a Z table: We calculate the z-score individually, then subtract the probabilities in a way that makes sense.15 \\(P(X &lt; -1) = P((X-\\mu)/\\sigma &lt; (-1 - \\mu)/\\sigma) = P(Z &lt; (-1 - 2)/3) = P(Z &lt; -1) = 0.1587\\). Similarly, \\(P(X &lt; 5) = P(Z &lt; 1) = 0.8413\\). The answer is 0.8413 - 0.1587 = 0.6826, which is very close to what we got with the Empirical Rule.\nWith R:\n\npnorm(5, mean = 2, sd = 3) - pnorm(-1, mean = 2, sd = 3)\n\n[1] 0.6826895\n\n\n\n\nGoing Backwards\nWhat’s the first quartile of an N(2,3) distribution? It’s the point at which 25% of the distribution is smaller. In other words, P(X &lt; Q1) = 0.25. How do we find Q1?\nWe can look up 0.25 as a probability. That is, as a value in the body of the Z table. This will give us the corresponding z-score.16 Unfortunately, 0.25 isn’t in the table. The closest values are 0.2514 (which is a Z score of -0.67) and 0.2483 (Z score of -0.68). On a test situation, -0.67 and -0.68 would both be valid answers, as would -0.675.\nIn R, the “q” family of functions are the reverse lookup functions. That is, You tell them the probability, and they return the z-score.\n\nqnorm(0.25, mean = 0, sd = 1)\n\n[1] -0.6744898\n\n\nHowever, we’re not done yet! We found the quartile for a standard normal distribution. We have to go backwards in the standardization formula. In essence, we have found Z and we need to find X.\n\\[\\begin{align*}\n\\frac{x - \\mu}{\\sigma} = z \\Leftrightarrow x = z\\sigma + \\mu\n\\end{align*}\\]\nTo finish this question, we say that the first quartile of a N(2, 3) distribution is -0.67*3 + 2 = -0.01.17\nIn R:\n\nqnorm(0.25, mean = 2, sd = 3)\n\n[1] -0.02346925"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "href": "L07-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "title": "6  The Normal Distributions",
    "section": "6.6 Problems: Verifying the Empirical Rule",
    "text": "6.6 Problems: Verifying the Empirical Rule\n\n68-95-99.7"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-z-scores",
    "href": "L07-Normal_Distributions.html#problems-z-scores",
    "title": "6  The Normal Distributions",
    "section": "6.7 Problems: Z-scores",
    "text": "6.7 Problems: Z-scores\n\n\n\\(P(z \\le 2.25)\\)\n\\(P(z \\le -2.25)\\)\n\\(P(z \\ge 2.25)\\)\n\\(P(z \\ge -2.25)\\)\n\n\n\n\\(P(-2 \\le z \\le 2)\\)\n\n$P(Z ; and; Z )\n\n\\(P(2 \\le z \\le -2)\\)\n\\(P(0 \\le z \\le 2)\\)\n\\(P(-2 \\le z \\le 0)\\)\n\\(P(Z \\ge 2\\; or\\; Z \\le -2.5)\\)\n\n\n\n\\(P(Z \\le z) = 0.5\\)\n\\(P(Z \\ge z) = 0.4238\\)\n\nWhat is \\(z\\)?"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-standardizing",
    "href": "L07-Normal_Distributions.html#problems-standardizing",
    "title": "6  The Normal Distributions",
    "section": "6.8 Problems: Standardizing",
    "text": "6.8 Problems: Standardizing\n\nThe birthweights of cute widdle babies born at full-term is \\(N(3350, 440)\\).\n\nLow birthweight babies are those with a weight less than 2500. Probability of this?\nHigh birthweight is above 4200. Probability?\nProbability of either low or high?\n\n\nA paper claimed that their control group was normal with a mean of 7 headaches per month, and the treatment group had a mean of 3.\nThe paper later claims that there’s only a 10% chance of seeing fewer than 3 headaches in the control group.\nThe paper never provided the sd. What is it?"
  },
  {
    "objectID": "L07-Normal_Distributions.html#participation",
    "href": "L07-Normal_Distributions.html#participation",
    "title": "6  The Normal Distributions",
    "section": "6.9 Participation",
    "text": "6.9 Participation\n\n\n\\(P(Z &lt; 1.5)\\)\n\\(P(Z &gt; -1.5)\\)\n\\(P(Z &lt; 1.2 or Z &gt; 1.3)\\)\n\\(X\\sim N(0,2)\\), find \\(P(X &lt; 2)\\)\n\\(X\\sim N(\\mu, 5)\\) and \\(P(X \\le 2) = 0.25\\), find \\(\\mu\\)\n\\(X \\sim N(2, 4)\\). Find the IQR."
  },
  {
    "objectID": "L07-Normal_Distributions.html#summary",
    "href": "L07-Normal_Distributions.html#summary",
    "title": "6  The Normal Distributions",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\nMost values are close to the mean, with fewer values as you get further away.\nThe mean and sd are sufficient to draw the whole curve.\nProbabilities are areas. The area of a single point is 0.18\n68% is within one sd of the mean, 95% within 2 sd, and 99.7% within 3 sd\n\n\\(P(\\mu - 1\\sigma \\le X \\le \\mu + 1\\sigma) \\approx 0.68\\).\n\\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma) \\approx 0.95\\).\n\\(P(\\mu - 3\\sigma \\le X \\le \\mu + 3\\sigma) \\approx 0.997\\).\n\nFor standard normal, the values on the x axis are z-score.\nThe cdf, P(X &lt;= x), is used to calculate areas.\n\nThe table can be found in the back of the textbook for standard normal. To standardize, use the formula \\((x-\\mu)/\\sigma\\).\npnorm(x, mean = 0, sd = 1) gives the standard normal cdf. If mean and sd are not specified, pnorm() assumes you want standard normal.\n\n\\(P(a \\le X \\le b) = P(X \\le b) - P(X \\le a)\\)\n\nEmpirical rule: pnorm(1) - pnorm(-1); pnorm(2) - pnorm(-2); …\n\nYou need a lot of practice with these kinds of problems. Do not check the answers prematurely.\n*norm functions:\n\nrnorm(n, mean, sd) generates random numbers\ndnorm(x, mean, sd) gives the height of the curve at the point x. This is not a probability.\npnorm(q, mean, sd) = \\(P(X \\le q)\\).\nqnorm(p, mean, sd) finds \\(q\\) such that \\(P(X \\le q) = p\\).\n\nIt is the backwards version (inverse function) of pnorm().\npnorm(qnorm(0.5)) returns 0.5, qnorm(pnorm(2)) returns 2."
  },
  {
    "objectID": "L07-Normal_Distributions.html#self-study-questions",
    "href": "L07-Normal_Distributions.html#self-study-questions",
    "title": "6  The Normal Distributions",
    "section": "6.11 Self-Study Questions",
    "text": "6.11 Self-Study Questions\n\nFor each of the probability statements, draw the normal distribution and add shading for the probability. For example, P(Z &gt; 1) should be a normal distribution with everything under the curve and larger than 1 shaded in. This is a very good way to help internalize the fact that all probabilities are areas.\nIn P(Z &lt; 1.32) = 0.9066, what do 1.32 and 0.9066 represent? Where are they on the Z table. If I were to give you one and not the other, could you find the missing number?\nWrite down all of the probability statements on a separate piece of paper. Solve them without looking at these notes. More practice, more better.\nPicture two normal distributions: one looks taller, and one looks wider. Which one has the larger standard deviation?\nExplain why the standard deviation does not affect the shape of the normal distribution. Now, explain why it does affect the shape.19"
  },
  {
    "objectID": "L07-Normal_Distributions.html#more-questions",
    "href": "L07-Normal_Distributions.html#more-questions",
    "title": "6  The Normal Distributions",
    "section": "6.12 More Questions",
    "text": "6.12 More Questions\nIf you have not calculated at least 50 or 60 different normal probabilities by the midterm, you have probably not done enough practice.\nFor each of these questions, start by trying to use the empirical rule, then use the Z table, then confirm your answer with R. Answers with R are shown below, but you should only check these once you’re confident with your own answer.20\n\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 1?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 2?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 3?\n\\(X \\sim N(-2, 500)\\), find the 75% quantile (aka Q3).\n\\(X \\sim N(3.14, 15.9)\\), what proporion of values are between 2.71 and 8.28?\nSuppose 25% of a normal distribution is below 0, and the mean of this distribution is 1. What’s the standard deviation?21\nWhat to Expect claims that the average baby weighs about 7.5 lbs, with a “normal”22 range of 5.8 to 10 lbs. If the “normal” range is defined as the middle 95%, what is the standard deviation of birth weights?\nYou’re asked to estimate the number of M&M’s in family-sized bags. You’re pretty sure that they are normally distributed and you think the mean is 600. How do you go about guessing the sd? One way is to say that you think it’s “unlikely” that there are more than 650 M&Ms in any given bag.23\n\nIf “unlikely” = 10%, that is, only 10% of the bags have more than 650 M&M’s, what is the sd?\nIf “unlikely” = 5%, what is the sd?\n\nIn the population of Canadian women, what’s the probability that a randomly selected woman is further than 1.7 standard deviations from the mean?\nThere’s a peculiar model that applies to certain kinds of data. If you have \\(\\mu = \\sigma\\), then the normal distribution has certain nice properties.24 Suppose \\(X\\sim N(\\theta, \\theta)\\), where \\(\\theta\\) is just a stand-in for the mean and variance. If \\(P(X &lt; 8) = 0.2\\), what is \\(\\theta\\)?25\n\nI’m going to say it again before you check the answers: Pre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge. You should spend time struggling to convince yourself that you did it right. On an exam, you won’t have the answers so you’ll feel that struggle. Practice the exam struggle now, then you’ll be more confident in your answer on exams.\nHave you ever had that feeling that you knew the material because you could do all of the practice problems, but when you get the exam you forgot everything? That’s because you checked the answers before struggling. You taught yourself to anticipate the answers of those particular questions, rather than teaching yourself the material. The struggling is where you learn. It’s the same as exercise: no pain no gain.\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Questions 1, 2, and 3\n## ~~~~~~~~~~~~~~~~~~~~~~~\n1 - pnorm(c(1,2,3), mean = 0, sd = 2)\n\n[1] 0.3085375 0.1586553 0.0668072\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q4\n## ~~~~~~~~~~~~~~~~~~~~~~~\nqnorm(0.75, mean = -2, sd = 500)\n\n[1] 335.2449\n\nqnorm(0.75)*500 - 2 # Alternative, using standard normal\n\n[1] 335.2449\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q5.\n## ~~~~~~~~~~~~~~~~~~~~~~~\npnorm(8.28, 3.14, 15.9) - pnorm(3.14, 3.14, 15.9)\n\n[1] 0.1267548\n\n## Alternative version, with standard normal\na &lt;- (3.14 - 3.14)/15.9\nb &lt;- (8.28 - 3.14)/15.9\npnorm(b) - pnorm(a) # P(a &lt; z &lt; b) = P(Z &lt; b) - P(Z &lt; a)\n\n[1] 0.1267548\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q6: x = z*sigma + mu =&gt; sigma = (x-mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(0 - 1)/qnorm(0.25)\n\n[1] 1.482602\n\n## Verify that 0 is the first quartile\nqnorm(0.25, mean = 1, sd = (0 - 1)/qnorm(0.25)) # Good!\n\n[1] 0\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q7: empirical rule: 5.8 = mu - 2*sigma, so sigma = (7.5 - 5.8)/2\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(7.5 - 5.8)/2\n\n[1] 0.85\n\n## However, if 10 = mu + 2*Sigma,\n(10 - 7.5)/2\n\n[1] 1.25\n\n## The normal distribution doesn't work because this is a *skewed distribution*\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q8.a) sigma = (x - mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(650 - 600)/qnorm(0.9)\n\n[1] 39.01521\n\n## Verify:\npnorm(650, 600, 39.01521)\n\n[1] 0.9\n\n## Q8b:\n(650 - 600)/qnorm(0.95)\n\n[1] 30.39784\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q9: The \"Canadian Women\" part is irrelevant.\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## The area WITHIN the range is:\npnorm(1.7) - pnorm(-1.7)\n\n[1] 0.9108691\n\n## So the area outside this range is:\n1 - (pnorm(1.7) - pnorm(-1.7))\n\n[1] 0.08913093\n\n## Why is the \"Canadian Women\" part irrelevant?\n## The lower bound will be mu - 1.7*sd = 150.213. When we\n## standardize this, we get z = (x-mu)/sd = 1.7, so we'd use\n## 1.7 in the standard normal distribution\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q10\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## P(X &lt; 8) = 0.2, so let z = qnorm(0.2)\n## z = (x - mu)/sigma = (8 - theta)/theta\n## and therefore theta = 8/(z + 1)\n8/(qnorm(0.2) + 1)\n\n[1] 50.51182"
  },
  {
    "objectID": "L07-Normal_Distributions.html#footnotes",
    "href": "L07-Normal_Distributions.html#footnotes",
    "title": "6  The Normal Distributions",
    "section": "",
    "text": "This is done with integrals, but we won’t actually do this in this course.↩︎\nFor continuous distributions, “probability” and “area under the curve” are synonyms.↩︎\n\\((2\\pi\\sigma^2)^{-1/2}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right)\\)↩︎\nI generally keep a running tally of the number of normal distributions I draw on the board. Last time I did this, I was almost at 100. The moral: you should be drawing a lot of normal distributions!!!↩︎\nThis is also the reason why the empirical rule works! If you change the labels on the plot, \\(\\mu+\\sigma\\) stays in the same place so you can calculate the same probability.↩︎\nThe negative is important!↩︎\nRecall that \\(P(X=x) = 0\\) in continuous distributions, so we look at ranges.↩︎\nIf you’re curious, yes I’ve made a lot of Shiny apps. You can find them all here: https://github.com/DBecker7/DB7_TeachingApps↩︎\nThe rows are the digits before and after the decimal, the column is the second digit after the decimal.↩︎\nYou need to trust your ability to use the Empirical Rule, though.↩︎\nQuick maths - we’re just trying to get an okay guess, not the exact answer right now.↩︎\nIf the z-score were -3.54, we’d say the probability is 0.↩︎\nRow labelled -1.5, column labelled 0.02.↩︎\nVerify this!↩︎\nP(X &lt; 5) - P(X &lt; -1)↩︎\nRecall: the body of the Z table are probabilities, the margins are z-scores.↩︎\n-0.68*3 + 2 and -0.675*3 + 2 would also be acceptable.↩︎\nP(X=x) = 0↩︎\nHint: Use the app with “Sticky Axes” checked and unchecked.↩︎\nPre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge.↩︎\nHint: Find the z-score for Q1, then fill out the standardization formula with the values you have.↩︎\nNormal as in “usual”, not as in the normal distribution.↩︎\nThis is actually a very useful way to think about distributions, especially in Bayesian statistics.↩︎\nSorry, the details are far beyond the scope of this course.↩︎\nThis is one of the hardest questions you will encounter.↩︎"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#introduction",
    "href": "L09-Binomial_Probabilities.html#introduction",
    "title": "7  Binomial Probabilities",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nProbability models are ways of laying out all possible events as well as the probability of each event. For things like coins and dice, everything has the same probability and things work out nicely. In Two-Way tables, we have all the probabilities laid out in front of us. The Binomomial distribution is our first foray into a formulaic approach to probabilities.\n\nWith Coins\nIf we flip two coins, the outcomes are {HH, HT, TH, TT} and each of these are equally likely. Instead of looking at each event, what is the probability that there are 0 heads? 1 head? 2 heads?\nFor 0 and 2 heads, there is only 1 possibility, so it must be 1/4 for each. For 1 head, there are 2 possibilities, each with probability 1/4, so the answer is 2*1/4.3\nLet’s flip three coins. The outcomes are {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}, so each outcome has a probability of 1/8. Another way to come to this number is to look at the probability of heads: For HHH, the probability is 0.5*0.5*0.5 since there’s a 50% chance of heads and each coin flip is independent.4\n\n\n\n# Heads\nOutcomes\nProbability\n\n\n\n\n0\nTTT\n1/8\n\n\n1\nTTH, THT, HTT\n1/8+1/8+1/8 = 3/8\n\n\n2\nHHT, HTH, THH\n3/8\n\n\n3\nHHH\n1/8\n\n\n\nAlright, let’s do 4 coins. How many ways are there to get, say, 2 heads out of four flips? You can bet that a smart mathemetician has figured out a way to do this without writing them all out again! This is called combinatorics, and includes a lot of things that are not relevant right now. We’ll focus on the choose function. For three coins, “3 choose 1” means “out of 3 options, choose 1 of them”. Sometimes this is shortened to “3C1”. As we saw in the table above, there’s 1 way to choose nothing (no heads), 3 ways to choose 1 thing, 3 ways to choose 2 things, and 1 way to choose 3 things.5 In R:\n\nchoose(n = 3, k = 0) # 3C0\n\n[1] 1\n\nchoose(n = 3, k = 1) # 3C1\n\n[1] 3\n\nchoose(n = 3, k = 2) # 3C2\n\n[1] 3\n\nchoose(n = 3, k = 3) # 3C3\n\n[1] 1\n\nchoose(n = 4, k = 2) # 4C2\n\n[1] 6\n\n\nSo for 4 coins, there is 4C2 = 6 ways to get two heads.6 What’s the probability of each of these 6 outcomes? Since there’s a 0.5 chance of heads and a 0.5 chance of tails, there’s a 0.5*0.5*0.5*0.5 = 0.5\\(^4\\) = 0.0625 chance. That means that there’s a 6*0.0625 = 0.375 chance of getting two heads out of four flips.7\nJust to be complete, let’s do this again for 5 coins. We’re already at the point where we need the choose function because there are too many outcomes to write out by hand. Let’s calculate some probabilities with R:\n\n## Probability of 4 heads out of 5 flips:\nchoose(5, 4) * 0.5^5\n\n[1] 0.15625\n\n## Probability of 3 heads out of 5 flips:\nchoose(5, 3) * 0.5^5\n\n[1] 0.3125\n\n\nFor completeness, let’s make sure these all add up to 1:\n\n## I really hope this adds to 1\n(choose(5, 0) * 0.5^5) +\n  (choose(5, 1) * 0.5^5) +\n  (choose(5, 2) * 0.5^5) +\n  (choose(5, 3) * 0.5^5) +\n  (choose(5, 4) * 0.5^5) +\n  (choose(5, 5) * 0.5^5)\n\n[1] 1\n\n\n\n\nWith Dice\nIf I roll two dice, what’s the probability that exactly 1 of them is a 3? One way this can happen is if the first dice is a 3 and the second one is not a 3. This probability is P(first dice is 3)*P(second dice is not 3). We know that P(first dice is 3) is easily seen to be 1/6. On the other hand, P(second dice is not 3) can be calculated as 1 - P(second dice is 3)8 = 1 - 1/6 = 5/6. So the probability is (1/6)*(1 - 1/6).\nWe can also have exactly one 3 if the first dice is not 3 but the second dice is 3. This has the same probability as the other way around: (1 - 1/6)*(1/6).\nNotice how this is the second of two options. Again, we get to use the choose function:\n\n## Probability of exactly one 3 in two dice rolls\nchoose(2, 1) * (1/6)*(1-1/6)\n\n[1] 0.2777778\n\n\nIf we roll 18 dice, what’s the probability that exactly four of them are 5? Regardless of the order, we have four dice that are 5 and 14 dice that are not 5. The probability of any one of the outcomes is (1/6)\\(^4\\)*(1 - 1/6)\\(^{14}\\) = 0.000060098. That’s pretty unlikely for this exact dice combination of dice rolls! But how many ways are there for this to happen? There are 18C4 = 3060, which is a lot, so there are a lot of opportunities for things with small probabilities. The probability of exactly four 5s out of 18 rolls is 18C5*(1/6)4*(1-1/6)14 = 0.1840. Even though an individual dice roll is unlikely, there are a lot of dice rolls that meet our criteria of four 5s out of 18 rolls!"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#binomial-probabilities",
    "href": "L09-Binomial_Probabilities.html#binomial-probabilities",
    "title": "7  Binomial Probabilities",
    "section": "7.2 Binomial Probabilities",
    "text": "7.2 Binomial Probabilities\nIn general, if we have \\(n\\) trials and the probability of the event of interest, a.k.a. success, is \\(p\\), then\n\\[\\begin{align*}\nP(x\\text{ successes in }n\\text{ trials}) = nCx*p^x*(1-p)^{n-x}\n\\end{align*}\\]\nFor the dice example, “x successes in n trials” can be interpreted as “four 6s in 18 trials”, where \\(x=4\\), \\(n=18\\), and the “14 rolls that are not four” comes from \\(n-x=14\\).\n\nConfusing (but Useful) Notation\nIn the statement above, we used \\(x\\) to refer to the number of heads. I like this. Let’s keep doing this.\nFor Binomial probabilities, we use the notation:\n\\[\\begin{align*}\nX \\sim B(n,p)\n\\end{align*}\\]\nwhich is read as “the random variable X is distributed as Binomial with n trials and probability of success p”.9 The “\\(\\sim\\)” just means “is distributed as”, which tells us where the probabilities are distributed. This is why \\(nCx*p^x*(1-p)^{n-x}\\) is called the probability distribution function, or pdf.10\nA random variable is just a variable that has a probability distribution,11 such as the number of heads out of 5 flips. Before flipping these coins we have no idea how many heads there will be, but we know the probability of each number. We always use upper case letters for random variables. Once we actually have a value (say, 1 heads), we use lower case. We often use the notation \\(P(X = x)\\) to refer to “the probability that the random variable \\(X\\) will have the specific value of \\(x\\)”. In other words, \\(X\\) is the unknown that could be anything, \\(x\\) is the specific probability that we’re interested in.\nI just want to talk about “distributions” a little bit more. A distribution tells you where the probabilities are. For coins, 50% of the probability is in Heads, 50% is in in tails. When we talk about “is the dice a 3?”, one-sixth of the probability is distributed to the 3 and five-sixths are distributed elsewhere.\nTo summarise, saying that \\(X \\sim B(n,p)\\), or that \\(X\\) is distributed as a Binomial random variable with \\(n\\) trials and probability of success \\(p\\), is the exact same as saying that \\(P(x\\text{ successes in }n\\text{ trials})\\) can be found using the equation \\(nCx*p^x*(1-p)^{n-x}\\). This is what it means to have a probability distribution function.\nTo see all of these probabilities at once, we can plot this as a graph. To reduce coding, let’s look at \\(X\\sim B(3, 0.4)\\):\n\nx &lt;- c(0, 1, 2, 3) # X values\ny &lt;- c(\n    choose(3, 0) * (0.4)^0 * (1 - 0.4)^3,\n    choose(3, 1) * (0.4)^1 * (1 - 0.4)^2,\n    choose(3, 2) * (0.4)^2 * (1 - 0.4)^1,\n    choose(3, 3) * (0.4)^3 * (1 - 0.4)^0\n)\n\n#plot(x,y) # This will plot them, but it looks kinda bad\n## Since X can only be 0, 1, 2, or 3, let's us a bar plot!\nbarplot(y, names = x,\n  main = \"pdf of B(3, 0.4)\", xlab = \"x\",\n  ylab = \"3Cx * p^x * (1-p)^(3-x)\")\n\n\n\n\nNotice how 1 is the most likely value, with 2 being much less likely. This makes sense - if the probability of heads is less than 0.5, we expect that more of the coin flips will be tails! If the probability of “heads” were 0.5, then we would expect 1 and 2 to be equally likely.\n\n\nExamples\n\nSuppose I have a coin that is weighted so that Heads comes up 80% of the time. What is the probability that I get 8 heads in 10 flips?\n\n\nchoose(10, 8) * (0.8)^8 * (1-0.8)^2\n\n[1] 0.3019899\n\n\n\nWhat’s the probability that I get anything other than 10 flips?\n\nTry it yourself!\n\nWhat’s the probability that I get more than 8 heads in 10 flips?\n\nSince the events “9 heads” and “10 heads” are disjoint, we can calculate these individually and add them together.\n\n\n\nchoose(10, 9) * (0.8)^9 * (1-0.8)^1 +\n  choose(10, 10) * (0.8)^10 * (1-0.8)^9\n\n[1] 0.2684355\n\n\n\n\nIn R\nTyping out the whole formula is getting boring. Surely R, a statistical programming language, has a way to do it for me, right? Of course!\n\nchoose(10, 8) * (0.8)^8 * (1 - 0.8)^2\n\n[1] 0.3019899\n\ndbinom(x = 8, size = 10, prob = 0.8)\n\n[1] 0.3019899\n\n\nThe dbinom() function has exactly the arguments that you would expect. Lower case x is the specific value, size is the number of coin flips, prob is the probability of success. The d stands for “density”, which for our purposes is the same as “distribution”.\nAs a special note, R will take a vector for x. We can find multiple probabilities at once:\n\ndbinom(x = c(8, 9, 10), size = 10, prob = 0.8)\n\n[1] 0.3019899 0.2684355 0.1073742\n\n\nThis allows us to easily plot the pdf:\n\nx &lt;- 0:10 # a vector of the numbers from 0 to 10\n\n## note: x is the name of the object AND the argument,\n## hence why I wrote \"x = x\"\ny &lt;- dbinom(x = x, size = 10, prob = 0.8)\n\nbarplot(height = y, names = x)"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "href": "L09-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "title": "7  Binomial Probabilities",
    "section": "7.3 Cumulative Binomial Probabilities",
    "text": "7.3 Cumulative Binomial Probabilities\nA cumulative probability is the probability of observing up to \\(x\\) successes in \\(n\\) trials. In other words, this is \\(P(X \\le x)\\): the probability that the random variable \\(X\\) is smaller than or equal to some specific number \\(x\\). This is referred to as the Cumulative Distribution Function, or cdf. Unlike what we saw in the normal distribution, it really matters whether it’s \\(P(X\\le x)\\) or \\(P(X&lt; x)\\)!\nWhat’s the probability that we get at most 4 heads in 10 flips? That’s the same as the probability of 0 heads plus the probability of 1 heads plus the probability of 2 heads plus…\n\n## Note: R evaluates the arguments *in order*\n## It expects the arguments in the order of \"x, size, prob\",\n## so it assumes the first argument is x, the second is size,\n## and the third is prob.\ndbinom(x = 0, size = 10, prob = 0.5) +\n  dbinom(1, 10, 0.5) +\n  dbinom(2, 10, 0.5) +\n  dbinom(3, 10, 0.5) +\n  dbinom(4, 10, 0.5)\n\n[1] 0.3769531\n\n\nWhat about the probability of at most 40 heads in 100 flips? Do I have to type all that out?\nNope! We can use the pbinom() function. First, let’s verify it with what we’ve already calculated:\n\npbinom(q = 4, size = 10, prob = 0.5)\n\n[1] 0.3769531\n\n\nNow, let’s find the probability of at most 40 heads in 100 flips:\n\npbinom(40, 100, 0.5)\n\n[1] 0.02844397\n\n\nIt’s surprisingly small! Let’s look at the pdf to see why:\n\nx &lt;- 30:70 # The pdf is REALLY small outside this range\n\n## I'm going to colour the bars where x &lt;= 40\n## Start with a bunch of white bars by REPeating the colour\n## white for as many x values as we have\nmycols &lt;- rep(\"white\", length(x))\n## Next, we change the colour where x &lt;= 40\nmycols[x &lt;= 40] &lt;- \"red\"\n\n## Calculate the distribution function\ny &lt;- dbinom(x, 100, 0.5)\nbarplot(height = y, names = x, col = mycols)\n\n\n\n\n\nExamples\nWhat’s the probability of at least 40 heads in 100 flips? Be careful here: it matters whether I ask “at least” or “more than”. The cdf always calculates “less than or equal to”12, and the complement of this is “strictly greater than.13 If I’m looking for”strictly greater than”, I need to be careful what I use!\nIn this case, P(X \\(\\ge\\) 40) = P(X &gt; 39) = 1 - P(X \\(\\le\\) 39) = 1 - pbinom(39, 100, 0.5)\n\n1 - pbinom(q = 39, size = 100, prob = 0.5)\n\n[1] 0.9823999"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "href": "L09-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "title": "7  Binomial Probabilities",
    "section": "7.4 Properties of the Binomial Distribution",
    "text": "7.4 Properties of the Binomial Distribution\nI define “math” as the process of making up rules just to see what happens. The Binomial distribution isn’t just some abstract entity that we discovered - it’s a set of rules we created that seem to logically fit some situations.14 So first: what are the rules?\n\nBinomial Assumptions\nI’m going to motivate these assumptions first. If you’re the type that just wants to memorize, you can skip to the end of this section.\nWe’ve been talking about flipping coins and rolling dice, which helped motivate this distribution. We wouldn’t be teaching you this distribution if it only applied to dice and coins, so when can we apply it?\nConsider flipping a “sticky” coin twice. It starts with a 50/50 chance of being heads, but the next flip has a 75% chance of being the same as the first.15 So if the first flip was heads, there’s a 75% chance that the second flip will be heads. If the first flip was tails, there’s a 75% chance that the second flip will be tails.\nLet’s first just calculate the probability of each outcome. The probability that the first flip is heads and the second flip is tails can be found using the Multiplication Rule, which states that P(A and B) = P(A)P(B|A). So P(HH) = P(first is H)P(second is H given that the first was H) = 0.5*0.75 = 0.375. Similarly, P(HT) = 0.125, P(TT) = 0.375, and P(TH) = 0.125.16\nLet’s compare these probabilities with the ones we calculated earlier. The probability of 0 heads with the fair coin was 1/4, and this value was calculated with the binomial distribution. With the sticky coin, the probability of 0 heads is 0.375, which does not come from the binomial distribution.\nFormally, the Binomial distribution assumes that each trial is independent and the probability of success is the same in each trial. While I didn’t touch on this, the only random thing should be the number of successes, not the number of trials. Finally, recall that, with the dice, I converted things to “3” or “not 3”; the Binomial distribution only works when each individual trial can only be one thing or another. More succinctly, the assumptions for the Binomial Distribution are:\n\nThere are n trials, and this number is known ahead of time.\nEach trial is either a “success” or a “failure”.\nEach trial is independent of the other trials.\nThe probability of success is the same for all trials.\n\n\n\nSide note: “probability of success is the same”\nAs an example, consider studying, say, the proportion of questions that a student got right on a multiple choice test. Each student has a different probability of getting each question correct. However, if we want to say something about the proportion of questions that a random student gets right on a test. In this sense, the fourth assumption is not violated.\nAs an alternative, consider a test where the students go one-by-one17 and can see the previous student’s solutions. In this case, the probability of success changes as you have more trials. This is where the problem lies - the students are still coming in a random order, but the probability of success changes.\nAs another alternative, suppose some students are cheating. They’re more likely to get the right answers together, so they’re answers are dependent on each other; knowing one cheater’s answer gives you a better guess at another cheater’s answer.\nIn summary, a different probability of success is only an issue if the researcher would be able to know this ahead of time. If the probability of success is different but we have a simple random sample with independent trials, there is no issue with this assumption.\n\n\nBinomial Mean and Variance\nNow that we know the assumptions, we can see what comes out of these assumptions. First, we can find the average value. It makes perfect sense that the average number of heads in 10 flips should be 5. There’s a 50/50 chance of heads, so you’d expect half of the flips to be heads! Formally, \\(\\mu = np\\).18 That is, the theoretical average is just the number of trials times the probability of success.\nWhat about the variance? It’s not as obvious. I’m going to try and give my own intuitive argument, but most teachers and textbooks simply skip this and have you memorize the answer. If this is your style, you can skip to the end of this section.\nIn the past, I have defined “variance” as something like “the average amount that you would be wrong if you always guessed the mean value.” Consider flipping one coin. If this coin is rigged and always comes up heads, the mean number of heads is 1 and you would always be right when you guess the mean. Intuitively, the variance here is 0. The same happens if the coin is rigged to always come up tails - the mean number of heads is 0, and the number of heads never varies so the variance is 0.\nWhat happens between 0 and 1? If the coin was heads 80% of the time, then your guess would be right 80% of the time. The actual value of the coin varies, but not too much. If the coin was heads 20% of the time, you’d still be right 80% of the time by guessing 0 heads each time. You’d be wrong most often if the coin had a 50% chance of being heads.\nSo we’ve established this: At \\(p=0\\) and \\(p=1\\), the variance is 0. The maximum value is at 0.5, and the variance should be the same if you’re 0.2 above 0.5 or 0.2 below (it’s symmetric around 0.5). The following plot, then, seems reasonable:\n\n\n\n\n\nThere’s a lot of math behind this, but the variance for Bin(1,p)19 turns out to be p(1-p). You can see that it would be symmetric around 0.5 and would be 0 whenever p=0 or p=1.\nIn general, the variance of a B(n,p) distribution is \\(\\sigma^2\\) = np(1-p)."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#conclusion",
    "href": "L09-Binomial_Probabilities.html#conclusion",
    "title": "7  Binomial Probabilities",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIf you have a known number of repeated trials that are independent and are either a “success” or “falure”, then the Binomial distribution is your friend. Once these assumptions are met, you can calculate the probability of any number of successes using the pdf, you know what the mean number of successes in \\(n\\) trials will be, and you know the variance!20\nAs a rule, if you see the phrase “Not enough information for a valid answer” as an option in a multiple choice question, double check that the assumptions are all met. If the observations are not independent, you need to know all of the conditional probabilities in order to calculate the answer, which you probably don’t have, so you’re missing information. If the probability of success changes from trial to trial, you need to know how it changes."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#self-test-questions",
    "href": "L09-Binomial_Probabilities.html#self-test-questions",
    "title": "7  Binomial Probabilities",
    "section": "7.6 Self-Test Questions",
    "text": "7.6 Self-Test Questions\n\nWhat happens when you put x = 0.5 into dbinom(x, 10, 0.5)? Interpret this in terms of flipping coins.\nI debated whether to include a section on “shape”, but decided to let you figure it out for yourself. I’ve already given you the code to plot the pdf. For each of the values of n (size) and p (prob), plot the pdf and describe the shape. Note that x should (almost) always be x &lt;- 0:n.\n\nBin(50, 0.5)\nBin(50, 0.8)\nBin(50, 0.2)\nBin(50, 0.02)\nBin(4, 0.25)\n\nFor each of the assumptions, give an example of a situation that violates only one of them, not the others."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#footnotes",
    "href": "L09-Binomial_Probabilities.html#footnotes",
    "title": "7  Binomial Probabilities",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nSince these events are disjoint, we can simply add them.↩︎\nFor coin flips, this is obvious. It won’t always be!↩︎\nAnd 0 ways to choose 4 things.↩︎\nVerify this by writing out all of the possibilities.↩︎\nDon’t be afraid to re-read this paragraph several times, there’s a lot of math here.↩︎\nThe dice is either 3 or it’s not 3, these probabilities must add to 1.↩︎\nNotice how we’re using upper case for the random variable, and lower case for the actual values. This is important for future stats classes, but just something you’ll see me do for now.↩︎\nI usually use lower case so you don’t confuse it with the PDF file extension.↩︎\nThere’s a much more correct, much more technical definition, but it’s outside the scope of this course.↩︎\nP(X \\(\\le\\) x)↩︎\nP(X \\(\\le\\) x) = 1 - P(X &gt; x)↩︎\nThe philosophy of math is extremely interesting. Most philosophers seem believe that we do discover math, rather than create it. I also believe this, but probability distributions are in a grey area for this part of philosophy. When all this is over we should grab a drink and discuss this.↩︎\nIf an engineer could make this coin for me I’d be infinitely grateful.↩︎\nAlways make sure the numbers that I give you add to 1 - I will try and trick you with this!↩︎\nin a random order↩︎\nWhy \\(\\mu\\) and not \\(\\bar x\\)? Because this is a theoretical result. You can think of this as being the “true” population.↩︎\nWhen n=1, this is also called the Bernoulli distribution, but this is not important right now.↩︎\nStatistics is the study of variance.↩︎"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "href": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "title": "8  Sampling Distributions",
    "section": "8.1 Prelude: Populations and Samples",
    "text": "8.1 Prelude: Populations and Samples\nThe main idea in the rest of the course is this: We can use a sample to say something about the population. Before we dive into that idea, let’s make a distinction.\n\nStatistic: A number that we calculate from data.\nPopulation parameter: The value of a statistic if it were calculated for the whole population.\nSample Statistic: The value of a statistic if it were calculated for a single sample.\n\nFor example, we find the mean by taking all of the values and adding them up, then dividing by the number of things we added. For heights of Canadians, the population parameter is the value we would get if we found every Canadians’ height and added them up, then divided by the population of Canada. We obviously can’t do this, but it’s useful to think about. The sample mean is the mean we get when we just have a sample. Since we can only get a sample, it would be super cool if we could use that sample mean to talk about what values of the population mean were reasonable guesses.\nIn the height example, the population was all Canadians. This isn’t always how we define the population! For example, if we wanted to know the average length of pregnancy, we’d be looking at a population of all people who get pregnant at some point in their lives."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#introduction",
    "href": "L10-Sampling_Distributions.html#introduction",
    "title": "8  Sampling Distributions",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nYou take a sample. You find the sample mean. Is this mean exactly equal to the population mean?3 Probably not.\nWait, did I just say probably not? How probably? We’ve done a few lectures on probability, so we can probably same describe the distribution somehow. What is the probability that the sample mean is within one standard deviation of the population mean? Two standard deviations?\nBecause of random sampling error,4 every sample is going to have a different mean. We expect most of the sample means to be close to the population mean, with fewer samples resulting in sample means that are further away. In other words, the sample mean should be close to the population mean, but due to sampling error there will be a little bit of a difference.\nThe variation within our sample should be similar to the variation within the population5, and the variance in the poulation tells us the variance in the sample means. Variation is not something to be afraid of, and sampling errors are not sampling mistakes; we can harness the variability within a sample to draw conclusions about the population!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "href": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "title": "8  Sampling Distributions",
    "section": "8.3 Sampling distribution of the sample mean",
    "text": "8.3 Sampling distribution of the sample mean\nBecause the value of a sample mean is random (since we took a random sample), there’s a probability distribution that describes it. I could just jump to the answer, but it’s best if I build up to it.\nThe app below6 will take a random sample from the population (in this case, normal), then find the mean and add it to a histogram. As you collect more means, the histogram gets more and more data. This simulates taking many many different samples.\n\nlibrary(ggplot2) # if this fails, run install.packages(\"ggplot2\")\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/samplingDist\")\n\nPlay around yourself! Start with \\(n\\) equal to 2 or 3. The sample shows the individual values, but it also shows the sample mean. Notice how the mean is usually closer to the population mean than any of the individual sample values.\nNow, take another sample! Again, the sample mean is closer to the population mean than most of the sampled values. Take more samples. Take 1000 more samples. Notice how the distribution of sample means is bell-shaped, but slightly skinnier than the population.\nRepeat what you did above, but use n = 25 or so. The histogram of sample means is even skinnier now! It’s still centered on the population mean, though!\nThese histograms are approximations to the sampling distribution of the sample mean. If you take an infinite number of samples and calculate the mean for each different sample, you’ll get a distribution of all possible sample means. This is what a sampling distribution is. I’m going to repeat that, since this is often a very difficult topic: the population distribution shows you the probability distribution for all possible individuals, while a sampling distribution shows you the probability distribution for all possible sample means. Each sample has a different mean, the sampling distribution describes many many samples."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#normal-populations",
    "href": "L10-Sampling_Distributions.html#normal-populations",
    "title": "8  Sampling Distributions",
    "section": "8.4 Normal Populations",
    "text": "8.4 Normal Populations\nIf the population is normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then there is some relatively straightforward math7 to show that:\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\nThat is, the distribution of all possible sample means8 is normal with the same mean as the population, but with a smaller standard deviation. Go back to the app and see this for yourself.\n\nExample\nSuppose the population of heights of Canadian women is N(162.3, 7.11).9 We’re going to try and build up some intuition for why the distribution of all means has a smaller variance than the distribution of the population.\n\nThe probability that a randomly chosen woman is taller than 170 cm is \\(P(X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11) = 0.139. So there’s about a 14% chance of finding a woman taller than 170 cm.\n(This is just for example - this question is not often important.10) If we take a sample of n=2 women, what’s the probability that both of them are taller than 170cm? If it’s a truly random sample, then the heights of the two women should be independent and we can just multiply their probabilities.11 This means that there’s approximately 0.14% chance of this. Obviously, if one woman taller than 170 is unlikely, then both women taller than 170 is very unlikely.\nIf we take a sample of n=2 women, what’s the probability that their average height is larger than 170? From above, we know that the distribution of the sample mean is \\(N(162.3, 7.11/\\sqrt{2})\\), so we can calculate this probability as \\(P(\\bar X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11/sqrt(2)) = 0.06. This is somewhere in between just one of them being taller than 170cm and both of them being taller than 170.\n\nWhen we took a sample of 2 women, one might have been taller than 170 but one might have been shorter, so the average ends up being less than 170. The sample mean is less variable than the individual values, so it’s less likely to be further away.12\nSummary: If you take two values from a normal distribution, the average of those two values is probably closer to the true mean than either of the individual values. If you found the average of 100 observations from a normal distribution, the mean is probably even closer to the true mean."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "href": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "title": "8  Sampling Distributions",
    "section": "8.5 Non-Normal Populations with Large Sample Size",
    "text": "8.5 Non-Normal Populations with Large Sample Size\nIn the previous example, we saw that a normal population distribution will result in a distribution for all possible sample means that is also normal, but with a smaller variance. If the population isn’t normal, but you have a large enough sample size, the sampling distribution is still normal. It’s kind of amazing, but it seems to work in practice!\nThe app below13 will help you understand this relationship. I use an “Exponential distribution” for the population, but this isn’t a distribution you really need to worry about. All you need to know is that the population clearly isn’t normal.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/nLarge\")\n\nRegardless of “lambda”14, as n increase, the sampling distribution becomes closer and closer to the normal distribution. By around n=30 or 40,15 they’re basically the same!16\nAgain,\n\\[\n\\text{If }X\\sim N(\\mu, \\sigma)\\text{ and n is ``large'', then }\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\n\\]\nwhere 60 is definitely “large”, 50 is probably “large”, 30 is debatably “large” (depending on what textbook you read), and anything less than 30 is definitely small. I will not test you on the grey areas here.\nThis result has a very special name:\nThe Central Limit Theorem: Given a simple random sample of size \\(n\\) (where \\(n\\) is “large”) from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean will follow a \\(N(\\mu, \\sigma/\\sqrt{n})\\) distribution.\nFor a perfectly normal population, this is true for any \\(n\\). For a population that just a little bit not normal, \\(n\\) must be moderately large. For a very not normal population (e.g. Binomial with \\(p\\) far from 0.5), we need \\(n\\) even larger. Still, as long as the sd of the population is finite, the sampling distribution will be normal for sufficiently large \\(n\\)!\n\nExamples\n\nThe angle of big toe deformation in 38 patients.\n\nThere’s an outlier, but the sampling distribution would still be normal even for relatively small \\(n\\).\n\nThe number of servings of fruit per day for 74 adolescent girls.\n\nThe distribution is clearly (???) skewed17. This makes sense - the number of fruits can only be as low as 0 and there may be many people who don’t eat a lot of fruit, but there will be a few eating many fruits per day!\nThe skewness of the data implies skewness in the population (assuming this is a good sample). No worries, though, the sampling distribution will still be normal! We just might need a larger sample size in future studies.\n\nThe lengths of 56 perch from a Swedish lake.\n\nThis is clearly a bimodal distribution, indicating that there might be two subgroups in these data.\nThe sampling distribution will still be normal (unimodal), but the mean of this sampling distribution will probably be somewhere in between the two peaks. In other words, it won’t be describing either of the apparent subgroups! No amount of beautiful theorems will ever fix errors in sampling.\nIn this case, we would want to find out why there are two subgroups before trying to say anything about the population distributions. If we actually have two types of fish, it’s better to study them separately!\n\n\n\nSource: Baldi & Moore, 4th Edition.\n\n\nNon-Normal Population with Small Sample Size\nThis is governed by the \\(t\\)-distribution, which will be covered later."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "href": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "title": "8  Sampling Distributions",
    "section": "8.6 Very Non-Normal: The Binomial Distribution",
    "text": "8.6 Very Non-Normal: The Binomial Distribution\nHere’s some mild deja-vu:\nYou roll a dice. You find the sample proportion of heads, denoted \\(\\hat p\\).18 Is this proportion exactly equal to the population proportion? Probably not.\nWait, did I just say probably not? How probably? What is the probability that the sample proportion is within one standard deviation of the population proportion?\n\nAside: The normal approximation to Binomial\nMost textbooks provide the rule: if both np and n(1-p) are larger than 1019, then the normal distribution is a good approximation to the binomial distribution. I prefer to let you see whether these rules make sense. The app below lets you change n and p, and shows a \\(B(n, p)\\) and an \\(N(np, \\sqrt{np(1-p)})\\)20 distribution.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/normBinom\")\n\nSet n = 20 and find p such that np &lt; 10. Also find p such that n(1-p) &lt; 10. What is the shape of the Binomial distribution in these cases? What do you notice about the normal distribution? Why do both np and n(1-p) need to be greater than 10?21\n\n\nBack to Binomial\nIt turns out that, with large \\(n\\) the sampling distribution of \\(p\\) also follows a normal distribution!22 Even though the population distribution isn’t even continuous,23 the normal distribution approximates it well when there are lots of samples.\nFor each sample, the actual proportion that you calculate is variable. You might get 3 heads out of 10 flips one time, then 8 heads out of 10 flips the next. On average, though, you’ll get 5 heads out of 10 flips. Formally, the mean of the sampling distribution of the sample proportion is \\(p\\).24\nThe variance is a little trickier. In the Binomial lecture notes, I said that the variance increases as n increases. However, when we calculate the proportion, we take the number of successes divided by n. According to some math that is not important for this course, this leads to a variance of the sampling distribution of the sample proportion of p(1-p)/n, which means that the standard deviation25 of the sampling distribution is \\(\\sqrt{p(1-p)/n}\\).\nTo recap: The variance of a Binomial distribution is \\(np(1-p)\\). If we take repeated samples from that Binomial distribution and calculate the proportion of sucesses, the variance will be \\(p(1-p)/n\\).26\n\n\nExample\nSuppose I’m rolling a dice 5 times. The probability of exactly 2 ones is defined by the Binomial distribution: dbinom(2, size = 5, prob = 1/6) = 0.16.27 The variance in the number of ones in 5 rolls is np(1/p) = 5/36.\nThe average number of ones in 5 rolls is np=5/6. The standard deviation of the number of ones in 5 rolls is \\(\\sqrt{np(1-p)} = \\sqrt{5/36}\\).\n\n\nExampling Distribution\nThe following code is not testable - you are not expected to write anything like this. I’m taking repeated samples from a B(50, 0.4) distribution and calculating the proportion of successes for each sample.\n\nset.seed(4)\nn &lt;- 75\np &lt;- 0.4\n\nbinom_proportions &lt;- c() # empty vector, to be filled later\n\nfor(i in 1:1000){ # repeat this 1000 times:\n    # This is confusing: I'm getting *one* sample of size n,\n    # but R labels the number of samples as n\n    new_sample &lt;- rbinom(n = 1, size = n, prob = p)\n    \n    # Add the proportion of successes to the vector\n    binom_proportions[i] &lt;- new_sample/n\n}\n\nhist(binom_proportions, \n    breaks = 10, # what happens if you make this larger?\n    freq = FALSE) # Divide the heights of bars by the number of obs.\ncurve(dnorm(x, mean = p, sd = sqrt(p*(1-p)/n)), add = TRUE, col = 3, lwd = 3)\n\n\n\n\nCopy and paste the code above into a script file and observe what happens when you increase the number of breaks. Why does this happen?28"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "href": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "title": "8  Sampling Distributions",
    "section": "8.7 Conclusion: Statistics is the Study of Variance",
    "text": "8.7 Conclusion: Statistics is the Study of Variance\nIn both of the sampling distributions above, the mean of the sampling distribution was the mean of the population. The difference between the population and the sampling distribution is the variance. In both sampling distributions, the variance decreases as n increases. If you sample the entire population every time you do a sample, there will be no variance in your estimate!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#self-study-questions",
    "href": "L10-Sampling_Distributions.html#self-study-questions",
    "title": "8  Sampling Distributions",
    "section": "8.8 Self-Study Questions",
    "text": "8.8 Self-Study Questions\n\nWhen do we use \\(N(\\mu, \\sigma/\\sqrt{n})\\) versus \\(N(\\mu, \\sigma)\\)? When do we use \\(N(p, \\sqrt{p(1-p)/n})\\) versus \\(N(np, \\sqrt{np(1-p)})\\)? This distinction is extremely important.\nIf the population is \\(N(2,4)\\) and we take a sample of size 10, explain why \\(\\frac{\\bar X - 2}{4/\\sqrt{10}}\\) follows a standard normal distribution. This is extremely important.\nWhat does it mean for the sample mean to be the same as the population mean? Will they be the same every time you take a sample?\nPlay around with the “normBinom” app shown above. Why is the normal distribution not appropriate when np&lt;10 or n(1-p)&lt;10?\nIn the “Histogram of binom_proportions”, what happens when you increase the number of breaks? What causes this phenomenon?"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#footnotes",
    "href": "L10-Sampling_Distributions.html#footnotes",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nRecall: population refers to the population of interest. The population mean is the true mean of the population.↩︎\nIn statistics, error does not mean mistake.↩︎\nAssuming we have a good sample**.↩︎\nIf you don’t have access to R right now, try this one.↩︎\nYou’ll probably see it in the next stats course you take.↩︎\ni.e. the sampling distribution of the sample mean↩︎\nNote: these numbers actually come from a sample, and we don’t know that the population is normal. We’re making some massive assumptions here.↩︎\nYou will not need to do something like this on a test.↩︎\nRemember the most important fact from probability: Multiplying probabilities only works when they’re independent.↩︎\nTake a moment and make sure you understand this relationship. Write out a description of it. Call a grandparent and try to explain it to them.↩︎\nOr the same app as before, with population set to Exponential.↩︎\nWhich controls how skewed the population distribution is.↩︎\nI will either ask you questions where n &lt; 30 (non-normal sampling distr.) or n &gt; 50 (normal sampling distr.), nothing in between.↩︎\nAlthough, in this case, the normal approximation is biased, but the bias decreases as n increases and you’re not expected to know these details.↩︎\nAnswer: right↩︎\ni.e. \\(\\hat p\\) = number of success divided by number of trials.↩︎\nOr sometimes 15. Again, I won’t test you on the grey areas.↩︎\nRecall that the mean and sd of a Binomial distribution are np and np(1-p), respectively.↩︎\nAnswers: Skewed; positive probability below 0 and above n; symmetric.↩︎\nAgain, we use the rule of thumb that \\(np&gt;10\\) and \\(n(1-p)&gt;10\\).↩︎\nThis is important.↩︎\nNot n*p, since the proportion of heads is x/n.↩︎\nWhich is simply the square root of the variance.↩︎\nNotice how they’re equal when n = 1. When n=1, we’re just taking individuals from the population and calling each individual a sample.↩︎\nIn other words, 2 successes in 5 trials, where a success is defined as “rolling a one”.↩︎\nHint: What are the possible values of \\(\\hat p\\)?↩︎"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "href": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "title": "9  Welcome to Inference!",
    "section": "9.1 Inference Basics",
    "text": "9.1 Inference Basics\n\nProbability vs. Inference\nIn probability, we have distributions and calculate how likely given values are. In inference, we have a value that came from a distribution and try to determine things about that distribution.\nRecall: Sampling Distributions\n\nIf the population is \\(N(\\mu,\\sigma)\\), the sampling distribution of the sample mean is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nAssuming an SRS, 95% of sample means should be within 2\\(\\sigma/\\sqrt{n}\\) of the population mean.3\n\n\n\nFlippin’ it: Confidence intervals\nInstead of asking “What’s the probability that a sample mean is further than 2\\(\\sigma\\) away?”, we can ask “If your sample mean is further than 2\\(\\sigma\\), is it reasonable to say that it comes from that particular population?”\nNotice the subtle shift - we’re now talking about something that we can do with just a sample. The Sampling Distributions section always assumed that the population mean was known and told us about potential sample means. We’re now shifting our perspective: given a sample mean, what are the potential population values?\nThe basic idea in this lecture is as follows: the sample should be similar to the population but a little bit off. What are the potential values of the population mean that are compatible with what we observed?"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "href": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "title": "9  Welcome to Inference!",
    "section": "9.2 Confidence Intervals",
    "text": "9.2 Confidence Intervals\n\nBackground\nGiven data, we want to make an inference about the population. Since \\(P(\\bar X = \\mu) = 0\\), we can’t just calculate the probability that we have the correct population mean. It’s always going to be 0!\nHowever, we can make guesses based on ranges! With confidence intervals, we create a range around our estimate that (hopefully) contains the true population mean. It won’t contain the true mean every time, but if we do things right, we can quantify our confidence that it does.\nAll CI’s that we learn in this class have the form: \\[\n\\text{Estimate} \\pm \\text{Margin of Error}\n\\]\n\n\nThe Margin of Error (MoE)\nIf the population is normal with mean \\(\\mu\\) and sd \\(\\sigma\\), then the Margin of Error is\n\\[\nMoE = (z^*)*(\\sigma/\\sqrt{n}) = \\text{Critical Value}*\\text{Standard Error}\n\\]\n\n\\(z^*\\) is a critical value. This is where we get our “confidence” from. This value is always positive.\n\\(\\sigma/\\sqrt{n}\\) is the standard deviation of the sampling distribution, which is also called the Standard Error.\n\n\n\nCritical Values\nIf \\(z^* = \\infty\\), it means that the confidence interval is infinitely wide. That is, we’re 100% confident that the true population mean is in the interval!\nIf \\(z^* = 0\\), it means the CI is just the point estimate. In other words, we’re 0% confident.\nUsually, we choose a confidence level in between 0 and 100. Values of 90%, 95%, or 97.5% are common. This values strike a nice balance between being useful and being less than infinity.\n\n\nCalculating critical values: 0.95%\nIf \\(X\\sim N(\\mu, \\sigma)\\), then the sampling distribution is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nWTo make a confidence interval, we want a range of values \\((L, U)\\) such that \\(P(L &lt; \\bar X &lt; U) = 0.95\\).\nThe normal distribution is symmetric. If we want 95% in the middle, then we need 0.025 below L and 0.025 above U. This is equivalent to values such that \\(P(\\bar X &lt; L) = 0.025\\) and \\(P(\\bar X &lt; U) = 0.975\\).\nWe can find \\(P(Z &lt; -z^*) = 0.025\\), then use the formula \\(x = z\\sigma+\\mu\\). However, since we’re using \\(\\bar X\\) instead (which has a standard deviation of \\(\\sigma/\\sqrt{n}\\) instead of \\(\\sigma\\)), this is \\(\\bar x = z^*\\sigma/\\sqrt{n} + \\mu\\).\nWe can do the same with \\(P(Z &lt; z^*) = 0.975\\) and find \\(\\bar x = z^*\\sigma/\\sqrt n + \\mu\\).\n\n\nWhat is \\(z^*\\)?\nFor \\(P(\\bar X &lt; L) = 0.025\\), \\(-z^* = -1.96\\) (almost -2).\nFor \\(P(\\bar X &lt; U) = 0.975\\), \\(z^* = 1.96\\) (almost 2).\nIn other words, it’s symmetric! The two ends of the interval are: \\[\n\\bar x = \\pm z^*\\sigma/\\sqrt{n} + \\mu\n\\]\nHowever, we don’t know the population mean. Instead, we have \\(\\bar x\\).\nA CI is defined as: \\[\n\\mu \\text{ is in the range } \\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\n\n\nSome notation: \\(\\alpha\\)\nA \\((1-\\alpha)\\%\\)CI is is defined as \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\nwhere \\(P(Z &lt; z^*) = \\alpha/2\\).\n\nFor a 95%CI, \\(\\alpha = 0.05\\) and \\(\\alpha/2= 0.025\\).\n\n\\(z^*\\) is found by finding the value such that \\(P(Z &lt;z^*) = 0.025\\).\nqnorm(0.025) = r round(qnorm(0.025), 4), so \\(z^* = 1.96\\).\n\nFor a 89%CI, \\(\\alpha = 0.11\\) and \\(\\alpha/2 = 0.055\\).\n\nqnorm(0.055) = r round(qnorm(0.055), 5), so \\(z^* = 1.6\\).\n\n\n\n\nInterpretation\n\nThere is no randomness in a 95% CI. The mean is fixed, the sd is fixed, the population mean is fixed.\nIt is NOT true that “95% of the time, the population mean falls in the CI”.\n\nThis is a classic gotcha.\n\nBy the way the CI is constructed, it will contain the population mean 95% of the time. We have no idea whether any particular one does, but 95% of them do.\n\nOn any given day, there’s a 10% chance of rain. However, it either rained yesterday or it didn’t. There’s not a 10% chance that it rained yesterday - it’s either 0% or 100%.\n\n\n\n\nSummary\nIf \\(X\\sim N(\\mu,\\sigma)\\), then a \\((1-alpha)\\%\\)CI is \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\] where \\(P(Z &lt; z^*) = \\alpha/2\\) can be found with qnorm (or a z-table).\n\nA 95% is based on finding the middle 95% of the sampling distribution, but centering it around \\(\\bar x\\).\n95% of the intervals constructed this way will contain the true population mean.\n\nA given interval has either a 0% chance or a 100% chance\n\nA point of sillyness: This assumes that \\(\\sigma\\) is known."
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "href": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "title": "9  Welcome to Inference!",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThis is using the empirical rule - the actual value is closer to 1.96.↩︎"
  },
  {
    "objectID": "L13-p_vals.html#overview-of-tests-of-significance",
    "href": "L13-p_vals.html#overview-of-tests-of-significance",
    "title": "10  Tests of Significance",
    "section": "10.1 Overview of Tests of Significance",
    "text": "10.1 Overview of Tests of Significance\n\nPhilosophy\n\nWe start with a “null” hypothesis, \\(H_0\\), which states that nothing “interesting” is going on.\n\nThe mean is exactly what we guessed, \\(H_0: \\mu = \\mu_0\\)\nThe effect of the drug is the same in both groups.\nSomething something “same as” something something.\n\nWe have an alternative hypothesis - things are different.\n\n\\(H_A: \\mu &gt; \\mu_0\\) (or \\(&lt;\\), or \\(\\ne\\))\n\nWe do our study and get our mean (for now, assume \\(\\sigma\\) known)\nWe check if our observed mean is “too unlikely” under the null.\n\nIf the null hypothesis is true, is our observed mean preposterous?\nThis is where the dreaded p-value comes in.\n\nWe make a decision - reject or don’t reject \\(H_0\\) - based on our p-value.\n\nTo summarize: We make a “guess” about the population. We collect data, and we determine whether or not our data is compatible with our guess. If it isn’t, then it’s the guess that must be wrong; not the data1.\nThe assumptions are the same as the assumptions for CIs:\n\nNormal population (or large sample size)\n\\(\\sigma\\) known\n\nWe will get away from this assumption later; for now it’s nice to ease into the concepts.\n\nSimple Random Sample (Independent Observations)"
  },
  {
    "objectID": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "href": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "title": "10  Tests of Significance",
    "section": "10.2 p-value by Example: Trailmaking Test for Fatigue",
    "text": "10.2 p-value by Example: Trailmaking Test for Fatigue\nThe following image shows the output of a “trailmaking” app. Subjects are shown the numbers on a touch screen and are tasked with drawing a line2 starting at 1, then 2, and so on without touching the other numbers. The time is recorded.\n\nIn my research, this app was given to aerial forest fire fighters. Flying a plane is a very challenging task to begin with, made much more challenging when there’s an active fire! The hypothesis is that pilots are measurably fatigued after a fire. However, this hypothesis must be converted into a mathematical construct that we can do something with!\nPilots perform the test many times before a long flight and once after. In samples from the aerial firefighters who were non-fatigued, it was found that completion time follows a normal distribution with mean 15 seconds and standard deviation 1.2 seconds3. We hypothesize that it took longer than that after the flight. \\[\\begin{align*}\nH_0: \\mu &= 15\\\\\nH_A: \\mu &&gt; 15\n\\end{align*}\\] The hypotheses above are created entirely based on the research question. We can (must) write the hypothesis before collecting data. \\(\\bar x\\) does NOT appear in hypotheses. Instead, the “15”s and the “&gt;” come from the hypotheses that fatigued pilots take longer than the population.\n\nResults\nWe caclulated a mean of 15.9 seconds from 16 pilots. Is this slower than 15 seconds? Obviously, these numbers are different, but is this a big difference? To tell whether two numbers are “far apart”, we need some sense of scale. In statistics, scale is given to us in the form of variance.\nThe population standard deviation is given as 1.2 seconds. How many standard deviations away from the hypothesized value is our sample mean? Well, since it’s a SAMPLE MEAN, the standard deviation is \\(1.2/\\sqrt{16} = 0.3\\) (again, this is also called the standard error). Our sample mean of 15.9 is 3 standard deviations4 above the hypothesized means.\nThe p-value for this is the probability of observing a value at least as far from the hypothesized mean, assuming that the hypothesized mean is the true mean5.\nOur p-value is P(Z &gt; 3) = 1 - P(Z &lt; 3) = 0.00136. Is our sample mean “unlikely” assuming that the null hypothesis is true?\nThe definition of “unlikely” will generally need to be given in the question. Usually, a significance level of \\(\\alpha = 0.05\\)7 is used8.\nSince our p-value is 0.0013 &lt; 0.05, our observed mean is “too unlikely.” So our hypothesis must be wrong!9 We conclude that the average time to complete the trail has increased, i.e. \\(\\mu &gt; 15\\)10. In this case, we say our result is statistically significant.\n\n\nSummary\nFrom the question, we got our hypotheses:\n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu &gt; 15\n\\end{align*}\\]\nWe caclulated our test statistic11:\n\\[ z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}  = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 0.9/0.3 = 3\\]\nWe looked up P(Z &gt; \\(z_{obs}\\))12 on our z-table, which gave us the p-value of 0.0013.\nSince this is a small probability (our p-value is less than our significance value of \\(\\alpha = 0.05\\)), we reject the null hypothesis in favour of the alternative.\nThis is the general approach to hypothesis testing: hypotheisize, calculate, find a normal value, then conclude."
  },
  {
    "objectID": "L13-p_vals.html#two-sided-p-values",
    "href": "L13-p_vals.html#two-sided-p-values",
    "title": "10  Tests of Significance",
    "section": "10.3 Two Sided p-values",
    "text": "10.3 Two Sided p-values\nIf your hypotheses are: \n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu \\ne 15\n\\end{align*}\\]\nthen you’re going to need to change things. In particular, you need to double the p-value for a one-sided test13. This is where the phrase “at least as extreme” comes in - we would reject anything this far away on either side.\nThe following shiny app demonstrates this. In particular, note what happens when you have a two sided alternative hypothesis and you double the wrong tails14.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\nTwo Sided Example\nGiven \\(\\sigma = 2\\), \\(n = 25\\), and \\(\\bar x = 6.6\\), test the hypothesis that the true population mean is not equal to 6 at the 10% level15.\n\\[\\begin{align*}\nH_0: \\mu = 6\\\\\nH_A: \\mu \\ne 6\n\\end{align*}\\]\ntest stat: \\(z_{obs} = \\frac{6.6 - 6}{2/\\sqrt{5}} = \\frac{0.6}{0.4} = 1.5\\)\nFind on z-table (or using R): P(Z &gt; 1.5) = P(Z &lt; -1.5) = pnorm(-1.5) = 0.0668\np-value = 2*0.0668 = 0.1336\nConclude: p &gt; \\(\\alpha\\), therefore do not reject. The p-value is not significant.\n\n\nCritical Values\nFor a two-sided test at the 5% level, what is the largest test statistic that would not be rejected?\nSince it’s a two-sided test at 5%, we would reject anything in the 2.5% area in either tail. Using the Z-table (or qnorm(0.05/2)), this would come from a test-statistic of 1.96. So if our test stat is 1.97, it would have a p-value below 0.05, and if it’s 1.95 it would have a p-value above 0.05.\nIn hypothesis testing, the critical value denotes the point at which z statistics16 are significant. If your z statistic is larger than 1.96, it will be statistically significant at the 5% level (for a two-sided test). This way, we can test significance without even calculating the p-value. Our conclusion will simply be that \\(p&lt;0.05\\), but this is often sufficient - it’s not important if the p-value is 0.044 versus 0.045.17\n\n\nHard Exam-Style Question\n\nA study reported that their two-sided p-value for \\(H_0:\\mu = 0\\) was significant at the 5% level, but not the 1% level.\nThey reported a mean of 10 and a sample size of 36\n\nWhat values could their standard deviation be?\nSolution:\n\nAt the 5% level, \\(z^* = 1.96\\), so:\n\n\\(1.96 = \\frac{x - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{10 - 0}{\\sigma/6}\\)\nRearranging, \\(\\sigma= \\frac{6*10}{1.96} = 30.61\\)\nSanity check: pnorm(10, 0, 30.61/sqrt(36)) = 0.975, as expected.\n\nAt the 1% level, \\(z^*\\) = -qnorm(0.01/2) = 2.576\n\n\\(\\sigma= \\frac{6*10}{2.576} = 23.292\\)\nSanity check: pnorm(10, 0, 23.292/sqrt(36)) = 0.995, as expected.\n\n\nConclusion: The standard deviation is between 23.3 and 30.6.\nIn this example, notice that a smaller standard deviation means a smaller significance level!\n\n\nCI vs. p-value\nRecall the following two facts:\n\nCI: \\(\\mu\\) is in the interval \\(\\bar x \\pm z^*\\sigma/\\sqrt{n}\\)\nTest statistic: \\(z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}\\)\n\nAs homework, rearrange the test statistic equation for \\(\\mu_0\\).\nA new definition of confidence intervals: A \\((1-\\alpha)\\)% CI contains every \\(\\mu_0\\) that would NOT be rejected by a test at the \\(\\alpha\\)% significance level.\nThis is why we don’t say that we “accept” the null hypothesis. There are an infinite number of hypothesis values in the CI - we can’t “accept” them all!18"
  },
  {
    "objectID": "L13-p_vals.html#self-study-questions",
    "href": "L13-p_vals.html#self-study-questions",
    "title": "10  Tests of Significance",
    "section": "10.4 Self-Study Questions",
    "text": "10.4 Self-Study Questions\n\nExplain the logic behind hypothesis testing in your own words. Make particular reference to the “at least as extreme as” part of the definition of a p-value.\nExplain why p-values are sample statistics.19\nWhat happens if a sample or study design is biased? In particular, suppose that the sample will systematically result in higher values that the population, and we’re testing \\(H_A:\\mu &gt; \\mu_0\\). What happens to the p-value?20\nFor CIs, I was adamant that we cannot speak of the probability that the population mean is inside the interval. We have now learned about the duality of CI and Hypothesis Testing, but we can speak of probability for test statistics21. What gives?22\nSuppose we are testing \\(H_A:\\mu &gt; 10\\) and we get a sample statistic \\(\\bar x = 10\\). What would the p-value for this be?\nFor a one-sided hypothesis test, what does it mean for our p-value to be larger than 0.5? Does this mean we did something wrong?23"
  },
  {
    "objectID": "L13-p_vals.html#footnotes",
    "href": "L13-p_vals.html#footnotes",
    "title": "10  Tests of Significance",
    "section": "",
    "text": "Unless it’s a bad sample/study design↩︎\n“trail”↩︎\nThese numbers actually come from the data of pre-flight trails, but we’re going to treat them as the population for now.↩︎\n15.9 is 3 steps of 0.3 above 15; (15.9 - 15)/0.3 = 3↩︎\nThis is the definition. The description must always include the part about “assuming that the hypothesized value is the true value”↩︎\nWe can only use a standard normal distrubution because the mean of the sampling distribution is assumed to be \\(\\mu_0\\), our hypothesized mean. If this weren’t the case, then we would not get a standard normal distribution and thus we wouldn’t be able to use this method. This is why the “assuming the null is true” bit is important.↩︎\nThe symbol \\(\\alpha\\) refers to the significance level, but also comes up in a \\((1-\\alpha)\\)%CI. Perhaps this is foreshadowing.↩︎\nPlease read the APA’s statement on p-values, found on OWL. At least one short answer question will be based on this.↩︎\nAgain: if our guess is incompatible with our data, then it’s our guess that’s wrong, not the data.↩︎\nNotice how this conclusion brings back the context of the question.↩︎\nLabelled \\(z_{obs}\\).↩︎\nWe used \\(&gt;\\) rather than \\(&lt;\\) because \\(&gt;\\) appears in our alternate hypothesis.↩︎\nIf you do this and find a p-value that is larger than 1, you used the wrong tails!↩︎\nIn the app, it’s denoted “Use absolute value”. This is because you can find \\(P(Z &gt; |z_{obs}|)\\) so that you always get the upper tail↩︎\nThat is, at the \\(\\alpha=0.1\\) significance level.↩︎\n\\(z_{obs}\\)↩︎\nIf we had taken a different sample, we would have gotten a different p-value - p-values have a sampling distribution as well!!!↩︎\nAlso, our tests only work in reference to the alternate hypothesis. We can only reject/not reject in reference to \\(H_A\\).↩︎\nThis implies that p-values have sampling distributions!↩︎\nWhile you’re at it, what happens to the CI?↩︎\n“p-value” is literally short for “Probability Value”.↩︎\nHint: what are we calculating probabilities for?↩︎\nHint: refer to the previous question.↩︎"
  },
  {
    "objectID": "L14-Inference_Cautions.html#interpreting-p-values",
    "href": "L14-Inference_Cautions.html#interpreting-p-values",
    "title": "11  Special Topics in Inference",
    "section": "11.1 Interpreting p-values",
    "text": "11.1 Interpreting p-values\nA p-value is the probability of a result that is at least as extreme as the one we observed, given that the null hypothesis is true.\nIt’s a measure of evidence against the null. We assume that the null is true, then ask how likely our sample would be. There isn’t a problem with our sample, so if it’s unlikely then it must be our assumption that is wrong.\n\n\n\n\n\n\n“Given that the null hypothesis is true”\n\n\n\nAny interpretation of a p-value that does not assume that the null hypothesis is true is a bad interpretation."
  },
  {
    "objectID": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "href": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "title": "11  Special Topics in Inference",
    "section": "11.2 Statistical Versus Practical Significance",
    "text": "11.2 Statistical Versus Practical Significance\nSuppose a new drug claims to increase your lifespan significantly. Wow! That sounds great!\nAfter hearing this claim, you dig into the paper that made the claim, and found that the drug increases the average lifespan by 3 hours. How can they claim this was significant???\nThis is the difference between statistical and practical significance. The probability of a result at least as extreme, given that the null hypothesis is true, says nothing of how extreme the results are. A very small effect size can be statistically significant, even if it’s not a noticable change in practice.\nFurthermore, maybe this new drug costs $1,000 per day and has intense nausea as a side-effect. A statistically significant difference says absolutely nothing about the practical effects.\n\n\n\n\n\n\np-values were never meant to be the goal of a study.\n\n\n\nThey are yet another tool in the health researcher’s repertoire, meant to test whether data provide enough evidence against a very particular hypothesis."
  },
  {
    "objectID": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "href": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "title": "11  Special Topics in Inference",
    "section": "11.3 Choosing a Significance Level",
    "text": "11.3 Choosing a Significance Level\nTo build intuition, we start from the extremes:\n\nA significance level of \\(\\alpha = 0\\) will never reject the null hypothesis.\n\nNo amount of evidence will convince you.\n\nA siginficance level of \\(\\alpha = 1\\) will always reject the null hypothesis.\n\nAny evidence will convince you.\n\n\nWhen setting a confidence level, you must consider how much evidence you require. To quote Carl Sagan: “Extraordinary claims require extraordinary evidence.”\nHere are some examples:\n\nA new cancer treatment costs 10 times as much and the patient will never heal back to 100% health. To justify such a procedure, we want to be reeeeeaaaaalllly sure that it works, so we might set a significance level of \\(\\alpha=0.001\\) before we collect any data.\nWe are making a slight change to an advertising strategy that is based on scientific evidence. We’re fairly certain it will work, and the consequences of it not working are very small. A larger significance level, perhaps \\(\\alpha = 0.1\\), might be appropriate.\n\n\\(\\alpha = 0.1\\) is probably the largest significance level you will ever encounter.\n\n👽 Aliens example: There’s an abberation in an image taken by a digital camera. According to the manufacturer, such an abberation would occur in 0.0001% of the pictures. If we think it’s aliens, that’s an extraordinary claim! We’d need some very strong evidence. Do you think that 0.0001% is strong enough? (We’ll return to this later in the chapter.)"
  },
  {
    "objectID": "L14-Inference_Cautions.html#hypothesis-errors",
    "href": "L14-Inference_Cautions.html#hypothesis-errors",
    "title": "11  Special Topics in Inference",
    "section": "11.4 Hypothesis Errors",
    "text": "11.4 Hypothesis Errors\nWhen you test a hypothesis, there are two types of errors: You could reject when the null is true or you could fail to reject when the null is false. The following matrix summarises this:\n\n\n\n\nH_0 is TRUE\nH_0 is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nGood!\n\n\n\nIn other words:\n\nType 1: False Positive\nType 2: False Negative\n\nThere’s another important point here: rejecting the null hypothesis does not mean that it’s actually false! Any number of things might have happened, such as including an outlier or taking a biased sample.\nSimilarly, failing to reject a null does not mean that it’s true. We’ve already talked about this a bit - confidence intervals are all values that would not be rejected by a hypothesis test, so there are many plausible null hypotheses! However, we can also fail to reject the null even though it’s false. This can also happen for multiple reasons:\n\nSample size is too small.\n\nThe distance between the null and the sample mean is calculated relative to the standard error. The standard error decreases with a larger sample, so if our sample isn’t big enough then we might not have collected enough evidence to reject the null, regardless of whether it’s true.\n\nLarge variability in the data.\n\nThis is the other thing that can increase the standard error. With more variation, the distance between the null and the sample mean doesn’t seem as large!\nWe can fix this with better sampling strategies and with better study designs, or by getting a larger sample size!\n\nOur significance level is too high.\n\nThis isn’t really something that we can change after we’ve seen the data.1\n\n\n\nThe Probability of Type 1 Errors\nWhat’s the probability your reject the null, even though it’s true? Let’s say we reject the null if the p-value is, say, less than 5%. This means that any value in the 5% tails of the distribution would lead to us rejecting the null hypothesis - even though it’s true!2 The probability that we do this is 5%, since there’s a 5% chance that we’ll see a value that is “too unlikey” at the 5% level.\nAs usual, I like to demonstrate things via simulation. Here’s the setup:\n\nSet the population parameters as \\(\\mu = 0\\) and \\(\\sigma = 1\\)\nSimulate normal data\nDo a two sided test for \\(H_0: \\mu = 0\\)\n\nNote that this null hypothesis is TRUE\n\nCount how many times we reject the null.\n\n\nset.seed(21); par(mar = c(2,2,1,1)) # unimportant\n## set an empty vector, to be filled with p-values\npvals &lt;- c() \n\nfor(i in 1:10000){ # repeat 10,000 times\n    # Simulate 30 normal values with a population mean of 0 and sd of 1\n    newsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n    # Test whether the population mean is 0\n    newsample_mean &lt;- mean(newsample)\n    newsample_sd &lt;- 1/sqrt(30) # Assuming population sd is known\n    my_z_test &lt;- 2 * (1 - pnorm(abs(newsample_mean), mean = 0, sd = newsample_sd))\n    # record the p-value (the output of t.test has some hidden values)\n    pvals[i] &lt;- my_z_test\n}\n## Testing at the 5% level\nsum(pvals &lt; 0.05) / length(pvals) # should be close to 0.05\n\n[1] 0.0481\n\n\nSince we’re testing at the 5% level, this value is close to 5%! It’s a little tricky to get your head around: If we think 5% is too unlikely, then we reject the null. However, things that have “only” a 5% chance happen about 5% of the time!\nThe histogram below shows all of the p-values we generated. The 5% cutoff isn’t anything special - a test at the 10% level will falsely reject the null 10% of the time. A test at the 90% level will falsely reject the null 90% of the time!\n\n## Fun fact: under the null hyothesis, all p-values are equally likely\n## this fun fact is not relevant to this course.\nhist(pvals, breaks = seq(0, 1, 0.05))\nabline(v = 0.05, col = \"red\", lwd = 3)\n\n\n\n\n\n\nThe Probability of Type 2 Errors\nFor a two-sided test, our hypotheses are: \\[\\begin{align*}\nH_0: \\mu &= \\mu_0\\\\\nH_A: \\mu &\\ne \\mu_0\\\\\n\\end{align*}\\]\nIf the null is actually false3, what’s \\(\\mu\\)? All we know is that it isn’t \\(\\mu_0\\).4 It could be a little above \\(\\mu_0\\), in which case it might be hard to reject \\(\\mu_0\\). It could be a far above \\(\\mu_0\\), in which case it might be easy to reject \\(\\mu_0\\).\n\nEx1: \\(\\mu = 0.001\\), \\(\\sigma = 1\\), and \\(\\mu_0 = 0\\).\n\nHard to reject \\(\\mu_0\\) since it’s so close to \\(\\mu\\) (low power)\nEasy to not reject the false \\(\\mu_0\\) (high probability of type 2 error)\nMost \\(\\bar x\\)’s would be close to \\(\\mu_0\\), relative to the standard error.\n\nEx2: \\(\\mu = 0.001\\), \\(\\sigma = 0.00000001\\), and \\(\\mu_0 = 0\\).\n\nEasy to reject \\(\\mu_0\\) since it’s so far from \\(\\mu\\) (high power)\nHard to not reject the false \\(\\mu_0\\) (low Type 2)\n\n\nPower is our ability to correctly reject a false null hypothesis, and is defined as 1 - P(Type 2 Error)\nNote that these examples are both missing the Standard Error, which incorporates sample size. The power depends on the distance between \\(\\mu\\) and \\(\\mu_0\\) relative to the standard error, not just the population standard deviation. We can partially control the standard error by having a better study design5 and a larger sample size, both of which would give us more power.\n\n\nPower by Simulation (DIY)\nThe following code calculates the power. Run it many times, changing \\(\\mu_0\\), \\(\\sigma\\), and \\(n\\) to see what happens to the power.\n\n## Set parameters\nmu &lt;- 0 # don't change this, but change the other parameters\nmu_0 &lt;- 0.1\nsigma &lt;- 0.5\nn &lt;- 50\n\n## Record p-vals\np_vals &lt;- c()\nfor(i in 1:10000){\n    newsample &lt;- rnorm(n, mu, sigma)\n    p_vals[i] &lt;- 2 * (1 - pnorm(abs(mean(newsample)), 0, sigma/sqrt(n)))\n}\n## The proportion of times the null was (correctly) rejected\nmean(p_vals &lt; 0.05) # Power\n\n[1] 0.0483\n\nmean(p_vals &gt; 0.05) # P(Type 2 Error)\n\n[1] 0.9517"
  },
  {
    "objectID": "L14-Inference_Cautions.html#multiple-comparisons",
    "href": "L14-Inference_Cautions.html#multiple-comparisons",
    "title": "11  Special Topics in Inference",
    "section": "11.5 Multiple Comparisons",
    "text": "11.5 Multiple Comparisons\nSuppose we have a coin that’s heads 5% of the time. What’s the probability of at least one heads in 10 flips?\nAs we saw in previous lectures: P(at least 1 heads in 10 flips) = 1 - P(no heads in 10 flips). We can calculate this in R:\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nWhy did I do go back to flipping coins? Did I forget which chapter I’m in?\nConsider the following problem:\nSuppose you’re testing 10 hypotheses at the 5% level. Assuming all of the null hypotheses are true, what’s the probability that at least one of them is significant?\nSince we’re testing at the 5% level, P(Type 1 Error) = 0.05, so\nP(\\(\\ge\\) 1 rejection in 10 hypotheses) =\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nIn other words, there’s about a 40% chance that we’d get at least one significant result even though all of the null hypotheses are true.6\n\n\n\n\n\n\nThe Multiple Comparisons Problem\n\n\n\nWhen checking more than one hypothesis, the probability of an error increases!\nThis happens for both Type 1 and Type 2 errors, but is especially important for Type 1 errors. If you test \\(n\\) errors at the \\(\\alpha\\)% level, then the probability of a Type 1 error is \\(1 -(1 - \\alpha)^n\\).\n\n\nSo how do we avoid the multiple comparisons problem? There are generally two ways to do it:\n\nSet a Family-Wise Error Rate, rather than an error rate for individual hypothesis tests.\n\nIf you’re going to check 10 p-values, use a smaller cutoff.\nThere are several ways to do this, with the most popular being the Bonferroni correction: for m values, a cutoff of \\(\\alpha/m\\) will result in rejecting at least one test \\(\\alpha\\)% of the time. For example, if you want a test at the 5% level but you’re testing 10 values, you should reject any individual hypothesis only if the p-value is less than \\(\\alpha/m = 0.05/10\\).\n\nOnly check one p-value!\n\nFor most studies, you should have single, well-defined hypothesis. State this hypothesis ahead of time, do all of your data preparation and get it loaded into R, then only test that hypothesis.\nIf you check a second hypothesis, then your significance level is a lie! Testing two true null hypotheses at the 5% level will result in a significant result 9.75% of the time.\n\n\nFailure to account for multiple hypothesis testing is bad science and it’s a path to the dark side. Consider this fantastic tool by fivethirtyeight. Play around with it - by checking a bunch of hypotheses, you can hack your way into finding one that supports your own point of view! In this particular example, your goal is to prove that either (a) the economy does better when a democrat in the white house or (b) the economy does better when a republican is in the white house. Both of these can be demonstrated with a statistically significant result if you check enough hypotheses!\nLet’s return to the 👽 aliens example. We observed an abberation that only happens in 0.0001% of the pictures taken. However, we took thousands of pictures! Even though this event is rare, it had many chances to happen. This is exactly what multiple hypothesis testing is demonstrating: rare events will happen if you give them enough chances! Rejecting the null when it is actually true is a rare event, but it can easily happen if we check a lot of p-values!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#summary",
    "href": "L14-Inference_Cautions.html#summary",
    "title": "11  Special Topics in Inference",
    "section": "11.6 Summary",
    "text": "11.6 Summary\n\nType 1 Error: Reject a true null\n\nProbability is \\(\\alpha\\)\n\nType 2 Error: Fail to reject a false null\n\nProbability depends on the distance between \\(\\mu\\) and \\(\\mu_0\\), relative to the standard error. In more advanced classes, you will calculate this or have something to calculate it for you.\n\nMultiple comparisons problem: The more hypotheses you test, the more likely it is that at least one of them is falsely labelled significant.\n\nTo prevent this, stop checking so many p-values or adjust your expectations!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#self-study-questions",
    "href": "L14-Inference_Cautions.html#self-study-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.7 Self-Study Questions",
    "text": "11.7 Self-Study Questions\n\nWe set up the null hypotheses as “nothing interesting is going on”. In light of this, explain why power is a good thing.\nIf we’re testing 5 hypotheses, what significance level should we use for each such that P(at least one type 1 error) = 0.05?\nIn simple (non-mathy) terms, explain why increasing sample size increases power.\nIf we have the hypotheses \\(H_0:\\mu = 1\\) versus \\(H_0:\\mu = 2\\), we can directly calculate the power. Run the following code to open the Shiny app, and interpret the results.\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/SimplePower\")"
  },
  {
    "objectID": "L14-Inference_Cautions.html#footnotes",
    "href": "L14-Inference_Cautions.html#footnotes",
    "title": "11  Special Topics in Inference",
    "section": "",
    "text": "We can think long and hard about it before seeing the data, but once we see the data we are commited to a particular significance level. Anything else is borderline fraud, depending on the circumstances.↩︎\nRecall: the p-value is the probability of a result that is at least as extreme assuming that the null hypothesis is true!↩︎\nFalse in the population, not just rejected due to a sample.↩︎\nI once saw a bag in a grocery store with a label that said “It’s Not Bacon”. I had no idea what was in that bag. In the video I said it was kale, but that turns out to be false.↩︎\nto reduce \\(s\\)↩︎\nNote that if the hypotheses are all based on the same data then they’re probably not independent.↩︎"
  },
  {
    "objectID": "L15-CI_for_Means.html#recap",
    "href": "L15-CI_for_Means.html#recap",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.1 Recap",
    "text": "12.1 Recap\n\nSilly confidence intervals\nIf \\(X\\sim N(\\mu,\\sigma)\\), where \\(\\sigma\\) is known, then a \\((1-\\alpha)\\)CI for \\(\\mu\\) based on \\(\\bar x\\) is: \\[\n\\bar x \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(z^*\\) is found such that \\(P(Z &lt; -z^*) = \\alpha/2\\),\n\nor we could have found \\(z^*\\) such that \\(P(Z &gt; z^*) = \\alpha/2\\),\nor \\(P(Z &lt; z^*) = 1 - \\alpha/2\\),\nor \\(P(Z &gt; -z^*) = 1 - \\alpha/2\\).\n\nA natural question is: why not use \\(s\\), the sample standard deviation?\nTo demonstrate why we can’t just use \\(s\\), I have set up a simulation. I like simulations.\n\n\nSimulation Setup\n\nTake random values from the standard normal distribution.\nCalculate the mean and sd.\nCalculate the 95% confidence interval with \\(\\sigma\\) and with \\(s\\), both using a \\(z\\) value.\nRecord whether the population mean is in the interval.\nCount how many intervals contain the population mean.\n\nShould be 95% of them!\n\n\nBefore we begin, I want to show some R code for finding confidence intervals. If you’re given that \\(\\bar x = 7.28\\), \\(n=15\\), \\(\\sigma = 1.24\\), and you want to calculate a 95% CI:1\n\nz_star &lt;- abs(qnorm(0.05/2))\nlower_bound &lt;- 7.28 - z_star*1.24/sqrt(15)\nupper_bound &lt;- 7.28 + z_star*1.24/sqrt(15)\nc(lower_bound, upper_bound)\n\n[1] 6.652485 7.907515\n\n\nAlternatively, we can use c(-1, 1) to stand in for “\\(\\pm\\)”. The code is a little weird to get your head around, but trust me - it works!\n\n7.28 + c(-1, 1)*z_star*1.24/sqrt(15)\n\n[1] 6.652485 7.907515\n\n\nSuppose that, unbeknownst to us, the true population mean was 7. To check if this is in our calculated confidence interval, we have to check that it’s larger than the lower bound AND less than the upper bound:\n\n7 &gt; 7.28 - z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n7 &lt; 7.28 + z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n\nThis can be combined into code as follows:\n\n(7 &gt; 7.28 - z_star*1.24/sqrt(15)) & (7 &lt; 7.28 + z_star*1.24/sqrt(15))\n\n[1] TRUE\n\n\nThis is enough to set up the simulation. Basically, we’re going to generate a random data set from a known population, then check if the confidence interval contains the true mean. We’ll do this thousands of times, and check which proportion contain the true mean. We’re hoping it’s 95%!\n\n\nSimulation Code\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05 / 2))\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor (i in 1:100000) { # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1) * z_star * pop_sd / sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1) * z_star * samp_sd / sqrt(n)\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.94887\n\nmean(s_does)\n\n[1] 0.92991\n\n\nThe CI based on \\(s\\) only contains \\(\\mu\\) 93% of the time! This is a pretty big discrepancy. What happens when you increase the sample size, n?2\nThe reason for this discrepancy is shown in the next section:"
  },
  {
    "objectID": "L15-CI_for_Means.html#the-variance-has-variance",
    "href": "L15-CI_for_Means.html#the-variance-has-variance",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.2 The Variance has Variance",
    "text": "12.2 The Variance has Variance\nRecall that the Sampling distribution is all possible values of a statistic when sampling from a population. We’ve covered the sampling distribution for the sample mean: Every time you take a sample, you get a different mean. The distribution of these sample means is \\(N(\\mu,\\sigma/\\sqrt{n})\\).\nThe same idea applies to the sample variance! Every time you take a sample, you get a different variance. The sampling distribution is not a normal distribution. In the next section, we’ll demonstrate this fact.\n\nSimulation: sample statistics\nI’m going to generate a bunch of samples from a \\(N(0, 0.2)\\) distribution. I’ll calculate the mean and variance from each distribution, then plot the histogram.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_means &lt;- c()\nsample_vars &lt;- c()\n\nfor (i in 1:100000) {\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    sample_means[i] &lt;- mean(new_sample)\n    sample_vars[i] &lt;- var(new_sample)\n}\n\npar(mfrow = c(1, 2))\nhist(sample_means, breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Means\")\ncurve(dnorm(x, pop_mean, pop_sd / sqrt(n)), add = TRUE,\n    col = 4, lwd = 2)\n## (n-1)s^2/sigma^2 follows a chi-square distribution on\n## n-1 degrees of freedom. If you understand this, you are\n## far too qualified to be taking this course. This fact\n## is outside the scope of the course.\nhist(sample_vars * (n - 1) / (pop_sd^2), breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Vars\")\ncurve(dchisq(x, n - 1), add = TRUE, col = 2, lwd = 2)\n\n\n\n\nAs you can tell from the fact that I knew how to draw the correct curve on the plots, the sampling distributions for the mean and variance are well known. Also, the sampling distribution for the variance is skewed, and therefore cannot be normal!\nWhen we use \\(\\bar x+ z^*s/\\sqrt{n}\\), \\(\\bar x\\) has variance, but so does \\(s\\).3 This is why the CI changes. When we know \\(\\sigma\\), the Margin of Error (MoE) is always the same. When the standard deviation changes for each sample, so does the MoE.\n\n\nSimulation: The Distribution of the Margin of Error\nThe sampling distribution of the Margin of Error is interesting to look at. This section is entirely optional - you just need to know that each sample has a different margin of error.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_MoEs &lt;- c()\nz_star &lt;- abs(qnorm(0.5/2))\n\nfor(i in 1:100000){\n    new_sample &lt;- rnorm(n=n, mean=pop_mean, sd=pop_sd)\n    sample_MoEs[i] &lt;- z_star*sd(new_sample)/sqrt(n)\n}\n\nhist(sample_MoEs, breaks = 25,\n    main = \"Sampling Dist of MoE\")\nabline(v = z_star*pop_sd/sqrt(n), col = 6, lwd = 2)\n\n\n\n\nThe vertical purple line is \\(z^*\\sigma/\\sqrt n\\).4 This is just a re-scaling of the sampling distribution of the sample variance, so it’s also skewed! Furthermore, the average MoE using \\(s\\) is smaller than the MoE using \\(\\sigma\\), even though it’s right-skewed:\n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Average MoE (s)\" = mean(sample_MoEs))\n\n    MoE (sigma) Average MoE (s) \n     0.04265848      0.04148352 \n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Median MoE (s)\" = median(sample_MoEs))\n\n   MoE (sigma) Median MoE (s) \n    0.04265848     0.04104300 \n\n\nThis is why the CI using \\(s\\) doesn’t capture the true mean as often - it’s giving us smaller intervals!"
  },
  {
    "objectID": "L15-CI_for_Means.html#removing-the-silliness",
    "href": "L15-CI_for_Means.html#removing-the-silliness",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.3 Removing the Silliness",
    "text": "12.3 Removing the Silliness\nThe distribution of the sample variance is not important.5 Instead, we care about the confidence intervals.\nI’m going to write this yet again: since \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\)), \\[\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\n\\] That is, you take the sample means, subtract the mean of the means, and divide by the standard error6, and you get a standard normal distribution.7\nOn the other hand, if we use \\(s\\) (which has it’s own variance), \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\n\\] where \\(n-1\\) is the degrees of freedom (or df).8 This is called the \\(t\\) distribution, and is a lot like the normal distribution but it has higher variance.\nBefore we move on, notice how the formula with \\(\\sigma\\) results in N(0,1), which does not require any information for our sample. In the \\(t\\) distribution, we need to know the sample size!\n\nThe t distribution\nThere are two main features of the \\(t\\) distribution that I want you to know:\n\nIt’s centered at 0, just like N(0,1).\nIt’s more variable than the normal distribution.\n\nThe second point is demonstrated in the following plot:\n\n\n\n\n\nThe red line corresponds to a sample size of 2.9 As the colours move through red to blue, we increase the sample size. At \\(df = \\infty\\), the \\(t\\) distribution is exactly the same as the N(0,1) distribution. For anything smaller, the \\(t\\) distribution puts more probability in the tails.\nThis shows up in the critical values:\n\nabs(qnorm(0.05/2)) # z^*\n\n[1] 1.959964\n\nabs(qt(0.05/2, df = 15 - 1)) # t^* n = 15\n\n[1] 2.144787\n\nabs(qt(0.05/2, df = 30 - 1)) # n = 30\n\n[1] 2.04523\n\nabs(qt(0.05/2, df = 50 - 1)) # n = 50\n\n[1] 2.009575\n\n\nNote that, just like how qbinom finds the value such of a binomial distribution such that 0.025% of the distribution is to the left and qnorm finds the z-values such that 0.025 is to the left, qt10 finds the t-value.\n\nn_seq &lt;- seq(2, 100, by = 2)\nt_seq &lt;- abs(qt(0.05/2, df = n_seq-1))\nplot(n_seq, t_seq, type = \"b\",\n    ylab = \"abs(qt(0.05/2, df = n - 1))\",\n    xlab = \"n\",\n    # the code for the title is not important.\n    main = bquote(\"As df -&gt; infinity, t\"^\"*\"*\" -&gt; z\"^\"*\"))\nabline(h = abs(qnorm(0.05/2)), col = 3, lwd = 2)\n## this code just puts a label on the axis - not important\naxis(2, abs(qnorm(0.05/2)), \"z*\", col = 3, font = 2, col.axis = 3)\n\n\n\n\nSince there’s more probability in the tails, you have to go further out to find the point such that 0.025 of the distribution is to the left.11 The \\(t\\) distribution allows for more variance due to the variance of \\(s\\), and it does this by having larger critical values.\n\n\nThe \\(t\\)-distribution\nThe \\(t\\) distribution has higher variance than the Normal distribution due to the extra uncertainty in estimating \\(s\\)."
  },
  {
    "objectID": "L15-CI_for_Means.html#the-t-confidence-interval",
    "href": "L15-CI_for_Means.html#the-t-confidence-interval",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.4 The \\(t\\) Confidence Interval",
    "text": "12.4 The \\(t\\) Confidence Interval\nNow that you understand the reasoning behind using wider confidence intervals, I can show you the formula/ \\[\n\\bar x \\pm t_{n-1}^*s/\\sqrt{n}\n\\]\nwhere \\(t^*_{n-1}\\) comes from abs(qt(alpha/2, df = n-1)).12\nThis has the same interpretation as the Z CI: 95% of the intervals constructed this way will contain the true population mean. This does NOT mean that there’s a 95% chance that the interval contains the true mean.\nWhat’s that? Of course, I can demonstrate by simulation! Thanks for asking! The following code is copied and pasted from above, only the critical value has been changed.\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, n - 1)) # NEW\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor(i in 1:100000){ # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1)*z_star*pop_sd/sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1)*t_star*samp_sd/sqrt(n) # NEW\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.95069\n\nmean(s_does)\n\n[1] 0.95091\n\n\nNow both of them contain the mean 95% of the time!13 The difference between them is that the t CI doesn’t have as much information as the Z CI - the Z CI knows what the population sd is, but the t CI doesn’t. This is kinda magical: using math, we can get the truth with fewer assumptions!"
  },
  {
    "objectID": "L15-CI_for_Means.html#examples",
    "href": "L15-CI_for_Means.html#examples",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.5 Examples",
    "text": "12.5 Examples\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(\\sigma = 0.01\\), find the 92%CI.\n\nThis is a bit of a trick: I gave you \\(\\sigma\\)! This always refers to the population standard deviation, so that’s what it is here. The Z CI can be found with the R code:\n\n\n\n0.4 + c(-1, 1)*abs(qnorm(0.08/2)) * 0.01/sqrt(100)\n\n[1] 0.3982493 0.4017507\n\n\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(s = 0.01\\), will a 92%CI be wider than or smaller than the CI from Example 1?\n\nWe use \\(t\\) to account for the extra variance we have when we estimate \\(s\\). More variance means wider tails! The CI will be wider!\n\n\n\n0.4 + c(-1, 1)*abs(qt(0.08/2, df = 100-1)) * 0.01/sqrt(100)\n\n[1] 0.3982312 0.4017688\n\n\nIt’s only slightly wider. The sample size is large enough that the variance in the estimate is small.14 Try this again with a smaller \\(n\\) and see what happens to the difference!\n\nIf \\(n=16\\) and the 95%CI for \\(\\mu\\) is (10, 15), what’s the variance?\n\nA general form of the CI is \\(\\bar x \\pm t^* s/\\sqrt{n}\\).\n\n\\(\\bar x\\) is in the centre, so \\(\\bar x\\) is 12.5\n\nThe MoE is 2.5, so \\(t^* s/\\sqrt{n} = 2.5\\).\n\n\\(t^*\\) is qt(0.05/2, 16 - 1) = 2.131\n\n\\(2.131s/\\sqrt{16} = 2.5\\), so \\(s = 2.5\\sqrt{16}/2.131 = 4.69\\)\nThe variance is \\(4.69^2 = 21.9961\\)"
  },
  {
    "objectID": "L15-CI_for_Means.html#summary",
    "href": "L15-CI_for_Means.html#summary",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.6 Summary",
    "text": "12.6 Summary\nThis lesson could have been two sentences: The sample standard deviation has variance, so each confidence interval based on \\(s\\) is slightly different. To account for this, we use the \\(t\\) distribution. Then again, when someone tells me their name at a party I immediately forget it. Hopefully this long-winded exploration helps you understand why these facts are true and how they’re relevant to the course.\nNote that all of the best practices for inference still apply! We can still get smaller intervals by taking better samples with larger sample sizes, and we still have to be careful to never speak of a calculated confidence interval in terms of chance.\nThe \\(t\\) confidence interval is actually used in practice. We’ll see some code that calculates the interval for us in the next lecture, and then we’ll never have to use qt() again! (Except possibly to demonstrate knowledge on tests.)"
  },
  {
    "objectID": "L15-CI_for_Means.html#footnotes",
    "href": "L15-CI_for_Means.html#footnotes",
    "title": "12  Confidence Intervals in Practice",
    "section": "",
    "text": "You’ll need to do this sort of thing on a test/assignment.↩︎\nRe-run the code and try it!↩︎\nBoth are random variables.↩︎\nRecall that this never changes since \\(\\sigma\\) is fixed.↩︎\nAnd very complicated.↩︎\nthe standard deviation of the sampling distribution↩︎\nThe word “standard” shows up way too much. Statisticians are bad at naming things.↩︎\nThis is another example of statisticians being bad at naming things.↩︎\nThe degrees of freedom is \\(n-1\\).↩︎\nThe person reading this is a cutie.↩︎\nTry this for other \\(\\alpha\\) values and larger \\(n\\).↩︎\nNote: I’m not even going to bother writing out the \\(P()\\) notation for \\(t^*_{n-1}\\) because you’ll never use it. You’ll only ever need to find \\(t^*_{n-1}\\) in this course.↩︎\nThis means it’s working!↩︎\nRecall: For both the sample mean and the sample proportion, the variance of the sampling distribution decreases as \\(n\\) increases.↩︎"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "href": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "title": "13  t-Tests for a Mean",
    "section": "13.1 When we use \\(s\\), we use \\(t\\)",
    "text": "13.1 When we use \\(s\\), we use \\(t\\)\nWe’ve been over this in confidence intervals, and the same thing applies to hypothesis tests! If the population is normal (or the sample size is large enough) and we have an SRS, then \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\]\nAgain, the \\(t\\) distribution is used to account for the extra variability from the estimated standard deviation.1\nThis means our test statistic is \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\]\nSince this is a \\(t\\) distibution, we use pt(t_obs, df = n -1), possibly one minus and/or double, depending on the alternate hypothesis.2\nThat’s it. That’s the big difference. When we estimate the standard deviation, we use the t-distribution.\n\n\n\n\n\n\nThe t-test for a population mean\n\n\n\nGiven a sample mean \\(\\bar x\\) and a sample standard deviation \\(s\\), our test statistic is: \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\] Our hypotheses and calculations of the p-value work the same as they did for the z-test."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#examples",
    "href": "L16-Hypothesis_Tests_for_Means.html#examples",
    "title": "13  t-Tests for a Mean",
    "section": "13.2 Examples",
    "text": "13.2 Examples\n\nPilot Fatigue\nIn the pilot fatigue example from the Understanding p-values lecture, we assumed that we had the population sd. I lied - it was actually a sample statistic! We should have used a t-test, not a z test.\nRecall:\n\n\\(H_0: \\mu = 15\\) versus \\(H_A: \\mu &gt; 15\\) with\n\\(n = 16\\), \\(\\bar x = 15.9\\), \\(s = 1.2\\) (not \\(\\sigma\\)) \\[\nt_{obs} = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 3\n\\]\n\nUsing the \\(t\\) distribution, our p-value is:\n\n1 - pt(3, df = 16)\n\n[1] 0.00423975\n\n\nThis is larger than our previous p-value of 0.0013. This will always be the case: if the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\n\n\n\n\n\n\np-values from a t-test are larger than a z-test (if you have \\(\\sigma=s\\))\n\n\n\nWe almost never know the population standard deviation, so we have extra uncertainty. With extra uncertainty, we require more evidence! Recall that a p-value is a measure of evidence against a null."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "href": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "title": "13  t-Tests for a Mean",
    "section": "13.3 Matched Pairs",
    "text": "13.3 Matched Pairs\nA matched pairs design allows us to use a one-sample t-test when it looks like we have two samples3. Since the pairs are matched, we can calculate the differences between pairs and treat this like a single vector of observations. It is honkey tonk ridonkulous to say that we know the true population standard deviation for the difference in observations, so a \\(z\\) test could never be appropriate.\nConsider the following example of a matched pairs experiment. Given a sample of brave volounteers, we create a small cut on both hands and put ointment on one of the two cuts4. This study design eliminates the variation in healing times for different people since both cuts are on the same person! For each individual, we observe a difference. That is, one observation per person!\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Ointment\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\nNote: Differences were calculated as With minus Without! The last row of this table now represents our data - we can forget that the other two rows exist!\nThis is where the assumption that we know the population standard deviation is especially preposterous: we’re looking just at the differences! Even if there’s a true value of the sd for healing time for all people, the standard deviation of the difference between healing times isn’t a reasonable quantity to speak of.\nSince we’re looking at the difference, we no longer have a hypothesized value of \\(\\mu_0\\). Instead, we hypothesize that the average pairwise difference is 0, i.e. \\(\\mu_{with-without} = \\mu_{diff} = 0\\)5. The alternative is “with” &lt; “without”, i.e. \\(\\mu_{diff} &lt; 0\\).6\n\nx &lt;- c(-0.78, 0.01, -0.33, -0.7, -0.2, 0.61, -0.87, -2.1)\nxbar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nt_obs &lt;- (xbar - 0)/(s/sqrt(n)) # xbar is with - w/out\n# Notice that we use pt() instead of pnorm()\npt(t_obs, df = n - 1) # Alternative is &lt;\n\n[1] 0.04662624\n\n\nSo our p-value is approximately 0.04. At the 5% level, the null hypothesis would be rejected and we would conclude that the ointment works7. At the 1% level, we would conclude that it doesn’t have a significant effect. This is why it’s important to know the significance level before calculating the p-value - we now get to choose whether our results are statistically significant!\n\nt-tests in Practice\nDo you think that researchers in the field are typing test statistics into their calculator? Of course not! We’re finally at the point in this class where the methods are so commonly used that the built-in functions in R can calculate them.\n\nwith_oint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.0, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4  , 6.7, 2.88, 7.8, 6.32)\ndifference &lt;- with_oint - without\nt.test(difference, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.04817\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n         -Inf -0.007063183\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that the output shows a one-sided confidence interval. This isn’t a big leap from what you know: a confidence interval consists of all of the values that would not be rejected by a hypothesis test, and this works for one-sided as well as two-sided alternate hypotheses!\nTo get a two-sided confidence interval, we can either leave alternative at it’s default value or set it to \"two.sided\". We can also change the significance level with the conf.level argument. For an 89%CI:\n\nt.test(difference, alternative = \"two.sided\", conf.level = 0.89)\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.09635\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n -1.04730428 -0.02519572\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that this calculated a two-sided p-value, which is twice what we saw before (and no longer significant at the 5% level!)."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "title": "13  t-Tests for a Mean",
    "section": "13.4 Self-Study Questions",
    "text": "13.4 Self-Study Questions\n\nExplain why: If the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\nIf a test is statistically signficant, does that mean there’s a large effect size? That is, does a hypothesis test tell you anything about the size of the effect?\n\nCompare this to confidence intervals.\n\nCan we interpret a \\(t\\) confidence interval as “all null hypothesis values that would not be rejected”?\nRe-do the ointment example, but using without - with.\n\nDraw a t distribution and mark the two test statistics, then fill in the area that corresponds to the p-value."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "href": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "title": "13  t-Tests for a Mean",
    "section": "",
    "text": "Which is used in the caclulation of the Estimated Standard Error.↩︎\nLike pnorm(), it always calculates the probability below the test statistic.↩︎\nWe’ll learn about two-sample t-tests in the next lecture.↩︎\nAnd most likely a bandage on both.↩︎\nIn other words, the healing times are the same for each subject↩︎\nThis is where it’s important to know that we did with minus without; we could have done without minus with, but then our alternate hypotheses would need to be “&gt;”.↩︎\nA p-value says nothing about the effect size, so we can’t say whether it’s practically significant↩︎"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "href": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "title": "14  Two-Sample t-Tests",
    "section": "14.1 How much can one more sample complicate things?",
    "text": "14.1 How much can one more sample complicate things?\n\nNotation: Subscripts everywhere!\nWe now have two samples.\n\\(\\bar X_1\\sim N(\\mu_1, \\sigma_1/\\sqrt{n_1})\\), where \\(s_1\\) is the estimated standard deviation of a given sample.\n\\(\\bar X_2\\sim N(\\mu_2, \\sigma_2/\\sqrt{n_2})\\) \nGoal: Are the means the same? I.e., is \\(\\mu_1 = \\mu_2\\)?\n\n\nHow one more sample complicates things\n\nWhat’s the sampling distribution of a difference?\nIt turns out, it’s complicated.\n\n\n\nThe mean of the difference\nHere’s the complicated bit: the mean of the difference is…\nthe difference in means. \\[\n\\bar X_1 - \\bar X_2 \\sim N(\\mu_1 - \\mu_2, ???)\n\\] \nOkay that was the easy part.\n\n\nThe standard deviation of a difference\n\n## approximating the sampling distribution\ndifferences &lt;- c()\nfor(i in 1:10000){\n    m1 &lt;- mean(rnorm(22, 0, 2))\n    m2 &lt;- mean(rnorm(33, 0, 3))\n    differences[i] &lt;- m1 - m2\n}\nsd(differences)\n\n[1] 0.6802144\n\n\n… it’s not at all obvious where this number comes from.\n\n\nThe standard deviation of a difference is…\nsomething like the sum\n\\[\nSE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\] \n\n## Check\nsd(differences)\n\n[1] 0.6802144\n\nsqrt(4/22 + 9/33) # Close enough\n\n[1] 0.6741999\n\n\n\n\nPutting it Together\n\\[\n\\bar X_1 - \\bar X_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\right)\n\\]\n\n\nAside: “Pooled Variance”\n\nAssumes the populations are perfectly normal and have the same variance.\nYou shouldn’t assume this."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#the-test-statistic",
    "href": "L17-Two_Sample_hypothesis_Tests.html#the-test-statistic",
    "title": "14  Two-Sample t-Tests",
    "section": "14.2 The Test Statistic",
    "text": "14.2 The Test Statistic\n\n\\(s\\) is estimated\n\\[\n\\frac{(\\bar X_1 - \\bar X_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}} \\sim t_{df=???}\n\\]\n\n\nThe degrees of freedom\n\nThere’s an ugly formula for the degrees of freedom. \nWe’re not going to use it.\n\nInstead, just use the smallest sample size, and subtract 1 (or use R).\n\n\nTwo-Sample t-test and CI\nWe are usually testing for the difference in means, i.e. \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_0: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\\[t_{obs} = \\frac{\\bar x_1 - \\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\n\\[\n\\text{p-value} = P(T &lt; t_{obs}) = \\texttt{pt(t\\_obs, df = min(n1, n2)-1)}\n\\]\nA \\((1-\\alpha)\\) CI for \\(\\mu_d\\) is \\[\n\\bar x_1 - \\bar x_2 \\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nwhere \\(t^*\\) is based on the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\)"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#examples",
    "href": "L17-Two_Sample_hypothesis_Tests.html#examples",
    "title": "14  Two-Sample t-Tests",
    "section": "14.3 Examples",
    "text": "14.3 Examples\n\nTwo-Sample versus Matched\nFrom the Ointment example:\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Oint\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\n\n\nIn R: Matched pairs (same as last lecture)\n\nwithoint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.01, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4, 6.7, 2.88, 7.88, 6.32)\ndiff &lt;- withoint - without\n\nt.test(x = diff, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -1.9421, df = 7, p-value = 0.04663\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n        -Inf -0.01332314\nsample estimates:\nmean of x \n   -0.545 \n\n\n\n\nBy “hand”: Two-sample t-test\n\nmdiff &lt;- mean(withoint) - mean(without)\nsediff &lt;- sqrt(var(withoint)/8 + var(without)/8)\n\npt(mdiff/sediff, df = 7)\n\n[1] 0.26042\n\n\n\n\nIn R: Two-sample t-test\n\nt.test(x = withoint, y = without, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  withoint and without\nt = -0.67584, df = 13.736, p-value = 0.2552\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf 0.8772719\nsample estimates:\nmean of x mean of y \n    5.155     5.700 \n\n\n\n\nConclusion\n\nIf you can have matched pairs, you should use a matched pairs test.\nMost of the time, you’ll need to use a two-sample t-test.\n\nDon’t get fooled by equal sample sizes!"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "title": "15  Large sample test for a proportion",
    "section": "15.1 Refresher",
    "text": "15.1 Refresher\nIn the lecture on sampling distributions, we learned that the sampling distribution of a sample proportion can be found as follows:\nIf \\(X\\sim B(n,p)\\) and \\(np&gt;10\\) and \\(n(1-p)&gt;10\\), then \\[\n\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\nThis relies on the population proportion to find the standard error, but this is never available.1 The standard error can be estimated and used in the sampling distribution: \\[\n\\hat p \\sim N\\left(p, \\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\\right)\n\\]\nI said that this would be the last time I change the standard error. I will now make a liar out of myself."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "title": "15  Large sample test for a proportion",
    "section": "15.2 Hypothesis tests for proportions",
    "text": "15.2 Hypothesis tests for proportions\nAs before, we write our hypotheses: \\[\nH_0:p = p_0 \\text{ vs. } H_A: p \\{&gt;or&lt;or\\ne\\} p_0\n\\]\nWe always write $H_0:p = $ and then fill in the value for \\(p_0\\), then we use that same value in the alternate hypothesis but use either \\(&gt;\\), \\(&lt;\\), or \\(\\ne\\) based on the wording of the question.\nAs before, we use the sampling distribution to find our p-value. In this case, though, we have a hypothesized value for the population proportion. In fact, we must assume that the null is true.2 If this is the case, we have the standard error!\nI swear, this is the last time I introduce a new standard error for the sampling distribution of the sample proportion. Assuming \\(H_0\\) is true (and the conditions are met), \\[\n\\hat p \\sim N\\left(p_0, \\sqrt{\\frac{p_0(1-p_0)}{n}}\\right)\n\\]\n\nThe Test Statistic\nAs you can guess from the sampling distribution, the test statistic is:\n\\[\nz_{obs} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]\nand then we can use the normal distribution as usual: \\[\nP(Z \\{&gt;or&lt;or\\text{ further away than}\\} z_{obs}) = \\dots\n\\]\nwhere we use \\(&gt;\\) if the alternate hypothesis uses \\(&gt;\\), \\(&lt;\\) if the alternate hypothesis uses \\(&lt;\\), and we look at the two tails if the alternate hypothesis is \\(\\ne\\).\nA common question is: which \\(p\\) do we use to check normality?\nFor a hypothesis test, we assume the null is true, i.e. \\(p=p_0\\). We should use this assumption everywhere! For a hypothesis test about a proportion, we check whether \\(np_0&gt;10\\) and \\(n(1-p_0)&gt;10\\)3."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "title": "15  Large sample test for a proportion",
    "section": "15.3 Example",
    "text": "15.3 Example\n\nMendelian Genetics\nTo test his theory that 75% of plants would inheret a dominant gene, Gregor Mendel cross bred pure breeds of pea plants. Out of 7324 plants, 5474 showed the dominant trait. At the 4.5% level, is this compatible with the hypothesis of 75% dominant?\nSolution:\n\nCheck: \\(np_0 = 7324*0.75 &gt; 10\\) and \\(n(1-p_0) = 7324*0.25 &gt; 10\\).\n\\(z_{obs} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.747 - 0.75}{\\sqrt{0.75*0.25/7324}} = -0.513\\)\n\\(p-val = 2 *P(Z &lt; z_{obs})\\) = 2*pnorm(-0.513) = 0.608\n\nWe doubled the \\(P(Z &lt; z_{obs})\\) because we want both tails. If you do this and your p-value is larger than 1, do \\(1 - P(Z &lt; z_{obs})\\) first and then double it.\n\nConclusion: Since \\(p-val &gt; \\alpha\\), we do not reject the null. The hypothesis that 75% of plants inherent the dominant trait is compatible with the data.\n\nThe last step is important: always word your conclusion in the context of the study.\n\n\nMendelian Genetics Part 2\nRecall from last lecture the duality of the CI and the hypothesis test. For this question, a 95.5%4 CI can be found as: \\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}} = 0.747 \\pm 2.005\\sqrt{\\frac{0.747(1-0.747)}{7324}}\n\\]\nwhich results in the CI (0.737, 0.758).\nNote that \\(\\hat p\\) was used in the SE since the CI is not based on a hypothesis. For proportions, it is not true that the CI contains all hypotheses that would not be rejected because the CI and the hypothesis test use different standard errors."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "title": "15  Large sample test for a proportion",
    "section": "15.4 Self-Study Questions",
    "text": "15.4 Self-Study Questions\n\nWhen do we use \\(\\hat p\\) in the standard error? When do we use \\(p_0\\)?\nExplain why we don’t estimate the standard error in a hypothesis test about a proportion."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "title": "15  Large sample test for a proportion",
    "section": "",
    "text": "f it was, then why are we doing inference?↩︎\nWe want to be strict about this so that it’s more convincing if we prove it wrong.↩︎\nAs before, both conditions must be true.↩︎\n\\(\\alpha = 0.045\\), so \\(1 - \\alpha = 0.955\\).↩︎"
  },
  {
    "objectID": "L19-CI_for_Proportions.html#introduction",
    "href": "L19-CI_for_Proportions.html#introduction",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\n\nThis is the same as the last video.\n\nBased on our data, we make an interval that we think describes the population.\nIn this case, we just have a different population distribution?\n\n\n\nAssumptions\nIn stats, assumptions give us power but only if they hold.\nAssumptions for a CI for \\(p\\) are the same as the assumptions for the binomial distribution, with the addition of an SRS."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#the-ci-for-p",
    "href": "L19-CI_for_Proportions.html#the-ci-for-p",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.2 The CI for \\(p\\)",
    "text": "16.2 The CI for \\(p\\)\n\nSampling Distribution of \\(\\hat p\\)\nAs we saw before the midterm, if the population is \\(B(n,p)\\), then under certain conditions,\n\\[\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\\]\n\n\nDeja-Vu\nSince \\(\\hat p \\sim N(p, \\sqrt{\\frac{p(1-p)}{n}})\\),\n\\[\n\\frac{\\hat p - p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]\nAgain, we can use the form \\(z = (x-\\mu)/\\sigma\\), but replace \\(x\\), \\(\\mu\\), and \\(\\sigma\\) with the correct values.\nA \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\]\n\n\nWe don’t know the variance, why not \\(t_{n-1}^*\\)?\n\nWe used \\(t_{n-1}^*\\) because we had to estimate \\(\\sigma\\)\nThere’s no \\(\\sigma\\) to estimate!\nThe variance of the Binomial distribution is entirely determined by \\(p\\)!\n\nBinom be crazy.\n\n\n\n\n… but Devan, we still don’t know \\(p\\)!\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\] which needs \\(p\\) in the second part of the equation.\nWhy not just plug in \\(\\hat p\\)?\nOkay fine. \n\\(\\sqrt{\\hat p(1-\\hat p)/n}\\) is called the estimated standard error, since its the sd of the sampling distribution, but it’s based on an estimate.\n\n\nFinal_Version_V2_Update_LastTry_Srsly.docx.pdf\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nwhere \\(z^*\\) is chosen such that \\(P(Z &lt; -z^*) = \\alpha/2\\).\n\n\nDevan Style: Simulation\n\nn &lt;- 100\np &lt;- 0.7\nSE_true &lt;- sqrt(p*(1-p)/n)\np_does &lt;- c()\nphat_does &lt;- c()\nthat_does &lt;- c()\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, df = n-1))\n\n\n\nDevan Style: Simulation\n\nfor(i in 1:10000){\n    new_sample &lt;- rbinom(n=1, size=n, prob=p)\n    phat &lt;- new_sample/n\n    SE_est &lt;- sqrt(phat*(1-phat)/n)\n    \n    pCI &lt;- phat + c(-1,1)*z_star*SE_true\n    phatCI &lt;- phat + c(-1,1)*z_star*SE_est\n    thatCI &lt;- phat + c(-1,1)*t_star*SE_est\n    \n    p_does[i] &lt;- pCI[1] &lt; p & pCI[2] &gt; p\n    phat_does[i] &lt;- phatCI[1] &lt; p & phatCI[2] &gt; p\n    that_does[i] &lt;- thatCI[1] &lt; p & thatCI[2] &gt; p\n}\n\n\n\nSimulation Results\n\nmean(p_does)\n\n[1] 0.9371\n\nmean(phat_does)\n\n[1] 0.9502\n\nmean(that_does)\n\n[1] 0.9502\n\n\nUsing the population proportion is… worse?\nDIY: Change \\(p\\) so that the normal approximation doesn’t apply."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#examlpes-and-cautions",
    "href": "L19-CI_for_Proportions.html#examlpes-and-cautions",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.3 Examlpes and Cautions",
    "text": "16.3 Examlpes and Cautions\n\nExample 1\nIt was found that 591 out of 700 people sampled supported a certain political position. Find a 91%CI.\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nn &lt;- 70\nphat &lt;- 68/70\nse_est &lt;- sqrt(phat*(1-phat)/n)\nz_star &lt;- abs(qnorm(0.09/2))\n\nphat + c(-1, 1)*z_star*se_est\n\n[1] 0.9376692 1.0051879\n\n\n… so it would be reasonable to say that the popluation proportion is larger than 1???\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nprop.test(x = 68, n = 70)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  68 out of 70, null probability 0.5\nX-squared = 60.357, df = 1, p-value = 7.912e-15\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.8913981 0.9950358\nsample estimates:\n        p \n0.9714286"
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#inference-is-the-inverse-of-probability",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#inference-is-the-inverse-of-probability",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.1 Inference is the Inverse of Probability",
    "text": "17.1 Inference is the Inverse of Probability\nThere is some true population parameter. When we collect a sample, we can a random collection of observations from this population parameter. In other words, we get randomness from a population value - this is Probability.\nIf we make assumptions about our data, we have the benefit of math. This math allows us to go backwards and say something aboot the population. In other words, we get a population value from randomess - this is Inference."
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-mean-sigma-known",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-mean-sigma-known",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.2 Inference about a Mean, \\(\\sigma\\) known",
    "text": "17.2 Inference about a Mean, \\(\\sigma\\) known\n\nAssumptions\n\nSRS from the population of interest\n\nThis ensures independence between observations\n\nPopulation is normal\nWe know \\(\\sigma\\)\n\n\n\nSampling Distribution\nSince \\(\\sigma\\) is known, we can standardize \\(\\bar X\\) as follows:\n\\[\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\n\\]\n\n\nConfidence Interval\nA \\((1-\\alpha)\\)CI can be found as:\n\\[\n\\bar x \\pm z^*\\frac{\\sigma}{\\sqrt n}\n\\]\nwhere \\(z^*\\) = abs(qnorm(alpha/2)).\nIf we take repeated samples from the population, 95% of the CIs that we construct this way will contain the true population mean. This is not a probability.\n\n## A 94.5% CI\nset.seed(2)\nsigma &lt;- 10\nalpha &lt;- 0.055\nx &lt;- rnorm(n = 30, mean = 4, sd = sigma)\nx\n\n [1]  -4.96914547   5.84849185  19.87845331  -7.30375674   3.19748243\n [6]   5.32420284  11.07954729   1.60301976  23.84473937   2.61212988\n[11]   8.17650751  13.81752777   0.07304644  -6.39668977  21.82228960\n[16] -19.11069085  12.78604581   4.35806718  14.12828692   8.32265155\n[21]  24.90819205  -7.99925820  19.89638200  23.54651642   4.04937777\n[26] -20.51706388   8.77237303  -1.96558169  11.92203270   6.89636710\n\nxbar &lt;- mean(x)\nn &lt;- length(x)\nzstar &lt;- abs(qnorm(alpha/2))\n\nxbar + c(-1,1)*zstar*sigma/sqrt(n)\n\n[1] 2.783345 9.790091\n\n\n\n\nHypothesis test\nPrior to collecting data, we have the hypotheses: \\[\\begin{align*}\nH_0:\\mu = \\mu_0\nH_A: \\mu &lt; \\mu_0\n\\end{align*}\\]\nWe also must set the significance level \\(\\alpha\\) prior to collecting data.\nOnce we collect data, we calculate the test statistic: \\[\nz_{obs} = \\frac{\\bar x - \\mu}{\\sigma/\\sqrt{n}}\n\\]\nSince our alterate hypothesis is “&lt;”, the p-value is found as: \\[\n\\text{p-value} = P(Z &lt; z_{obs})\n\\]\nIf \\(p &lt; \\alpha\\), we reject the null hypothesis. In our conclusion, we specify what this means in terms of the study.\n\n## Same x as before\n## testing if the population mean is less than 5\nzobs &lt;- (xbar - 5)/(sigma/sqrt(n))\npnorm(zobs)\n\n[1] 0.7595216\n\npnorm(zobs) &lt; alpha # If TRUE, reject null\n\n[1] FALSE\n\n\n\n\nR Code for z\nThere is none, because it’s bonkers to assume that the population sd is known. The z-test is taught for two reasons: to demonstrate the underlying mechanics, and to motivate sample size calculations (discussed later in this document)."
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-mean-sigma-estimated-by-s",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-mean-sigma-estimated-by-s",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.3 Inference about a Mean, \\(\\sigma\\) estimated by \\(s\\)",
    "text": "17.3 Inference about a Mean, \\(\\sigma\\) estimated by \\(s\\)\n\nAssumptions\n\nSRS from the population of interest\n\nThis ensures independence between observations\n\nPopulation is normal (or n is “large” enough)\n\n\n\nSampling Distribution\nSince \\(s\\) is estimated, there is extra variance in our test statistic. The sampling distribution is: \\[\\begin{align*}\n\\bar X &\\sim N(\\mu, \\sigma/\\sqrt{n})\\\\\nS &\\sim \\text{ Something Else}\\\\\n\\text{Therefore, } \\frac{\\bar X - \\mu}{S/\\sqrt n} &\\sim t_{n-1}\n\\end{align*}\\]\n\n\nConfidence Interval\n\\[\n\\bar x \\pm t_{n-1}^*s/\\sqrt{n}\n\\]\nwhere \\(t_{n-1}^*\\) = abs(qt(alpha/2, df = n-1)).\n\ns &lt;- sd(x)\ntstar &lt;- abs(qt(alpha/2, n - 1))\nxbar + c(-1, 1)*tstar*s/sqrt(n)\n\n[1]  2.001979 10.571458\n\n\n\n\nHypothesis Test\nPrior to collecting data, we have the hypotheses: \\[\\begin{align*}\nH_0:\\mu = \\mu_0\nH_A: \\mu \\ne \\mu_0\n\\end{align*}\\]\nWe also must set the significance level \\(\\alpha\\) prior to collecting data.\nOnce we collect data, we calculate the test statistic: \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\]\nSince our alterate hypothesis is “\\(\\ne\\)”, the p-value is found as: \\[\n\\text{p-value} = 2*(1-P(T &gt; |t_{obs}|))\\text{, or } 2*P(T &lt; -|t_{obs}|)\n\\] although this is best understood as checking the tails (i.e. if \\(t_{obs}\\) is negative, do \\(2*P(T &lt; t_{obs})\\) and if it’s positive do \\(2*P(T &lt; -t_{obs})\\)).\n\ntobs &lt;- (xbar - 5)/(s/sqrt(n))\ntobs\n\n[1] 0.6004565\n\n2*(1 - pt(abs(tobs), df = n - 1))\n\n[1] 0.5528661\n\n2*(1 - pt(abs(tobs), df = n - 1)) &lt; alpha # if TRUE, reject null\n\n[1] FALSE\n\n\n\n\nR Code for t\n\n## This gives the same CI and p-value as before\nt.test(x, mu = 5, alternative = \"two.sided\", conf.level = 1 - alpha)\n\n\n    One Sample t-test\n\ndata:  x\nt = 0.60046, df = 29, p-value = 0.5529\nalternative hypothesis: true mean is not equal to 5\n94.5 percent confidence interval:\n  2.001979 10.571458\nsample estimates:\nmean of x \n 6.286718"
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#two-sample-t-tests",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#two-sample-t-tests",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.4 Two-Sample t-tests",
    "text": "17.4 Two-Sample t-tests\n\nAssumptions\n\nAssumptions for one-sample t-tests hold for each sample\nSamples are indendent from each other.\n\nFor a two-sample t-test, we will NEVER assume that the standard deviation is known.\n\n\nSampling Distribution\n\\[\n\\bar X_1 - \\bar X_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\right)\n\\]\n\n\nConfidence Intervals\nA \\((1-\\alpha)\\) CI for \\(\\mu_d\\) is \\[\n\\bar x_1 - \\bar x_2 \\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nwhere \\(t^*\\) is based on the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\)\n\n\nHypothesis Tests\nWe are usually testing for the difference in means, i.e. \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_0: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\\[t_{obs} = \\frac{\\bar x_1 - \\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\n\\[\n\\text{p-value} = P(T &lt; t_{obs}) = \\texttt{pt(t\\_obs, df = min(n1, n2)-1)}\n\\]\n\n\nR Code for two-sample t-tests\nIn the code below, the null hypothesis is FALSE, but the effect size is small and the sample sizes aren’t large enough to detect a difference.\n\nx1 &lt;- rnorm(30, mean = 2, sd = 3)\nx2 &lt;- rnorm(30, mean = 2.1, sd = 3)\n\n## HA: mu1 - mu2 &lt; 0\nt.test(x1, x2, alternative = \"less\") \n\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = 0.61442, df = 57.845, p-value = 0.7293\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 1.980861\nsample estimates:\nmean of x mean of y \n 1.894851  1.362458 \n\n\nx1 and x2 have the same sample size, but there’s no reason to match up the first element of x1 with the first element of x2, the second element of x1 with the second element of x2, etc. The following code is completely inappropriate.\n\nt.test(x1 - x2, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  x1 - x2\nt = 0.57848, df = 29, p-value = 0.7163\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf 2.096155\nsample estimates:\nmean of x \n 0.532393"
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-proportion",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#inference-about-a-proportion",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.5 Inference about a Proportion",
    "text": "17.5 Inference about a Proportion\n\nAssumptions\nFirst, we check if a binomial distribution is appropriate:\n\nFixed number of trials.\nIndependent observations.\nOutcomes are either “success” or “failure”.\n\nE.g. Dice roll is a 5, patient recovered, etc.\n\nThe probability of success is constant for all observations.\n\nTo use tests and CIs, we need one additional assumption:\n\nThe sample is an SRS.\n\nThis covers assumption 3, since SRS ensures independence among observations. It also covers assumption 4: The probability that the first patient recovers is the same as the probability the second patient recovers since the patients come in a random order.\n\n\nSampling Distribution\nWe can only approximate the sampling distribution if both \\(np&gt;10\\) and \\(n(1-p)&gt;10\\), otherwise our sample is too small to say anything meaningful about \\(p\\). Assuming this is true, the population can be approximated by a normal distribution: \\[\nX\\stackrel{approx.}{\\sim} N(np, \\sqrt{np(1-p)})\n\\]\nNote that the equation above refers to \\(X\\), the number of successes out of \\(n\\) trials. This population distribution can be used to find the sampling distribution of the sample proportion: \\[\n\\hat p \\stackrel{approx.}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\nwhere \\(\\hat p\\) is the sample proportion (number of successes divided by number of trials) and \\(p\\) is the population proportion.\n\n\nConfidence Intervals\nThe sampling distribution involves the population proportion, which is not available to us. Instead, we use the sample proportion.\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nFor the t-test, the t distribution was used because \\(s\\) was estimated. We’re not estimating \\(s\\) here, so there’s no reason to use \\(t\\). We always use \\(z\\) for proportions.\n\n## Testing p &gt; 0.35\np0 &lt;- 0.35\nalpha &lt;- 0.045\nxbin &lt;- rbinom(n = 1, size = 30, prob = 0.3)\nphat &lt;- xbin / 30\nzstar &lt;- abs(qnorm(alpha/2))\n\nphat + c(-1, 1)*zstar*sqrt(p0*(1-p0)/30)\n\n[1] 0.2587633 0.6079033\n\n\n\n\nHypothesis tests\nPrior to collecting data, we have the hypotheses: \\[\\begin{align*}\nH_0: p = p_0\nH_A: p &gt; p_0\n\\end{align*}\\]\nWe also must set the significance level \\(\\alpha\\) prior to collecting data.\nAgain, the standard error involves the true population proportion. However, we now have a hypothesized proportion. This is what we must use.\nOnce we collect data, we calculate the test statistic: \\[\nz_{obs} = \\frac{\\hat p - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\n\\]\nSince our alterate hypothesis is “\\(\\ne\\)”, the p-value is found as: \\[\n\\text{p-value} = P(Z &gt; z_{obs})\n\\]\n\n## Testing p &gt; 0.35\np0 &lt;- 0.35\nalpha &lt;- 0.045\nxbin &lt;- rbinom(n = 1, size = 30, prob = 0.3)\nphat &lt;- xbin / 30\nzstar &lt;- abs(qnorm(alpha/2))\n\nzobs &lt;- (phat - p0)/sqrt(p0*(1-p0)/30)\n1 - pnorm(zobs)\n\n[1] 0.7170734\n\n1 - pnorm(zobs) &lt; alpha\n\n[1] FALSE\n\n\n\n\nR Code for p\nThere are two commands for tests/CIs for proportions. One uses something like our normal distribution, and one uses an exact test1.\nFor the following code, specifying alternative = \"greater\" gives us a one-sided confidence interval. This is not a concept that you need to understand, but this is why the CI looks weird.\nThe takeaway message here is this: In stats, there is usually more than one way to do things, and there’s not always a clear reason to use one over the other. We teach the basic normal approximation so that you understand at least some of the underlying mechanics, but either of these would be better in a real-world application.\n\n## Similar to our p-value (normal approx), but with fancier math\nprop.test(xbin, 30, alternative = \"greater\", conf.level = 0.955, p = 0.35)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  xbin out of 30, null probability 0.35\nX-squared = 0.14652, df = 1, p-value = 0.6491\nalternative hypothesis: true p is greater than 0.35\n95.5 percent confidence interval:\n 0.1676928 1.0000000\nsample estimates:\n  p \n0.3 \n\n\n\n## Exact test, using a different kind of fancy math\n## There are some drawbacks to using an exact test,\n## but that's outside the scope of this course.\nbinom.test(xbin, 30, p = 0.35, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  xbin and 30\nnumber of successes = 9, number of trials = 30, p-value = 0.7753\nalternative hypothesis: true probability of success is greater than 0.35\n95 percent confidence interval:\n 0.166326 1.000000\nsample estimates:\nprobability of success \n                   0.3 \n\n\n\n## CI from before \nphat + c(-1, 1)*zstar*sqrt(p0*(1-p0)/30)\n\n[1] 0.12543 0.47457\n\n## For confidence intervals, alternative must be two.sided\nprop.test(xbin, 30, alternative = \"two.sided\", conf.level = 0.955, p = 0.35)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  xbin out of 30, null probability 0.35\nX-squared = 0.14652, df = 1, p-value = 0.7019\nalternative hypothesis: true p is not equal to 0.35\n95.5 percent confidence interval:\n 0.1519501 0.4996657\nsample estimates:\n  p \n0.3 \n\n## For confidence intervals, alternative must be two.sided\nbinom.test(xbin, 30, alternative = \"two.sided\", conf.level = 0.955, p = 0.35)\n\n\n    Exact binomial test\n\ndata:  xbin and 30\nnumber of successes = 9, number of trials = 30, p-value = 0.7027\nalternative hypothesis: true probability of success is not equal to 0.35\n95.5 percent confidence interval:\n 0.1447633 0.4980533\nsample estimates:\nprobability of success \n                   0.3"
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#special-topics-in-inference",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#special-topics-in-inference",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "17.6 Special Topics in Inference",
    "text": "17.6 Special Topics in Inference\n\nSample Size Calculations\nIn order to propose a study2, researchers have to give some idea about what results they expect. As we saw before, a small confidence interval is always better3. In many grant proposals, researchers choose the sample size based on the desired width of their confidence interval.\nSuppose we want a 92% confidence interval for a mean no wider than 2 units on either side. In other words, we want \\(MoE = 2\\), i.e. \\(z^*sigma/\\sqrt n = 2\\). We know \\(z^*\\), but we don’t know what our standard deviation is! To get around this, we treat estimates from previous studies as if they’re population values.\nIf a previous study found a standard deviation of 5, then we have \\(1.75*5/\\sqrt n = 2\\), so \\(n = (1.75*5/2)^2 = 19.14\\). Of course, \\(n\\) must be an integer. If we round down to 19, then \\(MoE = 1.75*5/\\sqrt{19} = 2.007 &gt; 2\\), but we specified that the MoE cannot be larger than 2! When calculating sample size, never round down.\n\n\nSample Size Calculations 2\nIt is slightly more correct to use \\(s\\) and a \\(t^*_{n-1}\\) value than \\(\\sigma\\) and a \\(z^*\\) value. However, \\(t^*_{n-1}\\) relies on \\(n\\)! There’s no simple way to rearrange the equation for \\(n\\), so we would just check each value of \\(n\\) until we’re satisfied.\n\nMoE &lt;- 2\ns &lt;- 5\nalpha &lt;- 0.08\n\n## Using vectorized calculation\nn &lt;- 15:25\nMoE_vec &lt;- abs(qt(alpha/2, df = n - 1))*s/sqrt(n) \nMoE_vec\n\n [1] 2.436747 2.347173 2.266834 2.194240 2.128221 2.067837 2.012330 1.961075\n [9] 1.913554 1.869335 1.828051\n\n## Which n gives us the desired MoE?\nn[MoE_vec &lt; MoE]\n\n[1] 22 23 24 25\n\n\nSo we would use \\(n = 22\\).\n\nEverything below this line is outside the scope of this course.\n\n\n\nInference for Other Parameters\nNot everything is normal, and not everything is easily estimated. Here are some quick descriptions of inference for other parameters, along with their sampling distributions:\n\nA sequence of means, where we want to know if one of the means is significantly different from the others. This is called Statistical Process Control, or Statistical Quality Control.\n\nThe distribution of sample means is still normal, even if we have a sequence of them!4\nOften used in industrial applications to see when a machine needs to be tweaked.\n\nThe standard deviation \\(\\sigma\\): Not often done for one- or two-sample tests, but such tests exist. The sampling distribution is the same as the one I’ve been avoiding: the \\(\\chi^2\\)5\n\nFor Mixed Effects Models, tests around the standard deviation are very important.\nThere are Statistical Process Control methods for the standard deviation as well!\n\nThe median: Sometimes done, but require strong assumptions about the population.\n\nThe median is just one of the quartiles, the other quartiles can be estimated as well! The sampling distribution comes from Bootstrap sampling, which takes repeated samples from the sample (assumes the sample is representative of the population). This sounds counterintuitive, but there’s a lot of math showing that this is a reasonable thing to do.\nThis is sometimes called robust estimation, which is often a type of non-parametric estimation.\n\nExtreme values: Uses a result called an Extreme Value Theorem.\n\nThe Generalized Extreme Value Distribution describes the distribution of the largest value in a future sample. This is extremely6 useful in predicting/characterizing climate change: if our models predict certain values for maximum rainfall, temperature, etc., but we observe much larger values, then the nature of our data must be changing.\n\nThe spatial/temporal location of observations, e.g. predicting the likely locations of forest fires.\n\nSee my thesis for examples.\n\n\n\n\nBayesian Inference\nWhat if we could say that there’s a 95% chance that the population mean is in the confidence interval? In this case, the population parameter must have a probability distribution!\nIn order to make any inferences, we have to first make assumptions about the population parameter’s distribution. These assumptions must be reasonable but must not rely on our data (called a “prior” distribution). This is essentially what we do with hypothesis tests, but hypothesis tests are asking about a single value, whereas prior distributions say something about every possible population value. Just like with hypothesis tests, once we incorporate our data we can evaluate how reasonable our prior assumptions were.\nThis represents a seismic shift in our view of probability. Let’s go back to the coin example that I used for CIs: If a coin has been flipped, then it’s either heads or tails. There’s no probability inherent in the coin, it’s just our knowledge that’s uncertain. To a Bayesian, this is a probability! Bayesiand define probability as a measure of uncertainty. This allows for p-values and CIs as probabilities, although because of the way the math works out there are much better ways to go about these procedures."
  },
  {
    "objectID": "L19-Summary_of_Inference-Means_Proportions.html#footnotes",
    "href": "L19-Summary_of_Inference-Means_Proportions.html#footnotes",
    "title": "17  Inference: Overview, R Code, and Special Topics",
    "section": "",
    "text": "It uses the actual sampling distribution, which is outside the scope of this course.↩︎\nand get funding↩︎\nAssuming it still has the correct “coverage”, e.g. it actually will capture the true value 95% of the time.↩︎\nOr t.↩︎\nPronounced “kai square”, sometimes spelled “chi-square”.↩︎\npun intended↩︎"
  }
]