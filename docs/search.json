[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Life Sciences",
    "section": "",
    "text": "Introduction\nThese course notes structured to coincide with Baldi and Moore’s The Basic Practice of Statistics in the Life Sciences, 4th edition. However, I am working towards making them self-contained in order to provide these notes as an Open Educational Resources. In doing so, I will also borrow from OpenIntro Statistics for the Biomedical and Life Sciences, with attribution. As such, these resources have the same license (see the end of this page), except for those elements that are still present from Baldi & Moore (mainly exercises, which are being removed as I go)."
  },
  {
    "objectID": "index.html#completion-progress",
    "href": "index.html#completion-progress",
    "title": "Statistics for Life Sciences",
    "section": "Completion Progress",
    "text": "Completion Progress\nThese notes are still in progress.\n\nCh01 has been edited since the version posted in class.\nCh02-Ch09 are the versions posted to MyLS for this semester.\n\nThere may be missing images due to copyright concerns.\n\nCh10-13 are updated for this book, and are the course notes I will refer to moving forward.\nCh14+ are direct copies of old notes and need editing, which may involve addition of new material.\n\nIf you read ahead, you may need to re-read later."
  },
  {
    "objectID": "index.html#helpful-information",
    "href": "index.html#helpful-information",
    "title": "Statistics for Life Sciences",
    "section": "Helpful Information",
    "text": "Helpful Information\n\nText: Bald & Moore: The practice of statistics in the life sciences.\n\nRecommended, not required.\nCopy available in the library. Old versions work fine.\n\nAlternate (free) text (trialing):\n\nOpenIntro Statistics for Life Sciences\n\n\n\nThe textbook for this course is recommended but not required. We will not be using questions directly from the textbook so it’s fine if you have an old version. Instead, you can just use it as a reference for extra information about any of the course topics.\nI’ve also included an alternate textbook. If you use this, please share your experience with me so that I know whether I could switch to this free textbook!."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Statistics for Life Sciences",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\n\nCritically appraise published articles in health sciences research.\nUse industry standard tools to apply basic statistical concepts to real-world problems.\nUnderstand the use and application of statistical techniques such as descriptive and inferential statistics."
  },
  {
    "objectID": "index.html#accessing-materials",
    "href": "index.html#accessing-materials",
    "title": "Statistics for Life Sciences",
    "section": "Accessing Materials",
    "text": "Accessing Materials\n\nLectures posted on MyLS\nRStudio\n\nFree, open-source interface to the R programming language.\n\nSyzygy Jupyter (JuPyteR) Notebooks\n\nFree, web-based service for WLU students\nNo need to install R on your own computer\n\n\nYou can use either RStudio or syzygy for this course, RStudio has many fantastic bells and whistles that help you produce results and reports, whereas syzygy has an online interface and makes it easy to use without installing R on your own computer.\nFor lectures, I will be using VSCode, which is the main program that I use because it works well with python and R, as well as other languages that I need. I will switch to RStudio for many demonstrations just to show you how it works because this is the program that most people who do statistics will use. I will occasionally demonstrate some concepts using Jupyter notebooks because this is another common way that people do statistics and data science. You will not be tested on the features of Rtudio, VSCode or Jupyter notebooks, but mastery of RStudio will be extremely helpful for all future data analysis tasks beyond this course.\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 Unported License."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#introduction",
    "href": "L01-Intro_PicturingGraphs.html#introduction",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\n\nDefining “Statistics”\nMy definition: Statistics is the study of variance (or uncertainty).\n\nThe big question: is 1 statistically different from 100?\n\n1 vs. 100 apples? Yes.\n1 vs. 100 atoms in an apple? No.\n\n\n\nMany definitions of “statistics” are something along the lines of “methods for dealing with data”. This completely misses out on theoretical statistics, and makes it seem like statistics is a collection of recipes stating “if you have this data, use this method”. I think a better definition of statistics is that it’s the study of variance, whether that means studying the theoretical properties of variance or trying to explain variance in a particular data set.\nI like to ask the question: “is 1 statistically different from 100”. It may seem like they are obviously different numbers, but we can’t know that without the context. If you’re comparing numbers of apples, then yes, one apple is very different from 100 apples. However, if we’re looking at numbers of of atoms per apple, then one and 100 are both imperceptible numbers of atoms and thus we might say they’re practically the same. The difference in these two examples is the scale, and variance is a fantastic way to measure the scales of things. In my opinion, the main thing we will learn in this course is how to tell whether to numbers are different, given the scale of those two numbers. Another popular definition for statistics is “putting numbers in context”, and by “in context” they mean “relative to their variance”.\nIn my lecture notes, I use bold font for anything that you will be expected to be able to explain or define. You won’t necessarily see a full definition the first time you see a word in bold, but by the midterm/final it is something I expect you to know. A good way to study in this course is to keep a glossary of all of the words I’ve put in bold, with a definition file/note that you update as we learn more about that concept. And, of course, write a description as if you’re teaching someone else!\n\n\n\nWhy study variance?\n\nGive context to different numbers.\n\nThe size of the difference depends on the context.\n\nWe need to know how and why we were wrong.\n\nHow: What is the magnitude of the difference?\nWhy: Are we missing relationships? Bad sampling? Fundamental randomness?\n\n\nVariance is information!\n\nVariance comes from many sources. We might just be doing something wrong and missing out on important feature of our data, we might be collecting the data in a biased or incorrect way, or there might be some fundamental part of the problem that we will never be able to measure perfectly, and so the variance that we calculated may already be the smallest possible variance for this problem.\nI like to say that variance is information. Suppose we’re trying to figure out the heights of undergraduate students. We can calculate an average height, but it is entirely possible that nobody in our data set has a height that is exactly equally to the average height. We would want to quantify how much variation there is around that average height. If our sample includes a basketball team, then, this is variation due to some thing that we could have measured. In other words, the variation in our data can be explained by including another perspective. Once we have that perspective and we were included in our analysis are variance should be smaller, and therefore we have extracted more information. As we gain more information about our data, the variance in our estimate goes down - this is why I say that variance is information! Or, rather, variance is hiding information.\n\n\n\nDescriptive Versus Inferential Statistics\n\nDescriptive statistics are used to explore the data.\n\nGraphs/figures\nNumbers\n\nInferential statistics relate our data to the population.\n\nMust have a good sample first!\nOur sample has a mean. The population has a mean. How different do we expect them to be?\n\nhow different\n\n\n\n\nIn this course, we will learn about two classes of statistics. Descriptive statistics are the ones that we used to describe the sample that we obtained. This can include things like the mean/median/mode, the variance or the interquartile range, as well as bar charts, histograms box, plots, etc.\nInferential statistics are numbers that we calculate because we think have a relationship to the population. For instance, if we calculate the mean of our data, and we trust that our sample is good, then we expect this sample mean to be somewhat close to the population mean. Any time in this course I talk about the difference between two things, I will always mean “with reference to a measure of variance”. In this example, we have a sample mean as well as some measure of its variance, and this variance tells us how similar we expect the sample mean to be to the population mean. If we have a small sample variance, it means that we have a lot of information about the population mean. Variance contains information that we haven’t learned yet!1\nIn this course, we’re going to start by talking about descriptive statistics and work our way to inferential statistics."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "href": "L01-Intro_PicturingGraphs.html#descriptive-statistics-plots-and-graphs",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.2 Descriptive Statistics: Plots and Graphs",
    "text": "1.2 Descriptive Statistics: Plots and Graphs\n\nThe Palmer Penguins Data\nLet me introduce you to a dataset that I’m going to be returning to throughout the semester. This is called the Palmer penguins data, end it contains information on penguins from several islands in Antarctica. In the slides, I use some fancier code to only show some of the data, but I’ll display the full data set with simpler code below:\n\nlibrary(palmerpenguins)\nhead(as.data.frame(penguins))\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen             NA            NA                NA          NA\n5  Adelie Torgersen           36.7          19.3               193        3450\n6  Adelie Torgersen           39.3          20.6               190        3650\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4   &lt;NA&gt; 2007\n5 female 2007\n6   male 2007\n\n\n\nSpecies can be Adelie, Gentoo, or Chinstrap\nSex can be male or female2\nBill Length is measured in millimetres\nBody Mass is measured in grams\nNot shown: Island (one of three), bill depth (mm), flipper length (mm), year.\n\nThese data are really nice for teaching statistics because we can look at what factors contribute to body mass (since it’s easier to measure the length of a beak than it is to weigh a live penguin), or we can try to determine biosex using these measurements (since penguins normally have no external genitalia, but have other morphological differences). We can also fit a model to determine the species of each penguin so that we can get a full picture of what makes each species unique! All of these are examples of statistical analyses that we will cover in this course.\n\n\nTypes of Variables\n\n\n\npenguins[, c(\"species\", \"sex\", \"body_mass_g\")] |&gt;\n    head() |&gt;\n    knitr::kable()\n\n\n\n\nspecies\nsex\nbody_mass_g\n\n\n\n\nAdelie\nmale\n3750\n\n\nAdelie\nfemale\n3800\n\n\nAdelie\nfemale\n3250\n\n\nAdelie\nNA\nNA\n\n\nAdelie\nfemale\n3450\n\n\nAdelie\nmale\n3650\n\n\n\n\n\n\n\nSpecies is categorical\n\nMutually exclusive categories.\n\nSex is binary3\n\nOne or the other\nSpecial case of categorical with 2 possibilities\n\nBody mass is quantitative\n\nIt’s a number (quantity)\n\n\n\n\n\nThe type of variable is extremely important for choosing the right summary of the data.\nCategorical variables consist of two or more mutually exclusive categories, that is, each observation has a label and nothing has more than one label. Categorical variables may be ordered (such as “low”, “medium”, and “high”) or unordered (such as names or student numbers; putting names in alphabetical order is not usually meaningful for summarising the data or doing the analysis).\nThis distinction of “ordered” versus “unordered” matters to determine what visualization or model you might want to use to compare data within categories. For instance, we might want to see the change in response to a “low” treatment compared to a “medium” or “high” treatment, where we fully expect the response to the treatment to be lower for “low” treatment and higher for “high”. In contrast, suppose we knew the patients’ occupations. We have no expectation that the treatment reponse in the “electrician” group is lower or higher than any other group, because there’s no logical ordering to occupation.4\nBinary variables are a special case of categorical variables, which only have two categories. In this case, the ordering is rarely important, and thus we don’t really make the distinction between ordered binary and unordered binary. It’s a single difference either way, so we can just look at the differences within the categories.5\nQuantitative variables are those that are measured with numbers. Unlike “low” to “medium” versus “medium” to “high”, we know how big a step it is from 0 to 1 and then 1 to 2. Unlike categorical variables, quantitative variables have a concept of “in-between”; we have nothing between “medium” and “high”, but there are infinite numbers between 1 and 2.\nThere are, however, cases where there aren’t possible measurements between 1 and 2. These are called “discrete”6 For instance, the number of children that some has is either 0, 1, 2, etc. Discrete variables are somewhere in between categorical and continuous variables (variables that can take be number). Consider the following two examples of discrete variables: the number of children that someone has, and the number of cents in their bank account. In a study, we might want to look at everyone who has no children, everyone who has 1 child, everyone who has 2, etc. However, we would not want to compare everyon who has 0 cents in their bank account, everyone who has 1 cent in their account, 2 cents, 3, cents, etc. In terms of modelling and visualization, we will sometimes treat discrete variables as categorical and sometimes as continuous.7\nThe distinction between categorical and quantitative isn’t always this obvious, but can be very helpful for choosing the right kind of plot or numerical summary.\n\n\n\nGrey Areas\n\n\n\n\n\n\nStudent Numbers\n\n\n\nA student number looks like a quantitative variable, but it’s actually just a name (category)!\n\n\n\nThere are a couple gray areas when talking about variable types. The distinction between categorical and quantitative isn’t always perfectly clear. For example, student numbers are names, but they are completely made up of numbers. However, you wouldn’t treat these numbers as if one student number comes after another in the same way that you wouldn’t put students in an order based on their names. You technically can do this if you do it alphabetically or order the student numbers in order, but this isn’t a meaningful ordering. It’s not like one student number is larger than another student number, and taking the meaning of student numbers wouldn’t make any sense.\n\n\n\nIndividuals (Subjects)\nIndividual: the unit of study.\nIn the Palmer Penguins data set, a penguin is an individual.\n\n\n\n\n\n\nCO\\(_2\\) Measurements\n\n\n\nMonthly measurements of CO\\(_2\\) - the months are the individuals?\n\n\n\n\n\n\n\n\nPaired Observations\n\n\n\n\nSpousal pairs - the pairs are the individuals?\nBefore/after (e.g., weight loss) - people are individuals.\n\n\n\n\nAll of the variables we just talked about are measured on individuals. That is to say, an individual is what you are measuring when collecting data. This can take the form of a single penguin, in which case it’s obvious that this body mass belongs to this penguin; we have measured this body mass on this.\nThere are some gray areas to this as well, though. For example, if we’re measuring carbon dioxide every month, then we’re actually measuring carbon dioxide as the variable and months as the individuals, and we can measure other things on those individuals if needed.\nWe will also encounter pared observations in this course, which are measurements on two things at once. For example, we might be looking at spousal pairs, perhaps measuring the difference in height per pair. The variable we are measuring is the difference, and so we only have one observation per individual, which means that the individuals must be the pairs of people. A slightly more obvious example is something like a weight loss study were observing a change in weight for a certain person, even though we have two observations the individual is still the person who we’re measuring.\n\n\n\nExample: What are the Individuals? What are Variables?\n\nmtcars[1:6, 1:5]\n\n                   mpg cyl disp  hp drat\nMazda RX4         21.0   6  160 110 3.90\nMazda RX4 Wag     21.0   6  160 110 3.90\nDatsun 710        22.8   4  108  93 3.85\nHornet 4 Drive    21.4   6  258 110 3.08\nHornet Sportabout 18.7   8  360 175 3.15\nValiant           18.1   6  225 105 2.76\n\n\n\n\nIndividuals: Each car (not brand)\nVariables: wt, mpg, cyl, am, etc.\n\nwt: Quantitative; the weight of the car.\nam: Binary (categorical); whether the car is automatic or manual.\ncyl: It’s the number of cylinders, so it can be considered a number. However, there are only three possible values: 4, 6, or 8. We could consider these numbers, but it is likely more useful to think of 4 cylinder cars in one category, 6 cylinder in another category, and 8 cylinder in the last category. This way, when we do analysis, we are just comparing categories rather than see what happens when we add cylinders (“adding cylinders” makes it sound like we might add 1 cylinder to a 4 cylinder car, or like we might make predictions about what would happen with a 2 cylinder car)."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "href": "L01-Intro_PicturingGraphs.html#graphschartsplots",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.3 Graphs/Charts/Plots",
    "text": "1.3 Graphs/Charts/Plots\n\nPie Charts\n\nThe wedges must sum to 1.\n\nIf “Adelie” makes up 44% of the data, it should be 44% of the pie chart.\nAll penguins are either Adelie, Gentoo, or Chinstrap; no penguins have more than one species.\n\nIn contrast,\n\n\nMainly good for emphasizing one wedge\n\nEmphasizing can easily mean misrepresenting, whether accidentally or on purpose!\n\n\nhttps://www.darkhorseanalytics.com/blog/salvaging-the-pie\n\nIn this course, I will not be providing you the code required to make a pie chart. However, you should understand what a pie chart is, what data it works for (categories), and how they relate to bar charts.\n\n\n\nBar Charts: Categories\nBar charts are similar to pie charts, but better in practically every way.\nEach bar represents a category, and the height represents the number of observations in that category.\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(mtcars) +\n    aes(x = factor(am)) +\n    geom_bar() +\n    labs(x = \"Transmission Type\",\n        y = \"Count\")\n\n\n\n\n\nCompare this to the output of table(mtcars$am):\n\ntable(mtcars$am)\n\n\n 0  1 \n19 13 \n\n\n\nBar charts are primarily used to compare categories. The most common use of a bar chart is to count the number of observations in each category, and create a bar with a corresponding height. In this example, we see, automatic and manual transmissions, with automatic labelled as zero and manual labelled as one. We can see that approximately 22 cars are automatic and 13 cars are manual. Unlike a pie chart, we can read these numbers off of the plot and it’s easy to compare these two categories.\nThe code required to create the bar chart can be shown by clicking the “&gt;Code” icon. The data set is built into our so we don’t need to load anything to put this data. We do need to load in the ggplot library, though. In my lecture notes, you’ll see lots of code that looks like this, but you will not be tested on your ability to re-create this code. For those interested, here’s a quick breakdown of the functions I used:\n\nThe ggplot function tells R what data we will be using.\nThe aes() function sets up the plot “aesthetics”, such as what variable goes on the x-axis, what variable goes on the y-axis, what variable is assigned to a colour, what variable determines the shapes of points, etc.\ngeom_bar() actually draws the bar plot using the data set that ggplot() set up and the aesthetics, that aes() set up.\n\nTry running the code without this line and see what happens!8\n\nThe labs() function simply adds labels to the plot to make it look nicer.\n\nExercises\nThe following exercises are based on the exercises in Chapter 1 of Baldi & Moore, 4th ed.\n\nChildren’s food choices. Does the presence of popular cartoon characters on food packages influence children’s food choices? A study asked 40 young children (ages four to six) to taste two small pieces of Graham Crackers coming from a package with and a package without a popular cartoon character, and to indicate whether the two foods tasted the same or one tasted better. Unknown to the children, the crackers were the same both times. Here are the findings:\n\n\n\n\nTaste Preference\nNumber of Children\nPercent\n\n\n\n\nTastes better without character\n3\n\n\n\nTaste the same\n15\n\n\n\nTastes better with character\n22\n\n\n\n\n\nIdentify the individuals and the variable or variables in the study.\nPresent these data in a well-labeled bar graph.\nWould it also be correct to present these data in a single pie chart? Explain your reasoning.\nWhat do the data suggest about the influence of cartoon characters on Graham Cracker preference in young children\n\n\nThe study in Exercise 1 also asked the 40 children to taste small pieces of gummy fruit snacks and baby carrots presented in packages with and in packages without a popular cartoon character. For each food type, the children indicated which of the two options they would prefer to eat for a snack. (Note that this is a different question from the one asked in Exercise 1) The number and percent of children choosing the version with a cartoon on the package are displayed in the following table:\n\n\n\n\n\n\n\n\n\nFood Item\nNumber of children choosing the cartooon version\nPercent choosing the cartoon\n\n\n\n\nGraham Crackers\n35\n\n\n\nGummy and fruit snacks\n34\n\n\n\nBaby carrots\n29\n\n\n\n\n\nIdentify the individuals and the variable or variables in the study.\nMake a well-labeled bar graph of the data.\nWould it be correct to present these data in a single pie chart? Explain your reasoning.\nWhat can you conclude from these findings?\n\n\n\n\nOrdered and Unordered\nWhether the categorical variable is ordered or unordered affects the way we make the plot:\n\nOrdered: put the bars in order\nUnordered: put it in an arbitrary order\n\nAlternative: order according to largest to smallest.\n\n\n\n\nCode\nlibrary(palmerpenguins)\nggplot(penguins) +\n    aes(x = species) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories\")\n\n\n\n\n\n\n\nCode\nlibrary(forcats) # For rearranging \"factors\", aka. categorical variables\nggplot(penguins) +\n    aes(x = fct_infreq(species)) +\n    geom_bar() +\n    labs(x = \"Species\", y = \"Count\",\n        title = \"Unordered Categories, Ordered by Count\")\n\n\n\n\n\nThe bar chart is the de-facto standard for categorical variables, whether binary or otherwise. For quantitative, variables, we need other options.\n\n\nQuantitative Variables\nRecall the distinction between discrete and continuous:\n\nDiscrete (whole numbers)\n\nEx. Number of students in a classroom.\n\nContinuous (could be measured with more precision)\n\nEx. height\n\n\n\n\n\n\n\n\nGrey Area\n\n\n\nWhat type of variable is “dose level”, defined as either no dose, half dose, or full dose? They aren’t whole numbers, but we can’t measure them with greater precision!\n\n\n\nQuantitative variables are split into discrete and continuous variables. Discrete variables are generally represented by whole numbers, for example, the number of students in a given classroom.\nIn contrast, continuous numbers could be anything! I like to think of them as numbers that could’ve been measured with more precision if we had better tools. For example, peoples Heights could be measured to infinite precision if we had perfect tools, whereas we don’t need better tools to measure the number of children and family more precisely.\nOf course, as with all things, there is a gray area here. Many studies will choose to give their subjects either no dose, a half dose or a full dose. These are obviously numbers and it is very likely that the response for a 0.75 dose is somewhere in between the half dose and the full dose. However, we chose these numbers and thus there are only three possible numbers. No amount of measuring is going to give us something other than a half dose (any deviation in administration of the dose can hopefully be ignored for the purpose of the study). In the definitions we’ve used it is neither a whole number, nor cannot be measured with higher precision. For the purposes of visualization, we might actually want to use a bar chart as if this were a categorical variable. If the dose had more categories and we expected the response to have a smooth trend across different dose levels, then we might use visualizations meant for discrete data. If the dose could have been any number between zero and one then we might use visualization meant for continuous data.\n\n\n\nPlotting Quantitative Variables\nHere are the lengths of sharks:\n 9.4 12.1 12.2 12.3 12.4 12.6 13.2 13.2 13.2 13.2 13.5\n13.6 13.6 13.8 14.3 14.6 14.7 14.9 15.2 15.3 15.7 15.7\n15.8 15.8 16.1 16.2 16.2 16.4 16.4 16.6 16.7 16.8 16.8\n17.6 17.8 17.8 18.2 18.3 18.6 18.7 18.7 19.1 19.7 22.8\n\nWe could have measured more precisely!\n\nThis is a continuous variable.\n\nCan’t just draw a bar chart with all sharks that were 9.4, all that were 12.1, …\n\n\nHow many we display this collection of shark lengths? It is clear that there are many different values that we could’ve gotten for the length and so we might not want to use something like a bar chart. Let’s try it anyway.\n\n\n\nQuantitative Variables as a Bar Chart\n\n\nCode\nlibrary(ggplot2)\ntheme_set(theme_bw())\nsharks &lt;-  c(9.4, 12.1, 12.2, 12.3, 12.4, 12.6, 13.2, 13.2, 13.2, 13.2, 13.5,\n13.6, 13.6, 13.8, 14.3, 14.6, 14.7, 14.9, 15.2, 15.3, 15.7, 15.7,\n15.8, 15.8, 16.1, 16.2, 16.2, 16.4, 16.4, 16.6, 16.7, 16.8, 16.8,\n17.6, 17.8, 17.8, 18.2, 18.3, 18.6, 18.7, 18.7, 19.1, 19.7, 22.8)\n\nggplot() + aes(x = sharks) + geom_bar()\n\n\n\n\n\n\nThis plot demonstrates why bar chart isn’t appropriate for these data. We can see that each data point essentially gets its own bar, and so the heights are no longer meaningful. The exception is that these data are rounded to one decimal place, and so some lengths end up in the same bar. Knowing that some of our data are rounded to the same value is not necessarily meaningful for any analyses that we might want to do. Instead, we would like a chart that shows us where most of the data are, and whether or not they are clear patterns in these data.\n\n\n\nHistograms: Put observations into bins\nThe steps in building a histogram:\n\nChoose the bins.\n\ne.g. (0,10], (10,20], (20, 30], etc.\n\nThe notation (a, b] means that “a” is not included in the interval, but “b” is. We have no sharks that have a length of 0, but a shark with a recorded length of exactly 10 would be in the first bin, labelled (0, 10], not the second bin that is labelled (10, 20].\n\n\nCount the number of obs. in each bin.\nDraw a bar chart as if the bins are categories.\n\nBars should touch since there’s nothing in between.\n\n\n\n\nCode\n## Note that I've manually chosen the bin widths and centers.\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\nIn this histogram, the bins are (-1, 1], (1,3], (3,5]…\nNotice how the y-axis is still “Counts” (like a bar chart).\n\nMost of the time we will probably want to use a histogram to display quantitative, continuous data. A histogram is very much like putting continuous numbers into discrete bins, and then showing it as a bar chart. In this example, I chose bins from 1 to 3, then 3 to 5, then 5 to 7, and so on. For the bar on this histogram centred at a shark length of 12 we can see that there were five observations between 11 and 13. Note that the definitions of bins has a round bracket on the left side and the square bracket on the right side, this is to say that the left end point is not included but the right end point is included. This is just to account for cases where X may fall directly on the border between two bins, and we have to choose which bin. The actual bin we choose is arbitrary, kind of like driving in the left or the right. You will not be tested on whether you can remember which endpoint is inclusive.\nFrom the plot, we can see that most of the sharks are around 16 feet in length with sun going down to 10 feet and some as long as around 22 feet. The plot has a nice bell shape.\n\n\n\nHistograms: Bin Width Matters!\nThese histograms are showing the same data!\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=2, \n        center = 0, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nCode\nggplot() + \n    aes(x = sharks) +\n    geom_histogram(binwidth=1.5, \n        center = 0.75, # Only need to specify the center of one bin\n        colour = \"black\", fill = \"lightgrey\") +\n    labs(x = \"Shark Length\", y = \"Count\")\n\n\n\n\n\n\n\n\nIn the previous graph, it looked like the distribution of sharks followed a nice bell-shaped curve. However, if we use bins that are 1.5 units wide we get a plot that looks fairly different. It still looks like most sharks are around 16 feet and some go down to 10 and some go as high is 22 or 23, But we see a large bar that covers 12 to 13.5.\nWith histograms the bins that you choose are extremely important. Most software have default values that are generally reasonable, But it’s always always always worth investigating other bins.\nA simple version of the plot can be made as follows, where ggplot chooses the bins automatically. Note that this is rarely desireable, and you should almost always choose the bins yourself.\n\nggplot() + \n    aes(x = sharks) +\n    geom_histogram()\n\n\n\n\nBelow is an app to visualize the difference that the binwidth can make!\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/DensHist\")\n\n\n\n\nDescribing Distributions\nWhen you’re asked to comment on a histogram, always mention the following:\n\nShape: Unimodal/bimodal and skewness\n\nSkewness: put a glob of peanut butter on toast, “skew” it to one side.\n\nCenter: midpoint (mean/median)\n\nMode depends on the bin!\nSkewness shows up in the relation between mean and median: “Mean less (than median) means left (skew).”\n\nSpread: the range/variance/IQR\n\nMore on IQR later!\n\nOutliers: points that don’t fit the shape\n\nMore on outliers when we cover IQR!\n\n\n\nThere are many shapes that a histogram can show. A distribution can be skewed (or “heavy-tailed”), which means that it looks like a bell curve but one side has a longer/thicker tail. We also want to know about several measures of the center of the distribution, as well as how spread out it is. Outliers are also something interesting to note; outliers are something that are not part of the shape (so you wouldn’t consider them when evaluating the skewness of a distribution).\nTry drawing out each of these shapes/patterns!\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis represents a bimodal distribution because it has two peaks (the word “mode” can refer to the category with the most observations, but it can also refer to the top of a peak). This would be described as a bimodal distribution with centres around 190 and 215, ranges around 195 to 205 and 205 to 235, with both peaks being symmetric and without any outliers.\n\n\n\nExample: What is the Shape?\n\n\n\n\n\n\nThis is the classic sort of gotcha question that I like to use. This is actually a bar chart that I modified so that the bars have no space in-between - the x-labels are categories, not ranges! It may look somewhat symmetric and unimodal without any outliers, but the x-labels are out of order. These numbers are just numerical encodings of species names - 1 refers to Adelie penguins, 2 refers to Chinstrap, and 3 refers to Gentoo. These numbers were applied alphabetically because there isn’t really a logical way to order these species: they’re unordered categories!\nSo, basically, it does not make sense to talk about shape in a bar graph where the labels could have been put in any order!\n\n\n\nPurely Pedagogical: Stem-and-Leaf plots.\nConsider the data\n12, 43, 12, 32, 53, 66, 78, 25, 36, 12, 26,\n34, 98, 39, 44, 23, 15, 67, 1,  4,  54, 21\n\n\n\n\nStem-and-Leaf\n0  | 14\n10 | 2225\n20 | 1356\n30 | 2469\n40 | 34\n50 | 34\n60 | 67\n70 | 8\n80 |\n90 | 8\n\nSideways Histogram\n\nmydata &lt;- c(12, 43, 12, 32, 53, 66, 78, 25, 36, 12, 26,\n    34, 98, 39, 44, 23, 15, 67, 1,  4,  54, 21)\n\nggplot() + theme_minimal() +\n    aes(x = mydata) +\n    geom_histogram(binwidth = 10, center = 5,\n        fill = \"lightgrey\", colour = \"black\") +\n    scale_x_continuous(breaks = (0:10)*10, trans = \"reverse\", expand = c(0,0)) +\n    coord_flip() + \n    labs(x = \"Value\", y = NULL)\n\n\n\n\n\n\n\nThis visualization technique is shown purely for pedagogical reasons. A stem and leaf plot is like a histogram where the bins are all powers of 10. It isisplayed using the stem, which is the first digit and the leaf which is the second digit. For example, the number 78 has a 7 in the tens place (and were using the tens place as the stem) and an 8 in the ones place (and we using the ones place as the leaf). In the stem and leaf plot, 78 goes on the stem labelled 70 and it gets a leaf of eight. Going the other way, we can see a stem labelled 90 and a leaf labelled eight which corresponds to the number 98. For the stem labelled zero we have the numbers one and four, for the stem label 10 we have the numbers 12, 12, 12, and 15, and so on.\nEssentially, this is just a histogram. We’re instead of drawing a bar that corresponds to the number of observations in that bin, we are just listing the observations in that bin. Compare the stem and leaf plot to the sideways histogram on the right: in the first bin from 0 to 10 (not including 10) there are two numbers, one and four, and the length of the bin is two. For the stem label 20, we have the numbers 21, 23, 25 and 26, and this is displayed as a bar with link for in the histogram.\nThe main reason for showing this visualization technique is that it can be very useful for tests and quizzes, because it allows you to create a histogram without software. It also allows easy computation of the median since all of the leaves are in order. These are not used in practice, because in practice, you will have software to create histograms and find the median!\nNote that the shape of the distribution can be seen from the stem and leaf plot. It is a unimodal distribution that is right skewed and likely does not have an outlier (there is one point that appears to be separate from the others, but this is most likely due to bin choice).\n\n\n\nSummary\n\nIndividuals are what we make measurements on\n\nCan be pairs, dates, or people\n\nVariables are what we measure\n\nCan be derived from other measurements\n\nYou will not be asked to do anything with pie charts in this course.\nBar charts show counts of categories.\n\nCan optionally sum to 1 (like a pie chart).\n\nHistograms are like bar charts for binned data.\n\nBins matter.\nMust interpret shape."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#participation-questions",
    "href": "L01-Intro_PicturingGraphs.html#participation-questions",
    "title": "1  Picturing Distributions with Graphs",
    "section": "1.4 Participation Questions",
    "text": "1.4 Participation Questions\n\nQ1\nWhich of the following visualizations would be least appropriate for discrete data?\n\nBar Chart\nHistogram\nPie Chart\nStem-and-Leaf plot\n\n\n\nQ2\n\n\nWhat shape is the distribution on the right?\n\nSymmetric\nLeft Skewed\nRight Skewed\n\n\n\n\n\n\n\n\n\n\n\nQ3\nAge was collected as the number of days since birth. What kind of variable is this?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ4\nAge was collected as 0-17, 18-25, 25-34, or 35+. What kind of variable is this?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ5\nAge was collected as age in years, then rounded to the nearest 10. What kind of variable is the rounded value of age?\n\nOrdered categorical\nUnorded categorical\nDiscrete\nContinuous\n\n\n\nQ6\n\n\nApproximately what proportion of the data is larger than 0? Note that there are 40 data points.\n\n50%\n25%\n75%\n100%\n\n\n\n\n\n\n\n\n\nAnswers: 334133\nExercises\n\nPrescriptions of opioid pain relievers. Opioid pain relievers are prescribed at a higher rate in the United States than in any other nation, even though abuse of these medications can result in addiction and fatal overdoses. The CDC examined opioid pain reliever prescriptions in each state to find out how variable prescription rates are across the nation. Here are the 2012 state prescription rates, in number of prescriptions per 100 people, listed in increasing order:\n\nopiods &lt;- c(52.0, 57.0, 59.5, 61.6, 62.9, 65.1, 66.5, 67.4, 67.9, 69.6, 70.8, 71.2, 71.7, 72.4, 72.7, 72.8, 73.8, 74.3, 74.3, 74.7, 76.1, 77.3, 77.5, 79.4, 82.0, 82.4, 85.1, 85.6, 85.8, 88.2, 89.2, 89.6, 90.7, 90.8, 93.8, 94.1, 94.8, 96.6, 100.1, 101.8, 107.0, 109.1, 115.8, 118.0, 120.3, 127.8, 128.4, 137.6, 142.8, 142.9)\n\nMake a histogram of the state opioid pain reliever prescription rates using classes of width 10 starting at 50.0 prescriptions per 100 people. e.g. (50, 60]. Do this by hand first, then using R.\nWould you say that the distribution is single-peaked or multiple-peaked? Is it roughly symmetric or skewed?\n\nThe Statistical Abstract of the United States, prepared by the Census Bureau, provides the number of single-organ transplants for the year 2010, by organ. The following two exercises are based on this table:\n\n\n\nDisease\nCount\n\n\n\n\nHeart\n2,333\n\n\nLung\n1,770\n\n\nLiver\n6,291\n\n\nKidney\n16,898\n\n\nPancreas\n350\n\n\nIntestine\n151\n\n\n\n\n(1.14 in BM) The data on single-organ transplants can be displayed in\n\n\na pie chart but not a bar graph.\na bar graph but not a pie chart.\neither a pie chart or a bar graph.\n\n\n(1.15 in BM) Kidney transplants represented what percent of single- organ transplants in 2010?\n\n\nNearly 61%.\nOne-sixth (nearly 17%).\nThis percent cannot be calculated from the information provided in the table.\n\nSee also: OpenIntro Textbook problems relating to visualizations that we have learned, especially 1.30, 1.36, 1.37, 1.39, 1.40, 1.47."
  },
  {
    "objectID": "L01-Intro_PicturingGraphs.html#footnotes",
    "href": "L01-Intro_PicturingGraphs.html#footnotes",
    "title": "1  Picturing Distributions with Graphs",
    "section": "",
    "text": "It is worth noting that there is also variance that isn’t information, and there’s information that we’ll never have access to. Variance is an opportunity to learn, but there’s almost always a limit to how much we can learn.↩︎\nA note on gender/sexuality/biology: penguins, especially Chinstrap and Gentoo penguins, don’t have particularly strong gender roles, and often form same-sex couples. In this course, I will use the term “sex” to mean “biosex”, rather than “gender”, to indicate that we’re looking at morphological differences due to XX and XY chromosomes.↩︎\nPossibly categorical if there are any penguins with chromosomal abnormalities.↩︎\nWe may, however, know that factory workers have more exposure to a pathogen than those who work from home, but we would likely want to measure this directly rather than measuring it by proxy with occupation.↩︎\nWe often encode on of the categories as 0 and the other as 1, but this is usually either clear (0 = no treatment, 1 = treatment) or completely arbitrary (0 = femala, 1 = male) and this arbitrariness is acknowledged. Neither of these cases affect the way we make plots or run analyses that are based on binary variables.↩︎\nNot discreet.↩︎\nThere are methods/visualizations that are specific to discrete variables, but they only apply in very specific circumstances and will not be taught in this course.↩︎\nIt will still create a plot with the correct x and y axes, but won’t draw the bars.↩︎"
  },
  {
    "objectID": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "href": "L02-Describing_Distributions_Numbers.html#measures-of-spread",
    "title": "2  Describing Distributions with Numbers",
    "section": "2.1 Measures of Spread",
    "text": "2.1 Measures of Spread\n\nWhich has more variance?\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nAll have the same range (max - min).\n\n\nThese three data sets all have the same mean and median, but just looking at them shows that they are differnt collections of numbers. The first said, only has two unique values put those values are relatively far away from each other compared to the other sets. The second set is a more even spread from one to five. The third set has for value is equal to the mean and two values that may be outliers.\nTo me, the first set looks like it’s the most variable because all of the values are very far away from either the mean or the median. The second set has a smaller variance, because there are values closer to the mean. And the last set I expect to have the lowest variance, because most values are actually equal to the mean.\nThe formula for the variance, which will introduce next, matches this intuition. However, many students have different intuitions about which has the most variance and those are valid as well but are harder to quantify.\n\n\n\nMeasure of Spread: The Variance\nConsider set 1, which has a mean of 3:\n1 1 1 5 5 5\n\nThe distances to the mean are all -2 or 2\n\nIf we found the mean, we’d get 0! We need to make sure they’re all positive.\n\nAlternative 1: Absolute value. The average absolute distance to the mean is 2.\nAlternative 2: Squared value.\n\nThe average squared distance to the mean is 4\n\nImportant: This is not the actual variance calculation!\n\n\n\nThe variance is the average squared distance to the mean!\n(We use squared because math - more on this later).\n\nWe are basically looking at the average deviation from the mean. We want that deviation to be positive and there are several ways to do this. We have settled on squaring the numbers for the same reason we drive on the right side of the road: it’s just convention. There are benefits to using the absolute distance from the mean, but there are many mathematical advantages to squaring the values first.\n\n\n\nVariance Formula\n\n\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2\n\\]\nWe use \\(n-1\\) because of math reasons.\n\nThe easiest way to calculate this is to put it in a table:\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n1\n-2\n4\n\n\n2\n1\n-2\n4\n\n\n3\n1\n-2\n4\n\n\n4\n5\n2\n4\n\n\n5\n5\n2\n4\n\n\n6\n5\n2\n4\n\n\n\\(\\sum\\)\n18\n0\n24\n\n\n\nThe mean is 3, and the variance is 24/5 = 4.8.\n\n\n\nIn the table above, as before, the subscript \\(i\\) is just used to denote different observations. For example \\(x_1\\) is the first observation, \\(x_2\\) is the second observation in our data, and so on (this ordering is arbitrary).\nIn order to calculate the variance, we must first know the mean, and so it’s convenient to put this at the bottom of the table. We then squared the deviations from the mean and divided by \\(n-1\\). There are very good technical reasons why we divide by (n-1) that we won’t get into here. Come to my office and chat if you’d like to know more, or just ask ChatGPT!\n\n\n\n\n\n\n\\(n-1\\) in the denominator\n\n\n\nAs a quick explanation for \\(n-1\\), consider the variance of a single observation. It doesn’t vary! There’s not enough information to see how much variance there is. There isn’t enough information in our data. The \\(n-1\\) in the denominator enforces this - we can’t calculate the variance of one observation.\n\n\nNote that the variance can be calculated in R as follows:\n\nmy_values &lt;- c(1, 2, 2, 3, 4, 5, 6.3212, 3)\nvar(my_values)\n\n[1] 3.050982\n\n\n\n\n\nThe Variance and the Standard Deviation\n\\[\ns = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x - \\bar x)^2}\n\\]\nThe standard deviation (or sd) is just the square root of the variance.\n\nThis makes it have the same units as the original data.\n\n\nIn addition, if we have two data sets and the variance of one is larger than the other, then the standard deviation is also larger. They’re the same thing, just in different units!\nI will often refer to one when I mean the other. When I’m comparing standard deviations, I may call them variances because the same patterns will be there.\nHere’s the R code:\n\nsd(my_values)\n\n[1] 1.746706\n\nsd(my_values)^2 # the sd is the square root of the variance\n\n[1] 3.050982\n\n\n\n\n\nExercise: Variation of the three sets\n\n\nSet 1: 1 1 1 5 5 5\nSet 2: 1 2 3 3 4 5\nSet 3: 1 3 3 3 3 5\n\nDraw out bar plots.\nSet up the table and find the variance.\nCompare the standard deviations; make a conclusion.\n\n\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(x_i - \\bar x\\)\n\\((x_i - \\bar x)^2\\)\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n5\n\n\n\n\n\n6\n\n\n\n\n\n\\(\\sum\\)\n\n\n\n\n\n\n\n\n\nFill out the table above yourself!\n\n\n\nMeasure of Spread 2: The IQR\nThe IQR is very closely related to the median. But first, we will learn what quartiles are.\nConsider the data:\n1 2 3 4 5 6 7 8\nThe median of these data is 5; 50% of the data are to the left of this point.\n\n“Quartile”: Split the data into four.\n\nQ1: 25% of the data are to the left.\nQ2: 50% of the data are to the left (the median).\nQ3: 75% of the data are to the left.\n\n\n\n\nFinding Quartiles\n1 2 3 4 5 6 7 8\n\nFind the median\n\nIt’s 4.5. Cool.\n\nQ1 is just half of 50% - we’re finding a median again!\n\nQ1 is the median of everything to the left of the median.\nIn this case, 1 2 3 4 are the numbers to the left of the median, and so Q1 is 2.5.\n\nBy a similar argument, Q3 is 6.5.\n\n\nQ0 is where 0% of the data are to the left. In other words, it’s the minimum value in the data! Similarly, Q4 is the maximum value in the data.\n\n\n\n\n\n\nWarning\n\n\n\nThe algorithm we just used for computing the quartiles is not the only one! In R, there are NINE different ways to calculate the quartiles. You should stick to doing this by hand if you want to get the WeBWork answers right.\n\n\n\n\n\nThe five number summary\nLet’s use the folowing example:\n1, 3, 3, 4, 5, 5, 5, 6, 7, 7, 8, 8, 9, 10, 10, 11, 12\nThe quartiles give an excellent way to summarise data:\n\n\n\nQ0 (min)\nQ1\nQ2 (median)\nQ3\nQ4 (max)\n\n\n\n\n1\n4.5\n7\n8.5\n12\n\n\n\n\nThe five number summary just shows all five of the quartiles. Note that there are five quartiles, because zero is also one of them.\nFor practice, make sure you can calculate the median, and then the median of all the values to the left of it!\n\n\n\nVisualizing the five number summary: the boxplot\n\n\nThe plot on the right shows the body masses for the Palmer Penguins.\n\nThe lowest point is Q0\nThe left of the box is Q1\nThe thick line in the box is Q2 (the median)\nThe right of the box is Q3\nThe highest point is Q4\n\n\n\nlibrary(ggplot2)\nlibrary(palmerpenguins)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(y = body_mass_g) +\n    geom_boxplot() +\n    labs(y = \"Body Mass (g)\") +\n    coord_flip()\n\n\n\n\n\nggplot(penguins) + \n    aes(x = body_mass_g) +\n    geom_histogram(colour = 1, fill = \"lightgrey\") +\n    labs(x = \"Body Mass (g)\")\n\n\n\n\n\n\n\nThe boxplot and the histogram both demonstrate the right skew of the data, but the boxplot is much more compact!\nTake a moment to compare the two plots and make sure you can explain the skewness. Remember that 25% of the data are in each interval shown in the box plot!\n\n\n\nMeasure 2: The Inter-Quartile Range (IQR)\nThe IQR is defined as: Q3 - Q1.\n\nSame units as the original data\nRobust to outliers (unlike the sd)!\n\n\nThis is the second measure of spread that we will learn. The IQR is commonly used when we have highly skewed data or data with outliers. The sd measures the average squared deviation from the mean, whereas the IQR measures the middle 50% of the data.\nNotice how this is not centered on the median. Consider the following data:\n\nmy_values &lt;- c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 5, 6, 7, 8, 10)\nlength(my_values)\n\n[1] 20\n\nhist(my_values)\n\n\n\n\nThe median of these should be at position \\((n+1)/2 = (20 + 1)/2 = 10.5\\) (I used the length() function in R to count the number of observations for me). This value is halfway between 3 and 3, meaning it’s 3. Q1 is the median of the first 10 data points, which is at position 5.5, giving us a value of 2. Q3 is 5.5 positions from the end, which is 4.5. Thus the IQR is \\(4.5 - 2 = 2.5\\).\nFirst, does this make sense to you? Does 2.5 sound like a reasonable width for the middle 50%?\nNow consider that the distribution is clearly skewed to the right. This affects the variance a lot, but the IQR would have been the same no matter what the first 4 or last 4 values were.\n\n\n\nIQR for Outliers\nIn this class, we use a rule of thumb for calculating outliers. Anything that is…\n\nBelow the median minus 1.5*IQR, or\nabove the median plus 1.5*IQR\n\nis considered an outlier.\n\nThis rule of thumb is not based on any mathematical derivations, it just seems to work in most situations.\nThe idea is that the IQR gives a measure of spread, and the median gives the measure of the centre, so anything too far from the centre is an outlier. We use the spread to figure out how far away from the centre we are willing to accept. This will show up several times in this course. We’ve seen it in this example for the IQR and median because this is simple and easy to interpret.\nMost of the rest of this course will be spent looking at something similar for the mean. We will still use this idea of the centre plus or minus some measure of the spread, but will incorporate information about the sample and assumptions about the population that allow us to make much stronger conclusions beyond simply checking if something is an outlier.\n\n\n\nSummary\n\nThe “centre” is trying to measure the most common value.\n\nOften, this is our best prediction.\n\nThe “spread” is trying to measure the scale, or variation.\n\nGives context to the centre.\n\nThe mean and variance\n\nInterpretations and formulas are important.\n\nThe median and IQR\n\nCalculations, interpretations, five-number-summary, outliers, and boxplots are all important.\n\n\n\nWe saw the same thing a couple of times throughout this lesson. We saw measures of centre that try to describe the middle of a distribution and centres of spread that tell us how spread out the data are. The mean and the sd are intrinsically linked, and the median and IQR are intrinsically linked.\nWe also saw the rule-of-thumb to use IQR for finding outliers by using the median plus-an-minus some number times the spread. You better believe that this idea will show up again later in this course!\nBoxplots are a visual representation of the five number summary. These can be very small while still showing the shape of our data. However, these only work for unimodal data - there isn’t a good way to show a bimodal distribution on a boxplot. Also, it is very easy to plot two boxplots for two different data sets in order to compare the distributions.\nFor assignments and exams, be ready to calculate any of these values and compare the mean/median and sd/IQR. Also be ready to compare the five number summary to a boxplot.\nExercises\n\nSpider Silk. Spider silk is the strongest known material, natural or man-made, on a weight basis. A study examined the mechanical properties of spider silk using 21 female golden orb weavers, Nephila clavipes. Here are data on silk yield stress, which represents the amount of force per unit area needed to reach permanent deformation of the silk strand. The data are expressed in megapascals (MPa):\n\n164.00 173.00 176.10 236.10 251.30 270.50 270.50\n272.40 282.20 288.80 290.70 300.60 327.20 329.00\n332.10 351.70 358.20 362.00 448.90 478.70 740.20\n\nDescribe the shape, centre, and spread of the data using a histogram (code below).\nFind the mean and median yield stress. Compare these two values. Referring to the histogram produced by the code below, what general fact does your comparison illustrate?\nRe-run the code using different values of breaks. What do you see? (Note that this example uses base R rather than ggplot2 because it has simpler code - ggplot2 has more flexibility, but that flexibility isn’t necessary here.)\nUse the boxplot() function to create a boxplot (you do not need the breaks=10 part of the code). Compare this to the histogram. Also comment on any points that stand out (when there are outliers, R shows \\(Q2\\pm 1.5IQR\\) rather than Q0 and Q4).\nUse the \\(Q2\\pm 1.5IQR\\) formula by hand to find the outliers, and verify your calculations with the R plot.\n\n\nsilk_stress &lt;- c(164.00, 173.00, 176.10, 236.10, 251.30, 270.50, 270.50,\n    272.40, 282.20, 288.80, 290.70, 300.60, 327.20, 329.00,\n    332.10, 351.70, 358.20, 362.00, 448.90, 478.70, 740.20)\nhist(silk_stress, breaks = 10)\n\n\n\n\n\nDeep-sea sediments. Phytopigments are markers of the amount of organic matter that settles in sediments on the ocean floor. Phytopigment concentrations in deep-sea sediments collected worldwide showed a very strong right-skew. Of two summary statistics, 0.015 and 0.009 gram per square meter of bottom surface, which one is the mean and which one is the median? Explain your reasoning.\nGlucose levels. People with diabetes must monitor and control their blood glucose level. The goal is to maintain a “fasting plasma glucose” between approximately 90 and 130 milligrams per deciliter (mg/dl). The data tables below give the fasting plasma glucose levels for two groups of diabetics five months after they received either group instruction or individual instruction on glucose control.\n\nI provide the data as vectors in R, but you don’t need R for this question (it’s good practice to do it both ways).\n\ngroup &lt;- c(78.00, 95.00, 96.00, 103.00, 112.00, 134.00, 141.00, 145.00, 147.00,\n    148.00, 153.00, 158.00, 172.00, 172.00, 200.00, 255.00, 271.00, 359.00)\n\nindividual &lt;- c(128.00, 128.00, 158.00, 159.00, 160.00, 163.00, 164.00, 188.00, 195.00,\n    198.00, 220.00, 221.00, 223.00, 226.00, 227.00, 283.00)\n\n\nCalculate the five-number summary for each of the two data sets.\nMake side-by-side boxplots comparing the two groups. What can you say from this graph about the differences between the two diabetes control instruction methods? (Hint, you can create side-by-side boxplot using the code boxplot(variable_1, variable_2).)\nObtain the mean and standard deviation for each sample. Does this information give any clue about the shape of the two distributions?\nAdd to the historgrams a symbol representing the mean of each group and error bars representing one standard deviation above and below the mean. (You can do this by hand.) Compare this graphical summary with the boxplot display you also created.\nUse the 1.5 × IQR rule to identify any suspected outliers. Then look at the raw data to determine if unusually high or low values in either data set actually are outliers."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#relationships",
    "href": "L03-Scatterplots_Correlation.html#relationships",
    "title": "3  Scatterplots and Correlation",
    "section": "3.1 Relationships",
    "text": "3.1 Relationships\n\nExplanatory and Response Variables\n\nResponse: Responds to the explanatory variable.\n\nAlso called dependent variable.\n\nExplanatory: Explains the response variable.\n\nAlso called independent variable.\n\n\nKnowledge about explanatory tells us about the response.\n\nWe are not assuming the explanatory causes the response. We will not be covering causality in this course.\nWe are discovering tendencies, not rules.\n\n\nI just want to make this very clear: we are not looking for a causation. Instead, we’re just looking at whether or not to variables are related, and we think that measurements of one will be enough to tell us about measurements of the other. For example, if we think one variable is easy to measure and another is harder to measure, then we might want to set the easy to measure variable as the explanatory variable and see if it “explains” the harder to measure variable. This has nothing to do with the easy to measure variable causing the hard to measure one.\n\n\n\nExamples\n\nBlood alcohol content affects reflex time. – Some individuals may be more or less affected.\nSmoking cigarettes is associated with increased risk of lung cancer, and mortality. – Some heavy smokers may live to age 90\nAs height increases, weight tends to increase.\n\nHeight does cause weight, but there are other explanations.\n\n\n\nIn these examples, we carefully use words like “affects”, “associated with”, and “tends to”. For all of these examples we would expect a relationship of some sort, but the causality is not necessarily obvious.\nWe obviously expect the blood alcohol contact to affect reflex time. We expect this to be a causal relationship.\nIn the mid-1900s, it was hypothesized by cigarette companies that, rather than cigarettes causing cancer, people who were at increased risk of lung cancer with the sorts of people who also tended to smoke. Finding a relationship was not enough to convince people that it was cigarettes causing lung cancer. Even though we know that there’s a relationship between cigarettes and lung cancer, the techniques we learn in this course are not enough to conclude causality.\nHeight and weight are an example of how are the knowledge of one variable tells us about the other, without there being any causal relationship. We expect that taller people will have more mass, but there are also other reasons why somebody might have more mass that or not captured by their height."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#scatterplots",
    "href": "L03-Scatterplots_Correlation.html#scatterplots",
    "title": "3  Scatterplots and Correlation",
    "section": "3.2 Scatterplots",
    "text": "3.2 Scatterplots\n\nExample\n\n\nIn the data frame above, we have an observation of the number of power boats registered in each year, as well as the number of manatees that died in a collision with a powerboat in that year. The table shown above is only a small part of the data.\nThe rest of the data are shown in the plot. To create this plot, we put the number of powerboats registered on the X axis, and the number of manatee deaths on the Y axis. The annotation on the plot demonstrates how the points were added. One of the columns in the data is labelled 1977 and in that year they were to 755,000 powerboats registered and they were 54 manatee deaths that year. Because these two numbers are measured on the same individual (with the individual being the year in this example), we know that those two numbers go together. If we had a collection of peoples heights and a separate collection of peoples weights, but no knowledge of which individual each was collected on, then we would not be able to make a scatterplot. In order to make a scatterplot, we have to know which observation on the X axis is associated with which observation on the Y axis.\n\n\n\nWhat to look for\n\n\n\nOverall pattern\n\nLinear, curved, etc.\nDirection (increasing/positive, decreasing/negative)\nConstant variability\n\nDeviations from the pattern\n\nE.g., linear only in a small range\n\nOutliers\n\nAs before, discuss outliers separately from the pattern.\n\n\n\n\n\n\n\nIn general for this course were looking for a linear pattern. There are other models out there that fit nonlinear patterns, but we do not cover them in this course. There’s one way for things to be linear, and there are an infinite number of ways for things to be nonlinear. However, there are many common ways to account for non-linearity while still using a linear model.\nRegardless of whether something is linear or has some sort of curve, we are very interested in how strong of a pattern there is. For a linear model this means we want the points to be very close to the line, whereas for non-linear models we want the pattern to be very clear. We generally want patterns to pass the “facial impact test”, were the pattern is so obvious that it might as well be slapping you in the face (this is not an official test).\nAs with describing the shape of histograms, we treat outliers as something that are not part of the shape. We can have a clear linear pattern that happens to have an outlier.\nThe plot of manatees versus powerboats above would be described as a strong linear pattern, perhaps with some extra variation at larger X values.\n\n\n\nPenguins!\n\n\nWhat pattern is this?\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g) +\n    geom_point() + \n    geom_smooth(formula = y~x, method = \"lm\", se = FALSE) +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\n\nThe plot above shows a clear linear pattern. There is still some variation above and below the lines, but the pattern is still clear. It kinda looks like there may be two clusters; there’s a space between the two groups in the center of the X axis.\n\n\n\nAdding a Categorical Variable\n\n\nEach point has an \\(x\\) coordinate, \\(y\\) coordinate, and some other information.\nWe can encode that information with a colour!\n\n\nlibrary(palmerpenguins)\nlibrary(ggplot2)\ntheme_set(theme_bw())\n\nggplot(penguins) + \n    aes(x = flipper_length_mm, y = body_mass_g,\n        colour = species) +\n    geom_point() +\n    labs(x = \"Flipper Length (mm)\",\n        y = \"Body Mass (g)\")\n\n\n\n\n\n\n\nFrom this plot, we can see that the three species in these data all have a similar relationship, but still it might be worth separating out the groups and seeing what happens!\n\n\n\nThe Importance of Plotting: Anscombe’s Quartet\n\ndata.frame(\n    variable = names(anscombe),\n    mean = apply(anscombe, 2, mean),\n    sd = apply(anscombe, 2, sd)\n) |&gt; knitr::kable(row.names = FALSE)\n\n\n\n\nvariable\nmean\nsd\n\n\n\n\nx1\n9.000000\n3.316625\n\n\nx2\n9.000000\n3.316625\n\n\nx3\n9.000000\n3.316625\n\n\nx4\n9.000000\n3.316625\n\n\ny1\n7.500909\n2.031568\n\n\ny2\n7.500909\n2.031657\n\n\ny3\n7.500000\n2.030424\n\n\ny4\n7.500909\n2.030578\n\n\n\n\n\n\nIn this lecture were introducing plots before we talk about numerical summaries of two variables for a very good reason. The date is it displayed above is a well-known dataset called Anscombes quartet. Up to the first two decimal places, all of the variables in the data have the same mean and standard deviation. If this were all of the information you had, you might expect the plots to look similar.\n\n\n\nAnscombe’s Quartet\n\npar(mfrow = c(2,2), mar = c(3,3,2,1))\nplot(y1 ~ x1, data = anscombe)\nabline(lm(y1 ~ x1, data = anscombe))\n\nplot(y2 ~ x2, data = anscombe)\nabline(lm(y2 ~ x2, data = anscombe))\n\nplot(y3 ~ x3, data = anscombe)\nabline(lm(y3 ~ x3, data = anscombe))\n\nplot(y4 ~ x4, data = anscombe)\nabline(lm(y4 ~ x4, data = anscombe))\n\n\n\n\n\nClearly, there’s a very different pattern in each plot.\n\nThe first plot looks relatively linear with a little bit of random variation. For this data set a linear model does seem appropriate.\nThe plot at the top right she was a very clear pattern that is not linear, so we may be able to fit a model that accounts for this non-linearity.\nThe plot at the bottom left is almost a perfect line, but with an outlier. This outlier makes it so that the line that I have added to the plot doesn’t actually go through the perfect pattern that we can see if that outlier weren’t there.\nThe bottom right plot is a mess. If it weren’t for the outlier, the X values would all be identical! In this case, a scatterplot would not be appropriate. If I saw this while analysing my data, I would have assumed that X was supposed to be either constant (e.g., all X values should have been 8) or categorical. In both cases, a scatterplot would not be appropriate.\n\nDespite all of these wildly different shapes, all of these data sets have the same summary statistics.\n\n\n\nSummarizing Plots\n\nEach data point has an \\(x\\) and a \\(y\\). We plot \\(y\\) against \\(x\\).\n\n\\(y\\) is the response, \\(x\\) is the explanatory variable.\n\nWe’re looking to see if it’s linear. Linear models are something we know how to deal with!\n\nDeviations from linearity are noteworthy.\nOutliers are noteworthy.\n\nWe can incorporate more information in a scatterplot, especially categorical variables."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#correlation",
    "href": "L03-Scatterplots_Correlation.html#correlation",
    "title": "3  Scatterplots and Correlation",
    "section": "3.3 Correlation",
    "text": "3.3 Correlation\n\nMeasuring Strength of Linearity\n\n\nFrom plots, we can sorta see that one looks more linear than another.\nIt would be splendid if we could have a way to quantify this…\n\n\nlibrary(ggplot2)\ntheme_set(theme_bw())\nlibrary(patchwork)\nx &lt;- runif(100, 0, 10)\ny1 &lt;- 2 + 3*x + rnorm(100, 0, 4)\ny2 &lt;- 2 + 3*x + rnorm(100, 0, 1)\n\ng1 &lt;- ggplot() + aes(x = x, y = y1) + geom_point() +\n    labs(title = \"Strong correlation\")\ng2 &lt;- ggplot() + aes(x = x, y = y2) + geom_point() +\n    labs(title = \"Stronger correlation\")\ng1 / g2\n\n\n\n\n\n\n\nFrom this point on, we’re focusing on linear relationships. The plots above both demonstrate the same linear relationship, but with different “strength”s. Let’s measure that!\n\n\n\nThe correlation coefficient \\(r\\)\nRecall the formula for the variance: \\[\ns_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)^2 = \\frac{1}{n-1}\\sum_{i=1}^n(x_i - \\bar x)(x_i - \\bar x)\n\\]\nThe correlation coefficient is defined as: \\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\] where \\(s_x\\) is the s.d. of \\(x\\) and \\(s_y\\) is the s.d. of \\(y\\).\nIt’s like a variance for two variables at once!\n\nThis explanation might not stick for those of you who aren’t a fan of formulas, but I think this demonstrates an important aspect of the correlation coefficient. The formula for the standard deviation includes \\((x_i - \\bar x)(x_i - \\bar x)\\). If we replaced one of those with \\(y\\), we’d get \\((x_i - \\bar x)(y_i - \\bar y)\\), which is one step closer to the correlation coefficient. In other words, the correlation is a measure of how two (quantitative) variables vary together!\nLet’s try another approach. \\(x\\) has variance. \\(y\\) has variance. They also have variance with each other. This is measured by the correlation!\nIf neither of these explanations make sense, don’t worry! We’ll see plenty of correlations and get an intuition for how correlations are different with different data.\n\n\n\nThe range of \\(r\\)\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\n\\(s_x\\) and \\(s_y\\) are positive\n\\(s_x &gt; \\sum_{i=1}^n(x_i - \\bar x)\\), similar for \\(s_y\\)\n\nThis can’t be larger than 1\n\n\\(x_i - \\bar x\\) can be negative (same with \\((y_i-\\bar y)\\)).\n\nThe correlation coefficient can be anything from -1 to 1, with 0 representing no correlation and -1 and 1 representing perfect correlation.\n\nThe fact that the correlation can be negative is important. A correlation coefficient of -1 looks like a perfect downward slope.\n\n\n\nInterpreting correlation\n\n1 and -1 are perfect correlation.\n0.8 is a strong correlation (depending on context)\n\nPhysics: 0.8 is very very weak.\nSocial science: 0.8 is very very strong.\n\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/ScatterCorr\")\n\n\nThe app above shows data that start uncorrelated, then are slowly transformed into perfect correlation. If you hav R installed on your computer it should run just fine (you may need to run install.packages(\"shiny\") for the shiny package, and possibly install.packages(\"ggplot2\") if you haven’t already).\nFor more examples (and more info on the correlation coefficient in general), see the OpenIntro Textbook and let me know what you think of that textbook!\n\n\n\nComments on the correlation\n\\[\nr = \\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i - \\bar x}{s_x}\\right)\\left(\\frac{y_i - \\bar y}{s_y}\\right)\n\\]\n\nThe order of \\(x\\) and \\(y\\) can be switched\n\n2 times 3 is the same as 3 times 2.\n\nSince we’re subtracting the mean and dividing by the s.d., the units don’t matter!\n\nSwitching from kg to lbs has no effect on the correlation.\n\n\\(r&gt;0\\) means the line goes up. \\(r &lt; 0\\) means the line goes down.\nQuantitative only\nLinear only\nNot robust to outliers.\n\n\nLet’s explore some of these points with code!\n\nplot(y1 ~ x1, data = anscombe)\n\n\n\n\nIt looks relatively linear. Take a moment to think of how correlated these two variables are, and assign it a value between 0 and 1. This is how you would guess the correlation coefficient\nOn exams, you will be expected to differentiate between “not correlated” (about 0), “slightly correlated” (0.2 to 0.4), “very correlated” (0.6 to 0.8), and “near perfect correlation (almost exactly 1)”, or the negatives of these values; you won’t need to guess whether the correlation is 0.55 or 0.6.\nIn R, we calculate the \\(r\\) with the cor() function.\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nDoes this number make sense to you? It seems fairly high to me, but with small amounts of data it’s not that surprising. Think of it this way: if you removed a quarter of the data at random, would you still be able to see the pattern? If so, then it’s probably “very correlated”!\nThe first point states that the order doesn’t matter:\n\ncor(anscombe$y1, anscombe$x1)\n\n[1] 0.8164205\n\n\nThe units don’t matter:\n\ncor(anscombe$y1*5 + 1, anscombe$x1)\n\n[1] 0.8164205\n\n\nHowever, it does matter if we do a non-linear transformation, such as squaring the values. The correlation is a measure of linear association, so making things non-linear will affect it.\n\nplot(y1^2 ~ x1, data = anscombe)\n\n\n\ncor(anscombe$x1, anscombe$y1^2)\n\n[1] 0.7992029\n\n\nFor these data, squaring didn’t have much of an effect (as we can see in the plot), but we still saw a change in \\(r\\)! Notice that a unit change had absolutely no effect on \\(r\\). In general, we either expect things to be exactly the same or they can be completely different; very few things are “almost equal” in the general case (they may be almost equal with one set of data, but that means nothing for completely different sets of data).\n\n\n\n\\(r\\) measures linear correlation\n\n\nEnzymatic activity is known to be affected by temperature. A study examined the activity rate (in micromoles per second, μmol/s) of the digestive enzyme acid phosphatase in vitro at varying temperatures (measured in kelvins, K). The findings are displayed in the following table.\n\nDescribe the relationship\nExplain why it doesn’t make sense to describe this as “positively associated” or “negatively associated”.\nIs this a strong or a weak relationship? Explain.\n\n\n\n\n\n\nSolutions:\n\nThe relationship increases with an upward curve from temperatures of 300K to 340K, when it turns downward sharply and decreases to 355K.\nThe association is different for different X values. This is not a linear relationship, which means we have to do extra work to make sure that we cover all the non-linearities.\nThis is a very strong relationship. The pattern clearly passes the facial impact test that we discussed before. It is far from a linear relationship, but it’s clearly noticable.\n\n\n\n\nAgain, always plot your data!!!\n\n\nAll of the plots in the Anscombe quartet have the same correlation coefficient.\n\\(r\\) is a measure of linear association - if it’s not linear, \\(r\\) can’t be interpreted!!!\n\n\n\n\n\n\n\n\n\nIt’s important to note that \\(r\\) can always be calculated for numeric data. If we had student numbers as well as a categorical variable that used 0 to represent black, 1 to represent asian, etc., then we could technically calculate the correlation coefficient. This would be utterly meaningless!!!!!\n\n\n\nExample: Penguins\n\n\n\n\n\n\nThis is an example of something called Simpson’s Paradox: If we don’t account for the sub-groups, we get the opposite affect! As we can see in the plot, if we have all the groups together than it looks like a negative correlation, but once we separate groups each individual group has a positive correlation.\n(Note that I hid the code for this plot - the code I used to ensure the colours matched and I got the right layout is pretty advanced, and I also used some tricks along the way.)\n\n\n\nCorrelation Summary\n\n\\(r\\) is a measure of linear association\n\nI’ve said it plenty, I’ll say it again: \\(r\\) does not apply to non-linear patterns!\nAlways plot your data before calculating \\(r\\).\n\n\\(r\\) is like a measure of how two variables vary together.\n\nFormula is similar to the variance formula!\n\n\\(r\\) is a number between -1 and 1, with 0 meaning no correlation and 1 or -1 meaning perfect correlation.\n\nA negative \\(r\\) means a negative relationship (i.e. a line that goes down).\n\nEverything on the “Comments” slide is fair game for test questions."
  },
  {
    "objectID": "L03-Scatterplots_Correlation.html#participation-questions",
    "href": "L03-Scatterplots_Correlation.html#participation-questions",
    "title": "3  Scatterplots and Correlation",
    "section": "3.4 Participation Questions",
    "text": "3.4 Participation Questions\n\nQ01\nThe correlation coefficient measures the strength of the correlation.\n\nTrue\nFalse\n\n\n\nQ02\nIf \\(r &lt; 0\\), there is a negative linear correlation.\n\nTrue\nFalse\n\n\n\nQ03\nWhich of the following represents the strongest correlation?\n\n0.4\n0.7\n-0.8\n0\n\n\n\nQ04\n\n\nWhat is the best description of the plot on the right?\n\nNo correlation, has an outlier.\nStrong correlation, has an outlier\nSlight negative correlation\nShapeless\n\n\n\nx &lt;- c(runif(99, 0, 10), 11)\ny &lt;- c(rnorm(99), 20)\n\nggplot() + aes(x = x, y = y) + geom_point() #+ \n\n\n\n    #geom_smooth(method = \"lm\", formula = y~x, se = FALSE)\n\n\n\n\n\nExtra context: Fitting a Line\n\n\nA line would try and fit the outlier, which misleads us into thinking there might be a correlation!\n\n\nggplot() + aes(x = x, y = y) + geom_point() + \n    geom_smooth(method = \"lm\", formula = y~x, se = FALSE)\n\n\n\n\n\n\n\n\nQ05\nWhich statement is true.\n\nThe explanatory variable causes the response.\nThe response must be something measured after the explanatory variable.\nWe use the explanatory variable to explain the response, without assuming causality.\nThe correlation between the explanatory and response variable will be positive if the explanatory causes the response, negative if the response causes the explanatory.\n\n\n\nQ06\nWe can add colour to a plot using what type of variable?\n\nCategorical\nQuantitative\n\n\nExercises:\n\nThe following code will draw a plot and calculate the correlation coefficient. Currently, it’s doing this for the column mpg (response) versus the column wt (“weight”, explanatory) in the mtcars data which is built in to R.\n\nRe-run the code, but replace wt with disp (engine displacement), hp (horsepower), drat (rear axle ratio, although I couldn’t explain this further), and qsec (quarter mile time, in seconds). Comment on the apparent pattern and the magnitude of the correlation.\nChange wt tocyl, the number of cylinders. What do you notice about the plot, and how does this affect your interpretation of the correlation between mpg and cyl? Explain why cyl might be better incorporated as a categorical variable, even though it is indeed numeric.\nRepeat part (b) for am, which is “0” for automatic transmission and “1” for manual transmission.\n\n\n\nplot(mpg ~ wt, data = mtcars)\n\n\n\ncor(mtcars$mpg, mtcars$wt)\n\n[1] -0.8676594\n\n\n\nThe following figure comes from the article “Shared neural representations and temporal segmentation of political content predict ideological similarity” by De Brujin et al., published in 2023 (link to aricle here). The star on the plot indicates that they have found a statistically significant relationship (more on this next week). Is this a strong correlation?\n\n\n\nThe following figure comes from the article “Effect on Blood Pressure of Daily Lemon Ingestion and Walking” by Kato et al., published in 2013 (link to article here). Comment on the shape of this relationship. Recall how we described a “strong” shape as a shape that remains even if some of the data points were removed.\n\n\nExercises from OpenIntro Biotatistics textbook\nQuestions 1.35, 1.36, 1.37.\nFor further R practice and case studies, see the labs page for the OpenIntro textbook."
  },
  {
    "objectID": "L04-Regression.html#introduction",
    "href": "L04-Regression.html#introduction",
    "title": "4  Regression",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nThese notes are based on Chapter 4 of Baldi & Moore and Chapter 6.1 to 6.3 in OpenIntro Biostats.\nIn linear modelling, we have a collection of pairs \\(x_i\\) and \\(y_i\\). We think that there’s some sort of relationship between \\(x\\) and \\(y\\), and we think that a line is an adequate way to characterize that relationship1.\nJust like we assume that there’s a “true” population mean, there is also a “true” slope and intercept for the line that characterizes the relationship between \\(x\\) and \\(y\\). In the plot below, the green line represents the “true” relationship between \\(x\\) and \\(y\\), and the data are random values above and below that line2.\n\n\n\n\n\nIn high school, you may have learned a line as \\(y = mx + b\\). In statistics, we often use latin letters for estimates and greek letters for population parameters3. The population line is thus:\n\\[\ny_i = \\alpha + \\beta x_i + \\epsilon_i\n\\]\n\n\\(\\alpha\\) is the intercept.\n\\(\\beta\\) is the slope.\n\nA 1 unit increase in \\(x\\) corresponds to a \\(\\beta\\) increase in \\(y\\).\n\n\\(\\epsilon_i\\) is random noise (\\(N(0,\\sigma)\\)).\n\nAgain, we think of \\(x\\) as being fixed. The random noise is above and below the line, not side to side.\n\nThe formula implies that \\(y_i \\sim N(\\alpha + \\beta x_i, \\sigma)\\), since \\(y_i\\) is centered at \\(\\alpha + \\beta x_i\\) but randomly varies above and below the line with variance \\(\\sigma^2\\).\n\nThe word “regression” means to go backward. I like to think that we are “going backward” to the population numbers from the sample values4. Any situation where you are estimating a population parameter is technically a regression, but this terminology is not useful for this class.\nTo regress, we estimate the parameters using sample statistics. For linear regression, we use regular old latin letters instead of the fancy greek ones. \\(a\\) is the estimate for \\(\\alpha\\), \\(b\\) for \\(\\beta\\), and \\(e\\) for \\(\\epsilon\\). In order to do find these sample statistics, we minimize the squared error between the line and the data:\n\\[e_i^2 = (y_i - a - b x_i)^2\\]\nIn other words, we find \\(a\\) (for \\(\\alpha\\)) and \\(b\\) (for \\(\\beta\\)) that make the sum of the squared errors \\(e_i\\) as small as possible. We use the squared errors for the same reason we use squared deviations in the forumla for the variance: so that positive and negative values do not cancel out5.\nThe estimates \\(a\\) and \\(b\\) are as follows:\n\\[\\begin{align*}\nb &= rs_y/s_x\\\\\na &= \\bar y - b\\bar x\n\\end{align*}\\]\nThese are called the least squares estimates6. The equation for \\(b\\) is especially important!\nIn R, these can be calculated as follows. The mtcars data set is a collection of measurements made on various cars. In this example, we’ll regress the fuel efficiency (in miles per gallon, or mpg) against the weight of the car.\n\n## Load a built-in data set\ndata(mtcars) \n\n## Define which variables are x and y.\n## This isn't necessary, but helps with teaching\nx &lt;- mtcars$wt\ny &lt;- mtcars$mpg\n\n## Calculate the estimates by hand\nb &lt;- cor(x, y) * sd(y) / sd(x)\na &lt;- mean(y) - b * mean(x)\n\n## Print the estimates \nc(a, b)\n\n[1] 37.285126 -5.344472\n\n## Use the built-in functions\nsummary(lm(y ~ x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nx            -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nFrom this line, we can make predictions about new points by simply plugging in the \\(x\\) value. For example, let’s say we wanted to guess the mpg of a car that weighs 3,000 lbs. In the data, the units for weight are 1000 lbs, so this means plugging a value of wt=3 into the data.\n\na + b*3\n\n[1] 21.25171\n\n\nSo we would guess that a 3 ton car would have a fuel efficiency of 21.25 miles per gallon. Let’s look at this on a plot:\n\nplot(y ~ x)\npoints(3, a + b*3, col = \"red\", pch = 16)\n\n\n\n\nIt looks like this is somewhere around where we would expect.\nIf we repeat this for every possible \\(x\\) value, we get the regression line below:\n\nplot(y ~ x)\npoints(3, a + b*3, col = \"red\", pch = 16)\n## abline adds a line with slope b and intercept a to a plot.\nabline(a = a, b = b, col = \"red\")\n\n\n\n\nWe cal also see the values of \\(e\\), the residuals.\n\ne &lt;- y - (a + b*x)\nplot(e ~ x, main = \"Plot of the Residuals\")\n## abline can also draw a line with slope 0 (horizontal)\nabline(h = 0, col = \"grey\")"
  },
  {
    "objectID": "L04-Regression.html#regression-facts",
    "href": "L04-Regression.html#regression-facts",
    "title": "4  Regression",
    "section": "4.2 Regression Facts",
    "text": "4.2 Regression Facts\nHere are some facts about the least squares regression line:\n\nThe point \\((\\bar x, \\bar y)\\) is always on the line.\n\nLeast squares regression can be seen as putting a line through \\((\\bar x, \\bar y)\\) and rotating it until the squared error is the smallest.\n\n\\(s_y\\ge 0\\) and \\(s_x\\ge 0\\), so whenever \\(r &gt; 0\\), we know that \\(b &gt; 0\\).\n\nThe slope has the same sign as the correlation. Otherwise, the slope could be pretty much any number, regardless of the correlation.\nIf \\(r = 0\\), then \\(b = 0\\), and vice versa.\nOther than the sign and the special case of \\(r=0\\), there is no way to tell the value of \\(r\\) if all you know is \\(b\\).\n\nFor \\(r\\), the distinction between \\(y\\) and \\(x\\) doesn’t matter.\n\nFor the regression line, it absolutely matters!\n\nThe sum of the errors is 0.\n\n\n## The prediction at mean(x) is equal to mean(y)\n## In other words, (mean(x), mean(y)) is a point on the line\na + b * mean(x)\n\n[1] 20.09062\n\nmean(y)\n\n[1] 20.09062\n\n## Correlation doesn't care about order\ncor(x, y)\n\n[1] -0.8676594\n\ncor(y, x)\n\n[1] -0.8676594\n\n## Theoretically 0, but computers aren't perfectly precise\n## Note: e-14 refers to 10^-14, or 14 zeroes before the first digit\n    # So, pretty close to 0.\nsum(e) \n\n[1] 1.065814e-14"
  },
  {
    "objectID": "L04-Regression.html#percent-of-variation-explained",
    "href": "L04-Regression.html#percent-of-variation-explained",
    "title": "4  Regression",
    "section": "4.3 Percent of Variation Explained",
    "text": "4.3 Percent of Variation Explained\nBecause of some mathematical quirks, \\(r^2\\), the squared value of \\(r\\), can be interpreted as:\n\nThe percent of variation in \\(y\\) that can be “explained” by the linear model.7\n\nThe value of \\(r^2\\) can be calculated as: \\[\nR^2 = r^2 = \\frac{\\text{Variance of the predicted }y\\text{-values}}{\\text{Variance of the observed }y\\text{-values}}\n\\]\nI’ll explain this in steps. The first plot below shows just the values in \\(y\\). This collection of values has a own mean and variance.\nThe second plot shows the change in variance that the line “explains”. Instead of deviations above and below the mean, the variance can now be characterized as the deviations above and below the regression line. This variance will always be lower than the variance of \\(y\\) without incorporating \\(x\\)8.\nThe third plot shows where this variance went. The line itself has variance; there is deviation in the line above and below the mean of \\(y\\). This is the variance that gets explained by incorporating \\(x\\)! If you consider one of the points in \\(y\\), say \\(y_1\\), the distance between \\(y_1\\) and \\(\\bar y\\) can be split up into the difference between \\(\\bar y\\) and the regression line plus the distance between the regression line and \\(y_1\\).\n\n\n\n\n\nThe rest of the variance is left unexplianed. No regression will ever be perfect unless we are studing a very very simple .\nTo see this a different way, consider what happens when \\(r = 0\\)9. This will just be a horizontal line, and none of the variance is explained. On the other had, if \\(r = 1\\) then all of the points will be exactly on the line. All of the variance in \\(y\\) has been explained by the regression against \\(x\\) - there’s no variance left to be explained!10\nNotice how the R output includes"
  },
  {
    "objectID": "L04-Regression.html#extensions-and-cautions",
    "href": "L04-Regression.html#extensions-and-cautions",
    "title": "4  Regression",
    "section": "4.4 Extensions and Cautions",
    "text": "4.4 Extensions and Cautions\n\nPrediction\nFor a new \\(x\\) value, \\[y = a + bx\\] is the predicted value of \\(y\\). That is, if we have an \\(x\\) value, we can plug it into the equation and find out what value of \\(y\\) we would expect.\nNote: There is still variance around this prediction! Our “expected” value will never be exactly equal to the truth - The value of \\(y\\) at a given value of \\(x\\) follows a normal distribution11, and the probability of a single point is 0!\n\n\nExtrapolation\nExtrapolation is what happens when prediction goes wrong. In particular, it’s what happens when we try to make a prediction at an \\(x\\) value where we don’t have any data. Usually this means we’re predicting an \\(x\\) value far above or far below the range of our data, but it can also happen if there’s a gap in the middle of our data.\nIn the plot below, the black dots are the original data, and we’re trying to predict a new value at \\(x = 25\\). The red line is the true model that I generated the data from. The black line represents a linear model. This model fits the original data quite well12, but predictions are completely inappropriate for values outside the data.\n\n\n\n\n\n\n\nLurking Variables\nThe black line in the plot below represents a regression where all of the data was lumped together. As we can see, this line does not seem to fit the data well. There is a hidden relationship in the the data - the green points and the red points should be considered separately13.\n\n\n\n\n\nA more serious consequence of a lurking variable has shown up before in the Palmer penguins data. In that example, the lurking variable actually reversed the correlation - if we lumped the groups together we got a negative correlation (and therefore negative slope), but if we looked at the groups individually we got positive associations in all of the groups! This is called Simpson’s Paradox, and basically means that we have to be very careful about interpreting correlations!"
  },
  {
    "objectID": "L04-Regression.html#footnotes",
    "href": "L04-Regression.html#footnotes",
    "title": "4  Regression",
    "section": "",
    "text": "Very few things are actually linear, but lines are fantastic approximations to many things.↩︎\nWe assume that \\(x\\) is fixed, but \\(y\\) has random noise. In other words, \\(x\\) is not a random variable but \\(y\\) is.↩︎\nBecause we think it makes us fancy. Note that the Baldi & Moore textbook uses \\(a\\), \\(b\\), and \\(e\\) for everything.↩︎\nActually, the word comes from “regressing to the mean”, which comes from how children are closer to average height than their parents - they go back toward the mean. This is not important.↩︎\nAlso, because the calculus works out so much better.↩︎\nThere are other ways to estimate these parameters, but they’re outside the scope of this course. All regression lines that you see in the textbook and the notes will be least squares regression lines.↩︎\nUsually \\(r^2\\) is labelled \\(R^2\\) for historical reasons. Capitalization matters in math; it’s just coincidence that both lower case and upper case mean the same thing here.↩︎\nExcept when \\(r=0\\), can you explain why?↩︎\nTherefore the slope will also be 0.↩︎\nStatistics is still just the study of variance.↩︎\nOur prediction is just us guessing the mean value of \\(y\\) at different values of \\(x\\).↩︎\nEven though it’s not the true relationship, it’s a reasonable approximation.↩︎\nPossibly as a blocking variable.↩︎"
  },
  {
    "objectID": "L05-Probability.html#defining-probability-with-dice",
    "href": "L05-Probability.html#defining-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.1 Defining Probability with Dice",
    "text": "5.1 Defining Probability with Dice\nI find that the easiest way is to build this up by examples. Let’s start with rolling a dice.3 Let’s say you rolled the dice, and you got a 3. This is called a simple event. The collection of all simple events is called the Sample Space, which in this case is \\(\\mathcal S\\)={1,2,3,4,5,6}.\nNow, suppose that you’re about to roll a dice. You might be curious about whether it’s a 1, 2, 3, 4, 5, or a 6, but you might only care about the event that the outcome is even. Since there are multiple simple events that make up this event, it is called a compound event.\nThere’s no way for you to know what’s going to happen, but you know all of the possibilities and you know how likely they all are.4 This is called a Probability Model: The sample space along with the probability of all of the events.\nI said “the probability of all events”, but this is more complicated than it may seem and requires some explanation. For something like rolling a dice, you only need to know the probability of each simple event. Compound events, like the probability that the outcome is even, can be determined from these simple events.\nSuppose you’re playing a game where, if the outcome of the dice is less than a certain cutoff value you get to roll again (e.g., your character has a special ability that allows re-rolling of dice, but the re-roll condition depends on the situation). You know the probability of all of the simple events, but you need to know the cutoff value to actually compute any probabilities. Without the cutoff value, you cannot define the probability model.\nFor a dice, the probability model is simply: Each number has a probability of 1 in 6. But what does this mean? There are two perspectives on what a “probability” is: The Frequentist approach and the Bayesian approach. In this class we’re only going to learn the Frequentist definition of probability, but if you’re interested in learning more I’m happy to talk.5\nProbability (Frequentist Definition): The long run frequency of observing an event. In other words, it’s the number of times an event is observed divided by the number of trials after doing a near-infinite number of trials.\nFor the dice, if we rolled the dice 60 times, we would expect 10 of those rolls to be a 1, 10 of them to be a 2, etc. Due to randomness, we won’t get exactly that, but this is what we would expect. If we rolled 600 dice, we would expect 100 to be 1, etc. As we roll more dice, we get closer to the proportion of 1/6. The plot below this demonstrates this - it is the number of times that a dice was 1 divided by the number of trials, with the number of trials being increased. Notice how it takes a while for the “empirical” probability to reach the theoretical probably; as the number of trials approaches infinity, the proportion of rolls that showed a 1 will approach 1/6."
  },
  {
    "objectID": "L05-Probability.html#calculating-probability-with-dice",
    "href": "L05-Probability.html#calculating-probability-with-dice",
    "title": "5  Probability Background",
    "section": "5.2 Calculating Probability with Dice",
    "text": "5.2 Calculating Probability with Dice\nFirst, let’s introduce some notation. I will use P(x) to mean “The probability of x”. In some cases, the context will be clear, such as:\n\n“The probability of rolling a 1” = P(1)\n“The probability of rolling an even number” = P(even)\n“The probability of not rolling a 1” = 1 - P(1)6\n\nFor this section, we’ll assume that P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=1/6.7\n\n“Or”\nWhat’s the probability that we roll an even number? The even numbers are 2, 4, and 6, so what we’re really asking is “What’s the probability that we roll a 2, 4, or a 6?” In this case, the probability is P(2 or 4 or 6) = P(2)+P(4)+P(6) = 1/6 + 1/6 + 1/6 = 3/6 = 0.5.\nWe also could have figured out this probability by noting that half of the values are even, so a probability of 0.5 makes sense. It’s a good thing when our intuition matches our answer, as we’ll see next.\nLet’s consider the probability that the dice is even^ (which we will denote P(even)) or it’s strictly larger than 3 (denoted P(&gt;3)). This means the dice is either 2, 4, or 6, or it’s 4, 5, 6. Since there are 4 different numbers (2, 4, 5, and 6) that would match the criteria, the probability is 4/6. Let’s use our “or” rules to verify this!\nThe probability that the dice is even is 1/2. The probabilty that the dice is larger than 3 is also 1/2. So, obviously, P(even or &gt;3) = P(even) + P(&gt;3) = 1/2 + 1/2 = 1.\nWait.\nThat can’t be right.\nI think you may be able to see what went wrong. The P(even) = P(2) + P(4) + P(6), and P(&gt;3) = P(4) + P(5) + P(6). When we did P(even) + P(&gt;3), we added P(4) and P(6) twice! To get the right answer, we need to fix this. Since we added them twice, we must subtract them once. This brings us to…\n\n\nThe Addition Rule for “or”\nFor any two events A and B,8 the Addition Rule states:\n\\[\\begin{align}P(A\\; or\\; B) = P(A) + P(B) - P(A\\; and\\; B).\\end{align}\\]\nFirst, note that the probability of both events is P(Even and &gt;3) = P(4) + P(6), since 4 and 6 are both even and larger than 3.\n\\[\\begin{align*}\nP(Even\\; or\\; &gt;3) & = P(Even) + P(&gt;3) - P(Even\\; and\\; &gt;3)\\\\\n& = [P(2) + P(4) + P(6)] + [P(4) + P(5) + P(6)] - [P(4) + P(6)]\\\\\n& =  P(2) + P(4) + P(5) + P(6)\\\\\n& = 1/6 + 1/6 + 1/6 + 1/6\\\\\n& = 4/6\n\\end{align*}\\] \n\n\n“and”: Part 1\nThe word “and” came up in the addition rule, and so I should give a good definition of “and”. When we talk about events A and B, P(A and B) refers to the probability that they both happen together. It’s most helpful to see this as a Venn diagram:\n\nP(A) is the area of the circle labelled A, P(B) is the area of the circle labelled B, and P(A and B) is the area of the overlap between these two circles. P(A or B) is the total shaded area, including the yellow-green, green, and dark green.\nYou can see the Addition Rule at work here. If you add the area of A (which includes P(A and B)) to the area of B (which also includes P(A and B)), you’ve added P(A and B) twice!\nThere are two formulas for P(A and B). The first one is found by rearranging the formula for P(A or B):\n\\[\\begin{align*}\n\\small P(A\\; or\\; B) &\\small = P(A) + P(B) - P(A \\;and\\; B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(A) + P(B) - P(A \\;or\\; B)\n\\end{align*}\\]\nWhen in doubt, just remember P(A_B) = P(A) + P(B) - P(A_B), then put “or” in one blank and “and” in the other.\nThis formula won’t always get you to the solution, though. There will be many times where neither “and” nor “or” will be obvious, and we’ll need to do some more work to get them. We have special formulas for “and”, so we’ll usually try to figure out the “and” and then use it to figure out the “or”9.\n\n\n“given”: Conditional Probabilities\nA condition is something that must happen before you can proceed. A conditional probability is a probability that requires something else to happen, and usually involves a more complicated setup.\nLet’s look at another scenario. Let’s say I told you that the number on the dice was larger than 3. What’s the probability that the number on the dice is a 4? Intuiutively, it’s 1/3, since there are 3 possible numbers. Our notation fails us here, P(4) denotes the probability that the dice is a 4, which we already determined was 1/6. We can’t use P(4) for two things, so we need to add some notation.\nIn this case, the solution is to use a vertical bar, “|”, which is pronounced “given”. We write “P(dice is 4 | dice is greater than 3) = 1/3”,10 which is read as “The probability that the dice is 4, given that the dice is larger than 3.”\nA very important thing happened here: when we used a conditional probability (“given that”), we restricted the sample space. When we “condition” on an event, it means that we’re only looking at cases where that event happened. “The probability that the dice is 4, given that the dice is larger than 3” is another way of saying that we’re only considering events where the dice roll is greater than 3; we don’t care about 1, 2, or 3.\nWe defined “probability” as the total number of events divided by the total number of trials. For conditional probabilities, this means that we’re only looking at some of the trials.\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nThis formula is incorrect: “The number of ways that a dice can be 4” depends on the condition. For instance, the number of ways that a dice can be 2 is 0 since we’re told it’s larger than 3. We are actually looking at the number of ways that the dice can be both 4 and greater than 3. Let’s incorporate this information:\n\\[\\begin{align*}\n\\small P(dice\\; is\\; 4\\; |\\; dice\\; is &gt;3) = \\frac{\\#\\; ways\\;dice\\;can\\;be\\;4\\;\\;and\\;&gt;3}{\\#\\;ways\\;dice\\;can\\;be\\;&gt;3}=\\frac{1}{3}\n\\end{align*}\\]\nFor any two events A and B, conditional probabilities are defined as follows:11\n\\[\\begin{align*}\n\\small P(A | B) = \\frac{P(A\\;and\\; B)}{P(B)}\n\\end{align*}\\]\nThe equation above is a definition. It’s not the result of something else, it’s the way we define conditional probability. Rearranging it, though, gives us an important result.\n\n\n“and” Part 2: The Multiplication Rule\nFor any two events A and B, the Multiplication Rule states:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) = P(A|B)P(B)\n\\end{align*}\\]\nNote that P(B|A) = P(A and B)/P(A), so the multiplication rule can be extended:\n\\[\\begin{align*}\n\\small P(A\\; and\\; B) &\\small = P(A|B)P(B)\\\\\n\\small P(A\\; and\\; B) &\\small = P(B|A)P(A)\n\\end{align*}\\]\nIn other words, you can write it either way as long as the event that comes after the “|” also appears on it’s own.\nLet’s use this to answer the following question: What’s the probability that a dice is larger than 3 and even? By intuition, this should be 2/6 since there are two cases where both are true, but let’s verify with math!\nFirst, recall that P(&gt;3) and P(even) are both 1/2.\nP(&gt;3 | even) means that we’re look at the number of dice rolls that are larger than 3, but we’re only considering even dice rolls. We have 3 total dice rolls that are even, and 2 of those are larger than 3, so this probability is 2/3. Using the multiplication rule, P(&gt;3 and even) = P(&gt;3 | even)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\nThe other way works out the same. Given that the roll is larger than 3, there are 2 even rolls, which means that P(even | &gt;3) = 2/3. P(&gt;3 and even) = P(even | &gt;3)*P(even) = (2/3)*(1/2) = 2/6, which is what we got before!\n\n\nSpecial Cases: Independent or Disjoint\n\n\nDisjoint, a.k.a. Mutually Exclusive\nDisjoint events, also called mutually exclusive events, are events that cannot occur together. For example, the event that you roll a 4 and it’s also a 3. This simply does not work, so the probability is 0.\nMore formally, A and B are disjoint if P(A and B) = 0.\n\n\nThe Addition Rule for Disjoint Events\nIf A and B are disjoint, then P(A or B) = P(A) + P(B).12\nThis is actually why we were able to say P(even) = P(2 or 4 or 6) = P(2) + P(4) + P(6) = 3/6: the events “2”, “4”, and “6” are disjoint.\n\n\nIndependent\nTwo events are independent if the knowledge of one event tells you nothing about the other.13 For instance, if I flip two different coins and tell you that the first one was Heads, you still only have a 50/50 chance of guessing the second one.\nNotice the phrasing in the previous sentence: “If I tell you that the first one is heads…” That is, I’m restricting the sample space. Independence is all about conditional probabilities!\nFormally, A and B are independent if P(A | B) = P(A).\nThis adds further insight into conditional probabilities: P(A|B) is how likely A is, given that you know B happened. Knowledge of B changes your guess of the likelihood of A. If it doesn’t change your guess, then they are independent.14\nThe following app demonstrates this concept:\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/indep\")\n\nAnother lesson to take from the app above: Independence doesn’t look special. You can’t just tell that things are independent by looking at them.\n\n\nThe Multiplication Rule for Independent Events\nAny time I see a conditional probability, I immediately write down the formula. For dependence, we are saying:\nP(A|B) = P(A and B)/P(B)\nwhich is the same as\nP(A and B) = P(A|B)P(B)15\nIf two events are independent, then P(A|B)=P(A), therefore:\nP(A and B) \\(\\stackrel{indep}{=}\\) P(A)*P(B)16\nGet this tattood backwards on your forehead so you see it every time you look at yourself in the mirror: P(A and B) is ONLY equal to P(A)*P(B) when A and B are independent!!! Some textbooks start with this rule then move to the general rule, but far too many students start using P(A and B) = P(A)P(B) as if it’s always true. My entire thesis is based on whether you can say two things are independent, so it’s kind of a sore spot for me. DO NOT MIX THIS UP.\nFor example, are the events “even” and “&gt;3” independent? If you know that the dice roll is &gt;3, then there’s a 2/3 chance that it’s even. That is, P(even|&gt;3) = 2/3 \\(\\ne\\) 1/2 = P(even), so it’s not independent.\nAlternatively, we can calcuate P(even and &gt;3) = 2/3, but P(even)*P(&gt;3) = (1/2)*(1/2) = 1/4. Since 2/3 \\(\\ne\\) 1/4, these events are not independent.\n\n\nDisjoint means Dependent\nIndependence can be defined as “if you know that one event happened, you have no knowledge of the other event.” Disjoint can be defined as “if you know one event happened, you know for sure that the other one did not happen.” If two events are disjoint, they must be dependent. In fact, knowledge of one event means that you for sure know about the other - the exact opposite of independence!\n… except when one event is impossible. For instance, P(even and 7) = 0 since there are no dice rolls that are both even and 7, but this is also equal to P(even)*P(7) = 0 since there are no dice rolls that are 7."
  },
  {
    "objectID": "L05-Probability.html#word-problems",
    "href": "L05-Probability.html#word-problems",
    "title": "5  Probability Background",
    "section": "5.3 Word Problems",
    "text": "5.3 Word Problems\nQuestion 10.6 from the textbook:17\nThe National Survey on Drug Use and Health reports that 18.1% of all adults in the United States had a mental illness in 2014. Among adults with a substance use disorder, 39.1% had a mental illness. By comparison, only 16.2% of adults without a substance use disorder had a mental illness. The report also states that 3.3% of American adults had both a mental illness and a substance use disorder. Use the notation MI and SUD for mental illness and substance abuse disorder, respectively.\n\nExpress the four percents cited here as probabilities for a randomly selected American adult. Use proper probability notation.\nObtain the probability P(SUD|MI). Write a sentence reporting this probability in context.\n\nSolutions:\n\nThere are a couple of probabilities:\n\n“18.1% of all adults in the United States had a mental illness”: P(MI) = 18.1\n“Among adults with a substance use disorder, 39.1% had a mental illness.” The part that says “among adults with SUD” means that we’re only looking at people with SUD; we’re restricting the sample space. This is a condition, so our answer must be P(_ | SUD) = __. The blanks can be filled in as P(MI|SUD) = 0.391.\n“16.2% of adults without a substance use disorder had a mental illness.” The part that says “adults without a SUD” is also restricting the sample space, so our probability statement will be P(__ | no SUD) = __. The blanks are filled in as P(MI | no SUD) = 0.162.18\n“3.3% of American adults had both a MI and a SUD”. This clearly states and, so we are looking at P(MI and SUD) = 0.033\n\n\nPart b. is going to take a few steps. Let’s write down all the formulas that might help. Firstly. there’s no “or”, so that probably won’t do it.\n\nWant: P(SUD|MI)\n\nP(SUD|MI) = P(SUD and MI)/P(MI), so we need P(SUD and MI) and P(MI).\n\nHave:\n\nP(MI) = 0.181\nP(MI | SUD) = 0.391\nP(MI | no SUD) = 0.162\nP(MI and SUD) = 0.033\n\n\nBoth P(SUD and MI) and P(MI) are given in the question, so our answer is simply:\n\nP(SUD | MI) = P(SUD and MI)/P(MI) = 0.033/0.181 = 0.1823\n\nTherefore 18.23% of people with mental illness have substance abuse disorder.\nCompare this value to P(MI | SUD) = 0.391. In general, there is no easy relationship between P(A | B) and P(B | A). If you know what P(A | B) is, you can’t really guess at what P(B | A) is; you need a lot more information!"
  },
  {
    "objectID": "L05-Probability.html#two-way-tables",
    "href": "L05-Probability.html#two-way-tables",
    "title": "5  Probability Background",
    "section": "5.4 Two-Way Tables",
    "text": "5.4 Two-Way Tables\nI rigorously collected the following data19 on programming language usage for different disciplines using the most appropriate sampling methods.\n\n\n\n\nStats\nMath\nComp Sci\nTotal\n\n\n\n\nR\n90\n30\n40\n160\n\n\nPython\n10\n60\n100\n170\n\n\nMatLab\n15\n60\n15\n90\n\n\nJulia\n10\n10\n1\n21\n\n\nTotal\n125\n160\n156\n431\n\n\n\nFrom this table, we can calculate marginal and conditional probabilities.\nMarginal probabilities are calculated from the margins, which means that we ignore one of the variables. For example, P(Math) = 160/431 and P(Julia) = 21/431. Both of these proportions are based on the margins - they don’t take the other variable into account.\nConditional probabilities are the same idea as we saw earlier. Again, we are restricting our sample space by conditioning on another variable. For example, P(R | Stats) = 90/125, whereas P(Stats | R) = 90/160. The conditioning event determines which row/column we use. When we condition on Stats, we only look at the column labelled stats - we do not consider any of the other numbers. This is why P(R | Stats) has a numerator of 125, rather than 431.\nYou should be familiar with the following calculations:\n\nP(Stats) = 125/431\nP(Stats | Julia) = 10/21\nP(Matlab | Comp Sci) = ???20\nP(Stats and Julia) = 10/431\nP(Matlab and Stats) = 15/431\nP(Stats or Julia) = P(Stats) + P(Julia) - P(Stats and Julia) = 136/431\nP(Matlab or Stats) = 200\nP(Stats or R) = ???\nP(Stats or Math) = ??\n\nTwo-way tables can also be created in R using the table() function:\n\ndata(mtcars) # It's a very useful dataset\n\ncbind(mtcars$am, mtcars$cyl) # cbind BINDs Columns together\n\n      [,1] [,2]\n [1,]    1    6\n [2,]    1    6\n [3,]    1    4\n [4,]    0    6\n [5,]    0    8\n [6,]    0    6\n [7,]    0    8\n [8,]    0    4\n [9,]    0    4\n[10,]    0    6\n[11,]    0    6\n[12,]    0    8\n[13,]    0    8\n[14,]    0    8\n[15,]    0    8\n[16,]    0    8\n[17,]    0    8\n[18,]    1    4\n[19,]    1    4\n[20,]    1    4\n[21,]    0    4\n[22,]    0    8\n[23,]    0    8\n[24,]    0    8\n[25,]    0    8\n[26,]    1    4\n[27,]    1    4\n[28,]    1    4\n[29,]    1    8\n[30,]    1    6\n[31,]    1    8\n[32,]    1    4\n\ntable(mtcars$am, mtcars$cyl)\n\n   \n     4  6  8\n  0  3  4 12\n  1  8  3  2\n\n\nThe table above is telling us that there were 3 cars that were automatic (0) and had 4 cylinders.\n\n## Note: TRUE == 1, so the sum of a logical vector is the number of TRUEs\n## The \"&\" operator only returns true if BOTH conditions are true, i.e.\n## if mtcars$cyl == 4 AND mtcars$am == 0\nsum(mtcars$cyl == 4 & mtcars$am == 0)\n\n[1] 3"
  },
  {
    "objectID": "L05-Probability.html#self-study-questions",
    "href": "L05-Probability.html#self-study-questions",
    "title": "5  Probability Background",
    "section": "5.5 Self-Study Questions",
    "text": "5.5 Self-Study Questions\n\nExplain why P(A) + P(not A) must be 1.\nIf P(A) = 0.2, P(B) = 0.35,\n\nand P(A or B) = 0.75, find P(A and B).\nand P(A and B) = 0.15, find P(A or B).\nexplain why P(A and B) can only be as large as 0.2.\nexplain why P(A or B) must be at least 0.35.\n\nFor a 6-sided dice, show that the events “even” and “odd” are not independent.\nFor a 6-sided dice, show that the events “even” and “&gt;4” are independent.\nConsider flipping one coin and rolling one dice.\n\nList out all possible events (e.g., H1 for heads and 1, T4 for tails and a 4 on the dice).\nBased on your sample space, argue that P(T1) = 1/12.\nAre the events “coin is tails” and “dice is 1” independent? Give an intuitive and a mathematical reason.\n\nConsider a loaded dice, where the probability of 1, 2, 3, 4, and 5 are all 1/8.\n\nExplain why P(6) must be 3/8.\nWhat is P(even)?\nAre the events “even” and “&lt;3” independent?\n\n\nSolutions to Two-Way Table exercises: 3. 15/156; 8. 195/431; 9. 185/431"
  },
  {
    "objectID": "L05-Probability.html#footnotes",
    "href": "L05-Probability.html#footnotes",
    "title": "5  Probability Background",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThe singular form “die” is dieing out; the dictionary lists “dice” as singular noun, and the singular “dice” is clearer for new English speakers.↩︎\nAssuming there’s nothing unusual about your dice.↩︎\nMost of my work uses the Bayesian definition.↩︎\nThis is called a complement.↩︎\nThe sum of all probabilities must be 1.↩︎\nFor example, A = “Even”, B = “&gt;3”.↩︎\nThis lecture has some of the weirdest sentences.↩︎\nP(4 | &gt;3) just looks too confusing, so I added some words.↩︎\nTo remember this, I like to imagine the vertical bar falling on the the B and pushing it into the denominator.↩︎\nNot an important point: This is a rule, not a result. The General Addition Rule is a result of this rule, not the other way around.↩︎\nThe opposite of independence is dependence.↩︎\nJust like in correlations, dependence does not imply causation!↩︎\nThis equation is always true.↩︎\nThe “\\(indep\\)” over the equals sign is there to specify that this is only true if events are independent.↩︎\nBaldi, B. and DS. Moore. 2018. The Practice of Statistics in the Life Sciences. 4th Edition, W.H. Freeman and Company.↩︎\nThis is a great place to mention: There’s absolutely no reason why P(A|B) + P(A| not B) should add to 1.↩︎\nSource: I made it up.↩︎\nAnswer is at the end.↩︎"
  },
  {
    "objectID": "L07-Normal_Distributions.html#introduction",
    "href": "L07-Normal_Distributions.html#introduction",
    "title": "6  The Normal Distributions",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn this lecture we are looking at continuous distributions. Continuous distributions have an odd quirk. If a variable has a continuous distribution, then \\(P(X = x) = 0\\). That is, the probability of any specific value is infinitely small.\nThink of it this way: suppose that human heights go from 54 cm to 272 cm. For now, suppose all of these heights are equally likely. If we record heights to the nearest centimeter, there are 219 possible heights, so the probability that you are one of those heights is 1/219. If we round to the nearest mm, there are 21,900 different heights. As we get a more and more accurate measuring instrument, the probability of any given height goes to 0. It’s not that these heights are impossible, it’s that you’re probably not going to ever guess my exact height when we measure it with infinite accuracy.\nSo what do we do? How could we possibly calculate probabilities? Well, we measure ranges! You can’t guess my height exactly, but we can talk about the probability that my height is between 170 and 180 cm, or even the probability that my height would be 170, assuming we round to the nearest centimeter.\n\nSome Facts about Distributions\nBefore we begin, the following properties are true of any distribution, regardless of whether they are discrete or continuous.\n\nAll probabilities must be between 0 and 1.\nAll probabilities together must make 1.\n\nFor discrete, adding them all should get you to 1.\nFor continuous, the area under the density curve must be 1.1\n\nIf two events are disjoint, you must be able to add their probabilities.\n\nIt’s weird, but we have to define this as a rule first before we can calculate probabilities.\n\n\nThe first point should be obvious, and you won’t ever need to check whether the third point is true.\nThe second point is the important one: The total probability for all events must be 1. For continuous distributions like the normal distribution, that means that the area under the curve is 1.2"
  },
  {
    "objectID": "L07-Normal_Distributions.html#the-normal-distribution",
    "href": "L07-Normal_Distributions.html#the-normal-distribution",
    "title": "6  The Normal Distributions",
    "section": "6.2 The Normal Distribution",
    "text": "6.2 The Normal Distribution\nThe normal distribution is a way to define the probability of something using a function, but the function is complicated.3 Instead, we’ll jump right into how to use it and let software deal with the function.\nIn the introduction, I used the example of people’s heights. I made the assumption that all heights were equally likely, but this is just a bonkers thing to say. Instead, some heights are more likely than other heights. Of course, this doesn’t mean that, say, 170 cm is very unlikely, but 175 is likely, then 176 is unlikely, then 177 is very unlikely, then 178 is suddenly really likely again; most people have heights close to the average and heights further from the average are less likely. This is exactly what the normal distribution is for! Here’s what the normal distribution looks like:\n\nmu &lt;- 162.3\nsig &lt;- 7.11\nxseq &lt;- seq(mu-3*sig, mu + 3*sig, length.out = 300)\nyseq &lt;- dnorm(x = xseq, mean = mu, sd = sig) \n\nplot(xseq, yseq, type = \"l\",\n  main = \"Heights of Canadian Women\",\n  xlab = \"Height (cm)\", ylab = \"Prob. Density\")\n\n\n\n\nThe plot above has the highest point occurs at exactly 162.3, which is the best number I could find for the actual average height of Canadian women. This is denoted \\(\\mu\\). The width of the curve is a little trickier - how did I choose to make it go from 145 to 180? I could easily have stretched it out or squeezed it inwards in both directions. The width is defined by the standard deviation, which is denoted \\(\\sigma\\). Because of the way the normal distribution is defined, there’s nothing else we can change about it - knowing \\(\\mu\\) and \\(\\sigma\\) are enough to draw the entire curve.\n\n## Requires the \"shiny\" library\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/normShape\")\n\nIf we have a variable \\(X\\) that follows a normal distribution, we use the notation \\(X \\sim N(\\mu, \\sigma)\\).\n\n\n\n\n\n\nWarning\n\n\n\nSome textbooks use the notation \\(X \\sim N(\\mu, \\sigma^2)\\), i.e. they use \\(\\sigma^2\\) rather than \\(\\sigma\\). It’s like driving on the left or the right side of the road - both are fine, but we have to choose one and stick with it.\n\n\nThe idea that “most things are close to the center, and fewer things further away” can be very powerful. This applies to:\n\nHuman heights\nIncome for a given job position\nChange in stock price from day to day\n\nOn average the change is 0, but it does change. Small changes are much more likely than large ones, but large ones do happen.\nObviously, extreme events happen sometimes, and major changes can happen.\n\nIQ scores\nBirth weight\nHow much the prediction of a model differs from the truth"
  },
  {
    "objectID": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "href": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-1",
    "title": "6  The Normal Distributions",
    "section": "6.3 Calculating Normal Probabilities - Part 1",
    "text": "6.3 Calculating Normal Probabilities - Part 1\nThe height and width of the normal distribution are determined by the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)), and only the mean and standard deviation. The mean just moves the curve left and right, the standard deviation squeezes or stretches it.\nTo highlight this, we introduce something called the Empirical Rule, a.k.a. the 68-95-99.5 Rule. No matter what the mean of the distribution is, 68% of the probability is within 1 standard deviation of the mean. To say this another way, let’s extend our notation slightly. If \\(X\\sim N(\\mu,\\sigma)\\), we can say that:\n\\[\\begin{align*}\nP(\\mu - \\sigma \\le X \\le \\mu + \\sigma) \\approx 0.68\n\\end{align*}\\]\nTo phrase this in another way, if we were to draw random numbers from the normal distribution, 68% of them would be between 1sd below the mean and 1sd above the mean.\n\nset.seed(-4) # Ensure the same random numbers every time\n\n## generate 10000 random N(0,1) values\nx &lt;- rnorm(n = 10000, mean = 0, sd = 1) \n\n## You won't need to know how to write this code:\nsum(x &gt; -1 & x &lt; 1) # x is larger than -1 AND less than 1\n\n[1] 6766\n\n\nSo out of 10,000 random numbers from a N(0,1) distribution, 6,766 (67.66%) of them were above -1 but below 1. If we change the mean and sd, we still get the same results:\n\n## Mean is 4, sd is 30, so mean - 1sd = 4 - 30\n## Change the mean and sd for yourself to see what happens!\nmu &lt;- 4\nsigma &lt;- 30\nx2 &lt;- rnorm(n = 10000, mean = mu, sd = sigma)\nsum(x2 &gt; (mu - sigma) & x2 &lt; (mu + sigma)) # Not exactly 68%, but approximate!\n\n[1] 6845\n\n\nAs you can guess from the name “68-95-99.7 Rule”, 68% being within one sd is only part of the story. The 95 refers to 95% being within 2sd of the mean, and the 99.7 refers to 99.7% being within 3sd of the mean.\n\nsum(x &gt; -2 & x &lt; 2) # within 2sd of the mean\n\n[1] 9523\n\nsum(x &gt; -3 & x &lt; 3) # within 3\n\n[1] 9970\n\n## Try this with x2 as well!\n\nSome variant of the following image appears in countless textbooks:\n\nAs a small side note, the image above uses the word “data”. By this, it means that if this is the population, then 68% of all the data that it were possible to collect would be within one standard deviation of the mean. As we saw in the simulated data above, this number is almost never going to be perfect.\n\nTrickier calculations\nIf 68% of the data is between \\(\\mu - \\sigma\\) and \\(\\mu + \\sigma\\), then there’s still 32% of the distribution outside this range. The normal distribution is symmetric, so this 32% gets split exactly in half and 16% of the distribution is below \\(\\mu - \\sigma\\), and 16% is above \\(\\mu + \\sigma\\).\nBased on this calculation, we can say that 84% of any normal distribution is below \\(\\mu + \\sigma\\), and 84% is above \\(\\mu - \\sigma\\). Before we move on, draw out some normal distributions to convince yourself that 97.5% of any normal distribution is less than \\(\\mu + 2\\sigma\\).4\nYou should try the following calculations yourself, all of which can be done with basic arithmetic and the 68-95-99.7 Rule:\n\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu-\\sigma\\).\nBelow \\(\\mu+2\\sigma\\) and above \\(\\mu+\\sigma\\).\nAbove \\(\\mu + 2\\sigma\\) and below \\(\\mu + 3\\sigma\\)\nAbove \\(\\mu - 3\\sigma\\) and below 0."
  },
  {
    "objectID": "L07-Normal_Distributions.html#the-standard-normal-distribution",
    "href": "L07-Normal_Distributions.html#the-standard-normal-distribution",
    "title": "6  The Normal Distributions",
    "section": "6.4 The Standard Normal Distribution",
    "text": "6.4 The Standard Normal Distribution\nWe use a special letter (Z, pronounced “zed” because we’re Canadian) to denote a standard normal distribution. In particular, \\(Z\\sim N(0, 1)\\) is a normal distribution with mean 0 and standard deviation 1. Many many many many textbooks have a table in the back of them that gives probabilities for the standard normal distribution, and they call them \\(Z\\) tables.\nAll normal distributions have the exact same shape. In order to change the mean and sd, we can simply re-write the numbers on the axes. If we want to shift the whole curve to the left by 2 units, we can re-label the numbers on the x axis. If we change the sd, the plot might get “taller” or “shorter”, but if we zoom in on the plot we can make it look exactly the same!5\n\nStandardizing a Normal Distribution\nBecause they all look the same, we might as well work with just one of them! Suppose \\(X\\sim N(\\mu,\\sigma)\\). If we shift the whole curve to the left, then the mean shifts as well and the mean is 0. In other words, \\(X-\\mu \\sim N(0,\\sigma)\\). Now that the mean is at 0, \\(\\mu + 1\\sigma\\) is simply \\(\\sigma\\), \\(\\mu-3\\sigma\\) is \\(-3\\sigma\\), and so on. If we divide all of the numbers by \\(\\sigma\\), then \\(\\sigma\\) is simply 1, \\(-3\\sigma\\) is simply -3, and so on. To formalize this, if \\(x\\sim N(\\mu,\\sigma)\\), then\n\\[\\begin{align*}\n\\frac{X-\\mu}{\\sigma} = Z \\sim N(0, 1)\n\\end{align*}\\]\nThis is called standardizing a normal distribution. The resultant value is called the z-score.\nFor example, suppose a woman is 155.19 cm tall. If the true mean height of Canadian women is 162.3 and the standard deviation is 7.11, then this particular woman is exactly one standard deviation below the mean. This is the z-score, a.k.a. the standardized value; this woman’s z-score is -1.\nNow consider a woman who is 161.22 cm tall. Her z-score would be -0.152,6 meaning that she is 0.152 standard deviations below the mean.\nLet’s return to the 155.19 cm tall woman. If you take a woman at random from the population, what is the probability that the randomly chosen woman be be shorter than 155.19 cm? Based on the 68-95-99.7 rule, 68% of women are within one standard deviation of the mean, which is a range from 155.19 to 169.41. Since 68% of the women are betwen these two numbers, 16% of them are shorter than 155.19 (it is also true that 16% are taller than 169.41, but this was not required for the question).\nNow, what’s the probability that a randomly chosen woman is, say, shorter than 160 cm? This doesn’t fit nicely in the empirical rule, so we need another way to calculate probabilities. However, it’s worth stopping and trying to make a guess! The empirical rule tells us that 16% of women are below 155.19 cm, and we also know that 50% of women are shorter than the average of 162.3 cm (since the normal distribution is symmetric), so we expect that the answer is somewhere between 16% and 50%, probably closer to 50% since 160 cm is closer to 162.3 cm than it is to 155.19 cm."
  },
  {
    "objectID": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "href": "L07-Normal_Distributions.html#calculating-normal-probabilities---part-2",
    "title": "6  The Normal Distributions",
    "section": "6.5 Calculating Normal Probabilities - Part 2",
    "text": "6.5 Calculating Normal Probabilities - Part 2\nIn general, we use the cumulative distribution function (CDF, or cdf) to calculate probabilities. As with the cumulative probability tables we saw in the probability lectures, the cumulative probability calculates the area to the left of a particular point.7 Questions about the normal distribution generally come in three flavours:\n\nFind \\(P(X \\le a)\\)\nFind \\(P(X \\ge b)\\)\nFind \\(P(c \\le X\\le d)\\)\n\nThe cdf is defined as \\(P(X\\le x)\\), which allows us to answer questions like 1. For the standard normal distribution, a table of Z probabilities can be found at the back of the textbook. I’ve added a file that demonstrates how to use the Z-table in the Lecture Materials. This is something that is crucial to know for closed-book tests since you will need to caclulate probabilities somehow, but we can’t let you have a computer to run R! Before moving on, read “Intro to Ztable.pdf”.\nIn that file, there are some practice problems. Below, you’ll find a selection of solutions using R. For your own practice, try and calculate them with the Z-table (with some good drawings) and verify your answer with R.\n\n## 1. Find the probability of a z-value less than 1.11.\npnorm(1.11)\n\n[1] 0.8665005\n\n## 2. Find the probability of a z-value greater than 1.11\n1 - pnorm(1.11)\n\n[1] 0.1334995\n\n## 3. Find the probability of a z-value greater than -2.01 but less than 1.\npnorm(1) - pnorm(-2.01)\n\n[1] 0.8191292\n\n## 4. Verify the empirical rule: 68-95-99.7\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\npnorm(2) - pnorm(-2)\n\n[1] 0.9544997\n\npnorm(3) - pnorm(-3)\n\n[1] 0.9973002\n\n\nFor questions like \\(P(X\\ge x)\\), we can simply use the fact that \\(P(X \\ge x) = 1 - P(X&lt;x)\\). Since this is a continuous distribution and \\(P(X = x)=0\\), we also know that \\(P(X\\le x) = P(X&lt;x)\\) and we can just use the cdf. The last one is a little bit trickier.\nTo calculate the probability that a randomly chosen value will be within a given range, there are a few steps. Let’s use the same example as the textbook: If \\(X\\sim N(-2, 1)\\) find \\(P(-2.5\\le X\\le -1)\\). If we want to use the cdf, we need to re-write this in terms of \\(P(X\\le x)\\).\nHere’s how we do it. If we only find \\(P(X\\le-1)\\), then we have taken too much of the distribution. Everything to the left of -2.5 was something that should not have been included. So why don’t we just remove it? By this logic, we find \\(P(-2.5\\le X \\le -1) = P(X\\le -1) - P(X \\le -2.5)\\). This is shown graphically below:\n\nReturning to the heights example, the probability of a randomly chosen woman being less than 160 cm can be calculated as: \\[\n\\frac{x - \\mu}{\\sigma} = \\frac{160 - 162.3}{7.11} = -0.323488045\n\\] We can now look up -0.323 on the normal table. Try that out, and verify that you get the same value here:\n\npnorm(-0.323488045)\n\n[1] 0.3731628\n\n\nNote that R will do the standardization for you if you ask it politely.\n\npnorm(q = 160, mean = 162.3, sd = 7.11)\n\n[1] 0.3731628\n\n\nI have created a shiny app that lets you explore these calculations8. Feel free to use this to answer the questions in this lecture, and then double check the answers with the z-table.\n\n## install.packages(\"shiny\") # Run this if you get an error about \"package not found\"\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\nExamples\n\n\nEx1: P(X &lt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value less than 0?\n\nSolution 1: Standardize and Z-table. I’ll split this up into steps:\n\nStandardize: \\((x-\\mu)/\\sigma = (0 - 4)/2 = -2\\).\nFind -2 on the Z table: -2=-2.00, so this will be in the row labelled -2.0 and the column labelled 0.00,9 which is 0.0228.\nConclude: 2.28% of the N(4,2) distribution is below 0.\n\nSolution 2: Empircal rule.\n\nBefore calculating a normal probability, try and estimate how many standard deviations away from the mean the value is. In this case, 0 is 2 standard deviations from 4. The 68-95-99.7 rule states that 95% of the distribution is outside the range from \\(\\mu - 2\\sigma\\) to \\(\\mu + 2\\sigma\\), so 5% is outside of this range. This means that 2.5% is on either side, which means that 2.5% is below 0.\n\n\n\nA short version of Solution 2: By the 68-95-99.7 Rule, 95% is between 0 and 8. Therefore, 2.5% must be less than 0.\nAs you can see, the 68-95-99.7 rule is approximate. However, I highly recommend doing many practice problems with it. On a multiple choice question, if you can figure out the answer with the Empirical Rule than you might be able to guess the correct answer much quicker. You won’t get the exact answer, but if there’s only one answer that’s close to your guess, then that’s probably it.10\nSolution 3: R.\n\n## Standardize:\npnorm((0 - 4)/2)\n\n[1] 0.02275013\n\n## Same answer, without standardizing:\npnorm(q = 0, mean = 4, sd = 2)\n\n[1] 0.02275013\n\n\n\nIf \\(X\\sim N(1234, 56)\\), what’s the probability of a number smaller than 1432.\n\nBefore we begin: What do we expect the number to be? The mean is 1234, which is smaller than 1432. Is it a little smaller, or is it a lot smaller? Compared to the standard deviation, it’s a lot smaller. By the empirical rule, the vast majority of the distribution is below \\(\\mu + 3\\sigma\\), which is approximately 1400.11 We should expect an answer close to 1, since the area under the normal distribution is 1.\nSolution 1: \\((x-\\mu)/\\sigma = (1432-1234)/56 = 3.54\\), which is not on the Z table. When this happens (and we don’t have access to technology), we simply say the answer is 1.12\nSolution 2: The value we’re interested in isn’t 1, 2, or 3 standard deviations from the mean, so the Empirical Rule doesn’t apply. However, we can guess that our probability will be close to 1 since it’s larger than 3 standard deviations away.\nSolution 3: R.\n\npnorm(1432, mean = 1234, sd = 56)\n\n[1] 0.9997967\n\n\nIdeally, you would only ever use intuition from the Empirical rule, or use R. The Z-table is super convenient for written, in-person exams. It’s also nice for situations where you don’t have a computer with R available.\n\n\nP(X &gt; x)\n\nIf X has a mean of 4 and a sd of 2, what’s the probability of a value greater than 0?\n\nBefore we start: Use the empirical rule! 0 is 2sd below the mean, so the answer should be close to 97.5%\nWith the Z table: \\((x-\\mu)/\\sigma = -2\\), and we’ve already found this on the table as 0.0228. Since we’re looking at the right tail, our answer is 1 - 0.0228 = 0.9772.\nWith R:\n\n1 - pnorm(0, mean = 4, sd = 2)\n\n[1] 0.9772499\n\n\n\nSuppose \\(X\\sim N(23, 23)\\). What’s the probability of a value larger than 23?\n\nBefore we start: The normal distribution is perfectly symmetric, which we have learned means that the mean is equal to the median. The median marks the point where 50% of the distribution is smaller. So before doing any work, we know that the answer must be 50%\n\npnorm(23, 23, 23)\n\n[1] 0.5\n\n\n\n\nP(a &lt; X &lt; b)\n\nIf \\(X\\sim N(0, 1)\\), what’s the probability of a value between -1.52 and -0.5?\n\nSolution 1: We have a standard normal value, so we can look these values up directly. P(Z &lt; -1.52) = 0.064313 and P(Z &lt; -0.5) = 0.3085.14 We want the area between these two values. P(Z &lt; -0.5) contains everything from negative infinity to -0.5, but we only want values from -1.52 to -0.5. To fix this, we remove everything from negative infinity to -1.52. Our answer is 0.3085 - 0.0643 = 0.2442.\nSolution 2: R.\n\npnorm(-0.5) - pnorm(-1.52)\n\n[1] 0.2442821\n\n\nI have made a shiny app for you to visualize this:\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pnorm\")\n\n\n\\(X \\sim N(2,3)\\), find \\(P(-1 &lt; X &lt; 5)\\)\n\nBefore we begin: This is the empirical rule for 1sd. The answer is 68%.\nWith a Z table: We calculate the z-score individually, then subtract the probabilities in a way that makes sense.15 \\(P(X &lt; -1) = P((X-\\mu)/\\sigma &lt; (-1 - \\mu)/\\sigma) = P(Z &lt; (-1 - 2)/3) = P(Z &lt; -1) = 0.1587\\). Similarly, \\(P(X &lt; 5) = P(Z &lt; 1) = 0.8413\\). The answer is 0.8413 - 0.1587 = 0.6826, which is very close to what we got with the Empirical Rule.\nWith R:\n\npnorm(5, mean = 2, sd = 3) - pnorm(-1, mean = 2, sd = 3)\n\n[1] 0.6826895\n\n\n\n\nGoing Backwards\nWhat’s the first quartile of an N(2,3) distribution? It’s the point at which 25% of the distribution is smaller. In other words, P(X &lt; Q1) = 0.25. How do we find Q1?\nWe can look up 0.25 as a probability. That is, as a value in the body of the Z table. This will give us the corresponding z-score.16 Unfortunately, 0.25 isn’t in the table. The closest values are 0.2514 (which is a Z score of -0.67) and 0.2483 (Z score of -0.68). On a test situation, -0.67 and -0.68 would both be valid answers, as would -0.675.\nIn R, the “q” family of functions are the reverse lookup functions. That is, You tell them the probability, and they return the z-score.\n\nqnorm(0.25, mean = 0, sd = 1)\n\n[1] -0.6744898\n\n\nHowever, we’re not done yet! We found the quartile for a standard normal distribution. We have to go backwards in the standardization formula. In essence, we have found Z and we need to find X.\n\\[\\begin{align*}\n\\frac{x - \\mu}{\\sigma} = z \\Leftrightarrow x = z\\sigma + \\mu\n\\end{align*}\\]\nTo finish this question, we say that the first quartile of a N(2, 3) distribution is -0.67*3 + 2 = -0.01.17\nIn R:\n\nqnorm(0.25, mean = 2, sd = 3)\n\n[1] -0.02346925"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "href": "L07-Normal_Distributions.html#problems-verifying-the-empirical-rule",
    "title": "6  The Normal Distributions",
    "section": "6.6 Problems: Verifying the Empirical Rule",
    "text": "6.6 Problems: Verifying the Empirical Rule\n\n68-95-99.7"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-z-scores",
    "href": "L07-Normal_Distributions.html#problems-z-scores",
    "title": "6  The Normal Distributions",
    "section": "6.7 Problems: Z-scores",
    "text": "6.7 Problems: Z-scores\n\n\n\\(P(z \\le 2.25)\\)\n\\(P(z \\le -2.25)\\)\n\\(P(z \\ge 2.25)\\)\n\\(P(z \\ge -2.25)\\)\n\n\n\n\\(P(-2 \\le z \\le 2)\\)\n\n$P(Z ; and; Z )\n\n\\(P(2 \\le z \\le -2)\\)\n\\(P(0 \\le z \\le 2)\\)\n\\(P(-2 \\le z \\le 0)\\)\n\\(P(Z \\ge 2\\; or\\; Z \\le -2.5)\\)\n\n\n\n\\(P(Z \\le z) = 0.5\\)\n\\(P(Z \\ge z) = 0.4238\\)\n\nWhat is \\(z\\)?"
  },
  {
    "objectID": "L07-Normal_Distributions.html#problems-standardizing",
    "href": "L07-Normal_Distributions.html#problems-standardizing",
    "title": "6  The Normal Distributions",
    "section": "6.8 Problems: Standardizing",
    "text": "6.8 Problems: Standardizing\n\nThe birthweights of cute widdle babies born at full-term is \\(N(3350, 440)\\).\n\nLow birthweight babies are those with a weight less than 2500. Probability of this?\nHigh birthweight is above 4200. Probability?\nProbability of either low or high?\n\n\nA paper claimed that their control group was normal with a mean of 7 headaches per month, and the treatment group had a mean of 3.\nThe paper later claims that there’s only a 10% chance of seeing fewer than 3 headaches in the control group.\nThe paper never provided the sd. What is it?"
  },
  {
    "objectID": "L07-Normal_Distributions.html#participation",
    "href": "L07-Normal_Distributions.html#participation",
    "title": "6  The Normal Distributions",
    "section": "6.9 Participation",
    "text": "6.9 Participation\n\n\n\\(P(Z &lt; 1.5)\\)\n\\(P(Z &gt; -1.5)\\)\n\\(P(Z &lt; 1.2 or Z &gt; 1.3)\\)\n\\(X\\sim N(0,2)\\), find \\(P(X &lt; 2)\\)\n\\(X\\sim N(\\mu, 5)\\) and \\(P(X \\le 2) = 0.25\\), find \\(\\mu\\)\n\\(X \\sim N(2, 4)\\). Find the IQR."
  },
  {
    "objectID": "L07-Normal_Distributions.html#summary",
    "href": "L07-Normal_Distributions.html#summary",
    "title": "6  The Normal Distributions",
    "section": "6.10 Summary",
    "text": "6.10 Summary\n\nMost values are close to the mean, with fewer values as you get further away.\nThe mean and sd are sufficient to draw the whole curve.\nProbabilities are areas. The area of a single point is 0.18\n68% is within one sd of the mean, 95% within 2 sd, and 99.7% within 3 sd\n\n\\(P(\\mu - 1\\sigma \\le X \\le \\mu + 1\\sigma) \\approx 0.68\\).\n\\(P(\\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma) \\approx 0.95\\).\n\\(P(\\mu - 3\\sigma \\le X \\le \\mu + 3\\sigma) \\approx 0.997\\).\n\nFor standard normal, the values on the x axis are z-score.\nThe cdf, P(X &lt;= x), is used to calculate areas.\n\nThe table can be found in the back of the textbook for standard normal. To standardize, use the formula \\((x-\\mu)/\\sigma\\).\npnorm(x, mean = 0, sd = 1) gives the standard normal cdf. If mean and sd are not specified, pnorm() assumes you want standard normal.\n\n\\(P(a \\le X \\le b) = P(X \\le b) - P(X \\le a)\\)\n\nEmpirical rule: pnorm(1) - pnorm(-1); pnorm(2) - pnorm(-2); …\n\nYou need a lot of practice with these kinds of problems. Do not check the answers prematurely.\n*norm functions:\n\nrnorm(n, mean, sd) generates random numbers\ndnorm(x, mean, sd) gives the height of the curve at the point x. This is not a probability.\npnorm(q, mean, sd) = \\(P(X \\le q)\\).\nqnorm(p, mean, sd) finds \\(q\\) such that \\(P(X \\le q) = p\\).\n\nIt is the backwards version (inverse function) of pnorm().\npnorm(qnorm(0.5)) returns 0.5, qnorm(pnorm(2)) returns 2."
  },
  {
    "objectID": "L07-Normal_Distributions.html#self-study-questions",
    "href": "L07-Normal_Distributions.html#self-study-questions",
    "title": "6  The Normal Distributions",
    "section": "6.11 Self-Study Questions",
    "text": "6.11 Self-Study Questions\n\nFor each of the probability statements, draw the normal distribution and add shading for the probability. For example, P(Z &gt; 1) should be a normal distribution with everything under the curve and larger than 1 shaded in. This is a very good way to help internalize the fact that all probabilities are areas.\nIn P(Z &lt; 1.32) = 0.9066, what do 1.32 and 0.9066 represent? Where are they on the Z table. If I were to give you one and not the other, could you find the missing number?\nWrite down all of the probability statements on a separate piece of paper. Solve them without looking at these notes. More practice, more better.\nPicture two normal distributions: one looks taller, and one looks wider. Which one has the larger standard deviation?\nExplain why the standard deviation does not affect the shape of the normal distribution. Now, explain why it does affect the shape.19"
  },
  {
    "objectID": "L07-Normal_Distributions.html#more-questions",
    "href": "L07-Normal_Distributions.html#more-questions",
    "title": "6  The Normal Distributions",
    "section": "6.12 More Questions",
    "text": "6.12 More Questions\nIf you have not calculated at least 50 or 60 different normal probabilities by the midterm, you have probably not done enough practice.\nFor each of these questions, start by trying to use the empirical rule, then use the Z table, then confirm your answer with R. Answers with R are shown below, but you should only check these once you’re confident with your own answer.20\n\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 1?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 2?\n\\(X \\sim N(0,2)\\), what percent of the distribution is above 3?\n\\(X \\sim N(-2, 500)\\), find the 75% quantile (aka Q3).\n\\(X \\sim N(3.14, 15.9)\\), what proporion of values are between 2.71 and 8.28?\nSuppose 25% of a normal distribution is below 0, and the mean of this distribution is 1. What’s the standard deviation?21\nWhat to Expect claims that the average baby weighs about 7.5 lbs, with a “normal”22 range of 5.8 to 10 lbs. If the “normal” range is defined as the middle 95%, what is the standard deviation of birth weights?\nYou’re asked to estimate the number of M&M’s in family-sized bags. You’re pretty sure that they are normally distributed and you think the mean is 600. How do you go about guessing the sd? One way is to say that you think it’s “unlikely” that there are more than 650 M&Ms in any given bag.23\n\nIf “unlikely” = 10%, that is, only 10% of the bags have more than 650 M&M’s, what is the sd?\nIf “unlikely” = 5%, what is the sd?\n\nIn the population of Canadian women, what’s the probability that a randomly selected woman is further than 1.7 standard deviations from the mean?\nThere’s a peculiar model that applies to certain kinds of data. If you have \\(\\mu = \\sigma\\), then the normal distribution has certain nice properties.24 Suppose \\(X\\sim N(\\theta, \\theta)\\), where \\(\\theta\\) is just a stand-in for the mean and variance. If \\(P(X &lt; 8) = 0.2\\), what is \\(\\theta\\)?25\n\nI’m going to say it again before you check the answers: Pre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge. You should spend time struggling to convince yourself that you did it right. On an exam, you won’t have the answers so you’ll feel that struggle. Practice the exam struggle now, then you’ll be more confident in your answer on exams.\nHave you ever had that feeling that you knew the material because you could do all of the practice problems, but when you get the exam you forgot everything? That’s because you checked the answers before struggling. You taught yourself to anticipate the answers of those particular questions, rather than teaching yourself the material. The struggling is where you learn. It’s the same as exercise: no pain no gain.\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Questions 1, 2, and 3\n## ~~~~~~~~~~~~~~~~~~~~~~~\n1 - pnorm(c(1,2,3), mean = 0, sd = 2)\n\n[1] 0.3085375 0.1586553 0.0668072\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q4\n## ~~~~~~~~~~~~~~~~~~~~~~~\nqnorm(0.75, mean = -2, sd = 500)\n\n[1] 335.2449\n\nqnorm(0.75)*500 - 2 # Alternative, using standard normal\n\n[1] 335.2449\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q5.\n## ~~~~~~~~~~~~~~~~~~~~~~~\npnorm(8.28, 3.14, 15.9) - pnorm(3.14, 3.14, 15.9)\n\n[1] 0.1267548\n\n## Alternative version, with standard normal\na &lt;- (3.14 - 3.14)/15.9\nb &lt;- (8.28 - 3.14)/15.9\npnorm(b) - pnorm(a) # P(a &lt; z &lt; b) = P(Z &lt; b) - P(Z &lt; a)\n\n[1] 0.1267548\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q6: x = z*sigma + mu =&gt; sigma = (x-mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(0 - 1)/qnorm(0.25)\n\n[1] 1.482602\n\n## Verify that 0 is the first quartile\nqnorm(0.25, mean = 1, sd = (0 - 1)/qnorm(0.25)) # Good!\n\n[1] 0\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q7: empirical rule: 5.8 = mu - 2*sigma, so sigma = (7.5 - 5.8)/2\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(7.5 - 5.8)/2\n\n[1] 0.85\n\n## However, if 10 = mu + 2*Sigma,\n(10 - 7.5)/2\n\n[1] 1.25\n\n## The normal distribution doesn't work because this is a *skewed distribution*\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q8.a) sigma = (x - mu)/z\n## ~~~~~~~~~~~~~~~~~~~~~~~\n(650 - 600)/qnorm(0.9)\n\n[1] 39.01521\n\n## Verify:\npnorm(650, 600, 39.01521)\n\n[1] 0.9\n\n## Q8b:\n(650 - 600)/qnorm(0.95)\n\n[1] 30.39784\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q9: The \"Canadian Women\" part is irrelevant.\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## The area WITHIN the range is:\npnorm(1.7) - pnorm(-1.7)\n\n[1] 0.9108691\n\n## So the area outside this range is:\n1 - (pnorm(1.7) - pnorm(-1.7))\n\n[1] 0.08913093\n\n## Why is the \"Canadian Women\" part irrelevant?\n## The lower bound will be mu - 1.7*sd = 150.213. When we\n## standardize this, we get z = (x-mu)/sd = 1.7, so we'd use\n## 1.7 in the standard normal distribution\n\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## Q10\n## ~~~~~~~~~~~~~~~~~~~~~~~\n## P(X &lt; 8) = 0.2, so let z = qnorm(0.2)\n## z = (x - mu)/sigma = (8 - theta)/theta\n## and therefore theta = 8/(z + 1)\n8/(qnorm(0.2) + 1)\n\n[1] 50.51182"
  },
  {
    "objectID": "L07-Normal_Distributions.html#footnotes",
    "href": "L07-Normal_Distributions.html#footnotes",
    "title": "6  The Normal Distributions",
    "section": "",
    "text": "This is done with integrals, but we won’t actually do this in this course.↩︎\nFor continuous distributions, “probability” and “area under the curve” are synonyms.↩︎\n\\((2\\pi\\sigma^2)^{-1/2}\\exp\\left(\\frac{(x-\\mu)^2}{-2\\sigma^2}\\right)\\)↩︎\nI generally keep a running tally of the number of normal distributions I draw on the board. Last time I did this, I was almost at 100. The moral: you should be drawing a lot of normal distributions!!!↩︎\nThis is also the reason why the empirical rule works! If you change the labels on the plot, \\(\\mu+\\sigma\\) stays in the same place so you can calculate the same probability.↩︎\nThe negative is important!↩︎\nRecall that \\(P(X=x) = 0\\) in continuous distributions, so we look at ranges.↩︎\nIf you’re curious, yes I’ve made a lot of Shiny apps. You can find them all here: https://github.com/DBecker7/DB7_TeachingApps↩︎\nThe rows are the digits before and after the decimal, the column is the second digit after the decimal.↩︎\nYou need to trust your ability to use the Empirical Rule, though.↩︎\nQuick maths - we’re just trying to get an okay guess, not the exact answer right now.↩︎\nIf the z-score were -3.54, we’d say the probability is 0.↩︎\nRow labelled -1.5, column labelled 0.02.↩︎\nVerify this!↩︎\nP(X &lt; 5) - P(X &lt; -1)↩︎\nRecall: the body of the Z table are probabilities, the margins are z-scores.↩︎\n-0.68*3 + 2 and -0.675*3 + 2 would also be acceptable.↩︎\nP(X=x) = 0↩︎\nHint: Use the app with “Sticky Axes” checked and unchecked.↩︎\nPre-emptively checking the answer destroys any chance of learning and creates a false sense of knowledge.↩︎\nHint: Find the z-score for Q1, then fill out the standardization formula with the values you have.↩︎\nNormal as in “usual”, not as in the normal distribution.↩︎\nThis is actually a very useful way to think about distributions, especially in Bayesian statistics.↩︎\nSorry, the details are far beyond the scope of this course.↩︎\nThis is one of the hardest questions you will encounter.↩︎"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#introduction",
    "href": "L09-Binomial_Probabilities.html#introduction",
    "title": "7  Binomial Probabilities",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nProbability models are ways of laying out all possible events as well as the probability of each event. For things like coins and dice, everything has the same probability and things work out nicely. In Two-Way tables, we have all the probabilities laid out in front of us. The Binomomial distribution is our first foray into a formulaic approach to probabilities.\n\nWith Coins\nIf we flip two coins, the outcomes are {HH, HT, TH, TT} and each of these are equally likely. Instead of looking at each event, what is the probability that there are 0 heads? 1 head? 2 heads?\nFor 0 and 2 heads, there is only 1 possibility, so it must be 1/4 for each. For 1 head, there are 2 possibilities, each with probability 1/4, so the answer is 2*1/4.3\nLet’s flip three coins. The outcomes are {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}, so each outcome has a probability of 1/8. Another way to come to this number is to look at the probability of heads: For HHH, the probability is 0.5*0.5*0.5 since there’s a 50% chance of heads and each coin flip is independent.4\n\n\n\n# Heads\nOutcomes\nProbability\n\n\n\n\n0\nTTT\n1/8\n\n\n1\nTTH, THT, HTT\n1/8+1/8+1/8 = 3/8\n\n\n2\nHHT, HTH, THH\n3/8\n\n\n3\nHHH\n1/8\n\n\n\nAlright, let’s do 4 coins. How many ways are there to get, say, 2 heads out of four flips? You can bet that a smart mathemetician has figured out a way to do this without writing them all out again! This is called combinatorics, and includes a lot of things that are not relevant right now. We’ll focus on the choose function. For three coins, “3 choose 1” means “out of 3 options, choose 1 of them”. Sometimes this is shortened to “3C1”. As we saw in the table above, there’s 1 way to choose nothing (no heads), 3 ways to choose 1 thing, 3 ways to choose 2 things, and 1 way to choose 3 things.5 In R:\n\nchoose(n = 3, k = 0) # 3C0\n\n[1] 1\n\nchoose(n = 3, k = 1) # 3C1\n\n[1] 3\n\nchoose(n = 3, k = 2) # 3C2\n\n[1] 3\n\nchoose(n = 3, k = 3) # 3C3\n\n[1] 1\n\nchoose(n = 4, k = 2) # 4C2\n\n[1] 6\n\n\nSo for 4 coins, there is 4C2 = 6 ways to get two heads.6 What’s the probability of each of these 6 outcomes? Since there’s a 0.5 chance of heads and a 0.5 chance of tails, there’s a 0.5*0.5*0.5*0.5 = 0.5\\(^4\\) = 0.0625 chance. That means that there’s a 6*0.0625 = 0.375 chance of getting two heads out of four flips.7\nJust to be complete, let’s do this again for 5 coins. We’re already at the point where we need the choose function because there are too many outcomes to write out by hand. Let’s calculate some probabilities with R:\n\n## Probability of 4 heads out of 5 flips:\nchoose(5, 4) * 0.5^5\n\n[1] 0.15625\n\n## Probability of 3 heads out of 5 flips:\nchoose(5, 3) * 0.5^5\n\n[1] 0.3125\n\n\nFor completeness, let’s make sure these all add up to 1:\n\n## I really hope this adds to 1\n(choose(5, 0) * 0.5^5) +\n  (choose(5, 1) * 0.5^5) +\n  (choose(5, 2) * 0.5^5) +\n  (choose(5, 3) * 0.5^5) +\n  (choose(5, 4) * 0.5^5) +\n  (choose(5, 5) * 0.5^5)\n\n[1] 1\n\n\n\n\nWith Dice\nIf I roll two dice, what’s the probability that exactly 1 of them is a 3? One way this can happen is if the first dice is a 3 and the second one is not a 3. This probability is P(first dice is 3)*P(second dice is not 3). We know that P(first dice is 3) is easily seen to be 1/6. On the other hand, P(second dice is not 3) can be calculated as 1 - P(second dice is 3)8 = 1 - 1/6 = 5/6. So the probability is (1/6)*(1 - 1/6).\nWe can also have exactly one 3 if the first dice is not 3 but the second dice is 3. This has the same probability as the other way around: (1 - 1/6)*(1/6).\nNotice how this is the second of two options. Again, we get to use the choose function:\n\n## Probability of exactly one 3 in two dice rolls\nchoose(2, 1) * (1/6)*(1-1/6)\n\n[1] 0.2777778\n\n\nIf we roll 18 dice, what’s the probability that exactly four of them are 5? Regardless of the order, we have four dice that are 5 and 14 dice that are not 5. The probability of any one of the outcomes is (1/6)\\(^4\\)*(1 - 1/6)\\(^{14}\\) = 0.000060098. That’s pretty unlikely for this exact dice combination of dice rolls! But how many ways are there for this to happen? There are 18C4 = 3060, which is a lot, so there are a lot of opportunities for things with small probabilities. The probability of exactly four 5s out of 18 rolls is 18C5*(1/6)4*(1-1/6)14 = 0.1840. Even though an individual dice roll is unlikely, there are a lot of dice rolls that meet our criteria of four 5s out of 18 rolls!"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#binomial-probabilities",
    "href": "L09-Binomial_Probabilities.html#binomial-probabilities",
    "title": "7  Binomial Probabilities",
    "section": "7.2 Binomial Probabilities",
    "text": "7.2 Binomial Probabilities\nIn general, if we have \\(n\\) trials and the probability of the event of interest, a.k.a. success, is \\(p\\), then\n\\[\\begin{align*}\nP(x\\text{ successes in }n\\text{ trials}) = nCx*p^x*(1-p)^{n-x}\n\\end{align*}\\]\nFor the dice example, “x successes in n trials” can be interpreted as “four 6s in 18 trials”, where \\(x=4\\), \\(n=18\\), and the “14 rolls that are not four” comes from \\(n-x=14\\).\n\nConfusing (but Useful) Notation\nIn the statement above, we used \\(x\\) to refer to the number of heads. I like this. Let’s keep doing this.\nFor Binomial probabilities, we use the notation:\n\\[\\begin{align*}\nX \\sim B(n,p)\n\\end{align*}\\]\nwhich is read as “the random variable X is distributed as Binomial with n trials and probability of success p”.9 The “\\(\\sim\\)” just means “is distributed as”, which tells us where the probabilities are distributed. This is why \\(nCx*p^x*(1-p)^{n-x}\\) is called the probability distribution function, or pdf.10\nA random variable is just a variable that has a probability distribution,11 such as the number of heads out of 5 flips. Before flipping these coins we have no idea how many heads there will be, but we know the probability of each number. We always use upper case letters for random variables. Once we actually have a value (say, 1 heads), we use lower case. We often use the notation \\(P(X = x)\\) to refer to “the probability that the random variable \\(X\\) will have the specific value of \\(x\\)”. In other words, \\(X\\) is the unknown that could be anything, \\(x\\) is the specific probability that we’re interested in.\nI just want to talk about “distributions” a little bit more. A distribution tells you where the probabilities are. For coins, 50% of the probability is in Heads, 50% is in in tails. When we talk about “is the dice a 3?”, one-sixth of the probability is distributed to the 3 and five-sixths are distributed elsewhere.\nTo summarise, saying that \\(X \\sim B(n,p)\\), or that \\(X\\) is distributed as a Binomial random variable with \\(n\\) trials and probability of success \\(p\\), is the exact same as saying that \\(P(x\\text{ successes in }n\\text{ trials})\\) can be found using the equation \\(nCx*p^x*(1-p)^{n-x}\\). This is what it means to have a probability distribution function.\nTo see all of these probabilities at once, we can plot this as a graph. To reduce coding, let’s look at \\(X\\sim B(3, 0.4)\\):\n\nx &lt;- c(0, 1, 2, 3) # X values\ny &lt;- c(\n    choose(3, 0) * (0.4)^0 * (1 - 0.4)^3,\n    choose(3, 1) * (0.4)^1 * (1 - 0.4)^2,\n    choose(3, 2) * (0.4)^2 * (1 - 0.4)^1,\n    choose(3, 3) * (0.4)^3 * (1 - 0.4)^0\n)\n\n#plot(x,y) # This will plot them, but it looks kinda bad\n## Since X can only be 0, 1, 2, or 3, let's us a bar plot!\nbarplot(y, names = x,\n  main = \"pdf of B(3, 0.4)\", xlab = \"x\",\n  ylab = \"3Cx * p^x * (1-p)^(3-x)\")\n\n\n\n\nNotice how 1 is the most likely value, with 2 being much less likely. This makes sense - if the probability of heads is less than 0.5, we expect that more of the coin flips will be tails! If the probability of “heads” were 0.5, then we would expect 1 and 2 to be equally likely.\n\n\nExamples\n\nSuppose I have a coin that is weighted so that Heads comes up 80% of the time. What is the probability that I get 8 heads in 10 flips?\n\n\nchoose(10, 8) * (0.8)^8 * (1-0.8)^2\n\n[1] 0.3019899\n\n\n\nWhat’s the probability that I get anything other than 10 flips?\n\nTry it yourself!\n\nWhat’s the probability that I get more than 8 heads in 10 flips?\n\nSince the events “9 heads” and “10 heads” are disjoint, we can calculate these individually and add them together.\n\n\n\nchoose(10, 9) * (0.8)^9 * (1-0.8)^1 +\n  choose(10, 10) * (0.8)^10 * (1-0.8)^9\n\n[1] 0.2684355\n\n\n\n\nIn R\nTyping out the whole formula is getting boring. Surely R, a statistical programming language, has a way to do it for me, right? Of course!\n\nchoose(10, 8) * (0.8)^8 * (1 - 0.8)^2\n\n[1] 0.3019899\n\ndbinom(x = 8, size = 10, prob = 0.8)\n\n[1] 0.3019899\n\n\nThe dbinom() function has exactly the arguments that you would expect. Lower case x is the specific value, size is the number of coin flips, prob is the probability of success. The d stands for “density”, which for our purposes is the same as “distribution”.\nAs a special note, R will take a vector for x. We can find multiple probabilities at once:\n\ndbinom(x = c(8, 9, 10), size = 10, prob = 0.8)\n\n[1] 0.3019899 0.2684355 0.1073742\n\n\nThis allows us to easily plot the pdf:\n\nx &lt;- 0:10 # a vector of the numbers from 0 to 10\n\n## note: x is the name of the object AND the argument,\n## hence why I wrote \"x = x\"\ny &lt;- dbinom(x = x, size = 10, prob = 0.8)\n\nbarplot(height = y, names = x)"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "href": "L09-Binomial_Probabilities.html#cumulative-binomial-probabilities",
    "title": "7  Binomial Probabilities",
    "section": "7.3 Cumulative Binomial Probabilities",
    "text": "7.3 Cumulative Binomial Probabilities\nA cumulative probability is the probability of observing up to \\(x\\) successes in \\(n\\) trials. In other words, this is \\(P(X \\le x)\\): the probability that the random variable \\(X\\) is smaller than or equal to some specific number \\(x\\). This is referred to as the Cumulative Distribution Function, or cdf. Unlike what we saw in the normal distribution, it really matters whether it’s \\(P(X\\le x)\\) or \\(P(X&lt; x)\\)!\nWhat’s the probability that we get at most 4 heads in 10 flips? That’s the same as the probability of 0 heads plus the probability of 1 heads plus the probability of 2 heads plus…\n\n## Note: R evaluates the arguments *in order*\n## It expects the arguments in the order of \"x, size, prob\",\n## so it assumes the first argument is x, the second is size,\n## and the third is prob.\ndbinom(x = 0, size = 10, prob = 0.5) +\n  dbinom(1, 10, 0.5) +\n  dbinom(2, 10, 0.5) +\n  dbinom(3, 10, 0.5) +\n  dbinom(4, 10, 0.5)\n\n[1] 0.3769531\n\n\nWhat about the probability of at most 40 heads in 100 flips? Do I have to type all that out?\nNope! We can use the pbinom() function. First, let’s verify it with what we’ve already calculated:\n\npbinom(q = 4, size = 10, prob = 0.5)\n\n[1] 0.3769531\n\n\nNow, let’s find the probability of at most 40 heads in 100 flips:\n\npbinom(40, 100, 0.5)\n\n[1] 0.02844397\n\n\nIt’s surprisingly small! Let’s look at the pdf to see why:\n\nx &lt;- 30:70 # The pdf is REALLY small outside this range\n\n## I'm going to colour the bars where x &lt;= 40\n## Start with a bunch of white bars by REPeating the colour\n## white for as many x values as we have\nmycols &lt;- rep(\"white\", length(x))\n## Next, we change the colour where x &lt;= 40\nmycols[x &lt;= 40] &lt;- \"red\"\n\n## Calculate the distribution function\ny &lt;- dbinom(x, 100, 0.5)\nbarplot(height = y, names = x, col = mycols)\n\n\n\n\n\nExamples\nWhat’s the probability of at least 40 heads in 100 flips? Be careful here: it matters whether I ask “at least” or “more than”. The cdf always calculates “less than or equal to”12, and the complement of this is “strictly greater than.13 If I’m looking for”strictly greater than”, I need to be careful what I use!\nIn this case, P(X \\(\\ge\\) 40) = P(X &gt; 39) = 1 - P(X \\(\\le\\) 39) = 1 - pbinom(39, 100, 0.5)\n\n1 - pbinom(q = 39, size = 100, prob = 0.5)\n\n[1] 0.9823999"
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "href": "L09-Binomial_Probabilities.html#properties-of-the-binomial-distribution",
    "title": "7  Binomial Probabilities",
    "section": "7.4 Properties of the Binomial Distribution",
    "text": "7.4 Properties of the Binomial Distribution\nI define “math” as the process of making up rules just to see what happens. The Binomial distribution isn’t just some abstract entity that we discovered - it’s a set of rules we created that seem to logically fit some situations.14 So first: what are the rules?\n\nBinomial Assumptions\nI’m going to motivate these assumptions first. If you’re the type that just wants to memorize, you can skip to the end of this section.\nWe’ve been talking about flipping coins and rolling dice, which helped motivate this distribution. We wouldn’t be teaching you this distribution if it only applied to dice and coins, so when can we apply it?\nConsider flipping a “sticky” coin twice. It starts with a 50/50 chance of being heads, but the next flip has a 75% chance of being the same as the first.15 So if the first flip was heads, there’s a 75% chance that the second flip will be heads. If the first flip was tails, there’s a 75% chance that the second flip will be tails.\nLet’s first just calculate the probability of each outcome. The probability that the first flip is heads and the second flip is tails can be found using the Multiplication Rule, which states that P(A and B) = P(A)P(B|A). So P(HH) = P(first is H)P(second is H given that the first was H) = 0.5*0.75 = 0.375. Similarly, P(HT) = 0.125, P(TT) = 0.375, and P(TH) = 0.125.16\nLet’s compare these probabilities with the ones we calculated earlier. The probability of 0 heads with the fair coin was 1/4, and this value was calculated with the binomial distribution. With the sticky coin, the probability of 0 heads is 0.375, which does not come from the binomial distribution.\nFormally, the Binomial distribution assumes that each trial is independent and the probability of success is the same in each trial. While I didn’t touch on this, the only random thing should be the number of successes, not the number of trials. Finally, recall that, with the dice, I converted things to “3” or “not 3”; the Binomial distribution only works when each individual trial can only be one thing or another. More succinctly, the assumptions for the Binomial Distribution are:\n\nThere are n trials, and this number is known ahead of time.\nEach trial is either a “success” or a “failure”.\nEach trial is independent of the other trials.\nThe probability of success is the same for all trials.\n\n\n\nSide note: “probability of success is the same”\nAs an example, consider studying, say, the proportion of questions that a student got right on a multiple choice test. Each student has a different probability of getting each question correct. However, if we want to say something about the proportion of questions that a random student gets right on a test. In this sense, the fourth assumption is not violated.\nAs an alternative, consider a test where the students go one-by-one17 and can see the previous student’s solutions. In this case, the probability of success changes as you have more trials. This is where the problem lies - the students are still coming in a random order, but the probability of success changes.\nAs another alternative, suppose some students are cheating. They’re more likely to get the right answers together, so they’re answers are dependent on each other; knowing one cheater’s answer gives you a better guess at another cheater’s answer.\nIn summary, a different probability of success is only an issue if the researcher would be able to know this ahead of time. If the probability of success is different but we have a simple random sample with independent trials, there is no issue with this assumption.\n\n\nBinomial Mean and Variance\nNow that we know the assumptions, we can see what comes out of these assumptions. First, we can find the average value. It makes perfect sense that the average number of heads in 10 flips should be 5. There’s a 50/50 chance of heads, so you’d expect half of the flips to be heads! Formally, \\(\\mu = np\\).18 That is, the theoretical average is just the number of trials times the probability of success.\nWhat about the variance? It’s not as obvious. I’m going to try and give my own intuitive argument, but most teachers and textbooks simply skip this and have you memorize the answer. If this is your style, you can skip to the end of this section.\nIn the past, I have defined “variance” as something like “the average amount that you would be wrong if you always guessed the mean value.” Consider flipping one coin. If this coin is rigged and always comes up heads, the mean number of heads is 1 and you would always be right when you guess the mean. Intuitively, the variance here is 0. The same happens if the coin is rigged to always come up tails - the mean number of heads is 0, and the number of heads never varies so the variance is 0.\nWhat happens between 0 and 1? If the coin was heads 80% of the time, then your guess would be right 80% of the time. The actual value of the coin varies, but not too much. If the coin was heads 20% of the time, you’d still be right 80% of the time by guessing 0 heads each time. You’d be wrong most often if the coin had a 50% chance of being heads.\nSo we’ve established this: At \\(p=0\\) and \\(p=1\\), the variance is 0. The maximum value is at 0.5, and the variance should be the same if you’re 0.2 above 0.5 or 0.2 below (it’s symmetric around 0.5). The following plot, then, seems reasonable:\n\n\n\n\n\nThere’s a lot of math behind this, but the variance for Bin(1,p)19 turns out to be p(1-p). You can see that it would be symmetric around 0.5 and would be 0 whenever p=0 or p=1.\nIn general, the variance of a B(n,p) distribution is \\(\\sigma^2\\) = np(1-p)."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#conclusion",
    "href": "L09-Binomial_Probabilities.html#conclusion",
    "title": "7  Binomial Probabilities",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIf you have a known number of repeated trials that are independent and are either a “success” or “falure”, then the Binomial distribution is your friend. Once these assumptions are met, you can calculate the probability of any number of successes using the pdf, you know what the mean number of successes in \\(n\\) trials will be, and you know the variance!20\nAs a rule, if you see the phrase “Not enough information for a valid answer” as an option in a multiple choice question, double check that the assumptions are all met. If the observations are not independent, you need to know all of the conditional probabilities in order to calculate the answer, which you probably don’t have, so you’re missing information. If the probability of success changes from trial to trial, you need to know how it changes."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#self-test-questions",
    "href": "L09-Binomial_Probabilities.html#self-test-questions",
    "title": "7  Binomial Probabilities",
    "section": "7.6 Self-Test Questions",
    "text": "7.6 Self-Test Questions\n\nWhat happens when you put x = 0.5 into dbinom(x, 10, 0.5)? Interpret this in terms of flipping coins.\nI debated whether to include a section on “shape”, but decided to let you figure it out for yourself. I’ve already given you the code to plot the pdf. For each of the values of n (size) and p (prob), plot the pdf and describe the shape. Note that x should (almost) always be x &lt;- 0:n.\n\nBin(50, 0.5)\nBin(50, 0.8)\nBin(50, 0.2)\nBin(50, 0.02)\nBin(4, 0.25)\n\nFor each of the assumptions, give an example of a situation that violates only one of them, not the others."
  },
  {
    "objectID": "L09-Binomial_Probabilities.html#footnotes",
    "href": "L09-Binomial_Probabilities.html#footnotes",
    "title": "7  Binomial Probabilities",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nSince these events are disjoint, we can simply add them.↩︎\nFor coin flips, this is obvious. It won’t always be!↩︎\nAnd 0 ways to choose 4 things.↩︎\nVerify this by writing out all of the possibilities.↩︎\nDon’t be afraid to re-read this paragraph several times, there’s a lot of math here.↩︎\nThe dice is either 3 or it’s not 3, these probabilities must add to 1.↩︎\nNotice how we’re using upper case for the random variable, and lower case for the actual values. This is important for future stats classes, but just something you’ll see me do for now.↩︎\nI usually use lower case so you don’t confuse it with the PDF file extension.↩︎\nThere’s a much more correct, much more technical definition, but it’s outside the scope of this course.↩︎\nP(X \\(\\le\\) x)↩︎\nP(X \\(\\le\\) x) = 1 - P(X &gt; x)↩︎\nThe philosophy of math is extremely interesting. Most philosophers seem believe that we do discover math, rather than create it. I also believe this, but probability distributions are in a grey area for this part of philosophy. When all this is over we should grab a drink and discuss this.↩︎\nIf an engineer could make this coin for me I’d be infinitely grateful.↩︎\nAlways make sure the numbers that I give you add to 1 - I will try and trick you with this!↩︎\nin a random order↩︎\nWhy \\(\\mu\\) and not \\(\\bar x\\)? Because this is a theoretical result. You can think of this as being the “true” population.↩︎\nWhen n=1, this is also called the Bernoulli distribution, but this is not important right now.↩︎\nStatistics is the study of variance.↩︎"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "href": "L10-Sampling_Distributions.html#prelude-populations-and-samples",
    "title": "8  Sampling Distributions",
    "section": "8.1 Prelude: Populations and Samples",
    "text": "8.1 Prelude: Populations and Samples\nThe main idea in the rest of the course is this: We can use a sample to say something about the population. Before we dive into that idea, let’s make a distinction.\n\nStatistic: A number that we calculate from data.\nPopulation parameter: The value of a statistic if it were calculated for the whole population.\nSample Statistic: The value of a statistic if it were calculated for a single sample.\n\nFor example, we find the mean by taking all of the values and adding them up, then dividing by the number of things we added. For heights of Canadians, the population parameter is the value we would get if we found every Canadians’ height and added them up, then divided by the population of Canada. We obviously can’t do this, but it’s useful to think about. The sample mean is the mean we get when we just have a sample. Since we can only get a sample, it would be super cool if we could use that sample mean to talk about what values of the population mean were reasonable guesses.\nIn the height example, the population was all Canadians. This isn’t always how we define the population! For example, if we wanted to know the average length of pregnancy, we’d be looking at a population of all people who get pregnant at some point in their lives."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#introduction",
    "href": "L10-Sampling_Distributions.html#introduction",
    "title": "8  Sampling Distributions",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nYou take a sample. You find the sample mean. Is this mean exactly equal to the population mean?3 Probably not.\nWait, did I just say probably not? How probably? We’ve done a few lectures on probability, so we can probably same describe the distribution somehow. What is the probability that the sample mean is within one standard deviation of the population mean? Two standard deviations?\nBecause of random sampling error,4 every sample is going to have a different mean. We expect most of the sample means to be close to the population mean, with fewer samples resulting in sample means that are further away. In other words, the sample mean should be close to the population mean, but due to sampling error there will be a little bit of a difference.\nThe variation within our sample should be similar to the variation within the population5, and the variance in the poulation tells us the variance in the sample means. Variation is not something to be afraid of, and sampling errors are not sampling mistakes; we can harness the variability within a sample to draw conclusions about the population!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "href": "L10-Sampling_Distributions.html#sampling-distribution-of-the-sample-mean",
    "title": "8  Sampling Distributions",
    "section": "8.3 Sampling distribution of the sample mean",
    "text": "8.3 Sampling distribution of the sample mean\nBecause the value of a sample mean is random (since we took a random sample), there’s a probability distribution that describes it. I could just jump to the answer, but it’s best if I build up to it.\nThe app below6 will take a random sample from the population (in this case, normal), then find the mean and add it to a histogram. As you collect more means, the histogram gets more and more data. This simulates taking many many different samples.\n\nlibrary(ggplot2) # if this fails, run install.packages(\"ggplot2\")\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/samplingDist\")\n\nPlay around yourself! Start with \\(n\\) equal to 2 or 3. The sample shows the individual values, but it also shows the sample mean. Notice how the mean is usually closer to the population mean than any of the individual sample values.\nNow, take another sample! Again, the sample mean is closer to the population mean than most of the sampled values. Take more samples. Take 1000 more samples. Notice how the distribution of sample means is bell-shaped, but slightly skinnier than the population.\nRepeat what you did above, but use n = 25 or so. The histogram of sample means is even skinnier now! It’s still centered on the population mean, though!\nThese histograms are approximations to the sampling distribution of the sample mean. If you take an infinite number of samples and calculate the mean for each different sample, you’ll get a distribution of all possible sample means. This is what a sampling distribution is. I’m going to repeat that, since this is often a very difficult topic: the population distribution shows you the probability distribution for all possible individuals, while a sampling distribution shows you the probability distribution for all possible sample means. Each sample has a different mean, the sampling distribution describes many many samples."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#normal-populations",
    "href": "L10-Sampling_Distributions.html#normal-populations",
    "title": "8  Sampling Distributions",
    "section": "8.4 Normal Populations",
    "text": "8.4 Normal Populations\nIf the population is normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then there is some relatively straightforward math7 to show that:\n\\[\n\\bar X \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\n\\]\nThat is, the distribution of all possible sample means8 is normal with the same mean as the population, but with a smaller standard deviation. Go back to the app and see this for yourself.\n\nExample\nSuppose the population of heights of Canadian women is N(162.3, 7.11).9 We’re going to try and build up some intuition for why the distribution of all means has a smaller variance than the distribution of the population.\n\nThe probability that a randomly chosen woman is taller than 170 cm is \\(P(X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11) = 0.139. So there’s about a 14% chance of finding a woman taller than 170 cm.\n(This is just for example - this question is not often important.10) If we take a sample of n=2 women, what’s the probability that both of them are taller than 170cm? If it’s a truly random sample, then the heights of the two women should be independent and we can just multiply their probabilities.11 This means that there’s approximately 0.14% chance of this. Obviously, if one woman taller than 170 is unlikely, then both women taller than 170 is very unlikely.\nIf we take a sample of n=2 women, what’s the probability that their average height is larger than 170? From above, we know that the distribution of the sample mean is \\(N(162.3, 7.11/\\sqrt{2})\\), so we can calculate this probability as \\(P(\\bar X &gt; 170)\\) = 1 - pnorm(q = 170, mean = 162.3, sd = 7.11/sqrt(2)) = 0.06. This is somewhere in between just one of them being taller than 170cm and both of them being taller than 170.\n\nWhen we took a sample of 2 women, one might have been taller than 170 but one might have been shorter, so the average ends up being less than 170. The sample mean is less variable than the individual values, so it’s less likely to be further away.12\nSummary: If you take two values from a normal distribution, the average of those two values is probably closer to the true mean than either of the individual values. If you found the average of 100 observations from a normal distribution, the mean is probably even closer to the true mean."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "href": "L10-Sampling_Distributions.html#non-normal-populations-with-large-sample-size",
    "title": "8  Sampling Distributions",
    "section": "8.5 Non-Normal Populations with Large Sample Size",
    "text": "8.5 Non-Normal Populations with Large Sample Size\nIn the previous example, we saw that a normal population distribution will result in a distribution for all possible sample means that is also normal, but with a smaller variance. If the population isn’t normal, but you have a large enough sample size, the sampling distribution is still normal. It’s kind of amazing, but it seems to work in practice!\nThe app below13 will help you understand this relationship. I use an “Exponential distribution” for the population, but this isn’t a distribution you really need to worry about. All you need to know is that the population clearly isn’t normal.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/nLarge\")\n\nRegardless of “lambda”14, as n increase, the sampling distribution becomes closer and closer to the normal distribution. By around n=30 or 40,15 they’re basically the same!16\nAgain,\n\\[\n\\text{If }X\\sim N(\\mu, \\sigma)\\text{ and n is ``large'', then }\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\n\\]\nwhere 60 is definitely “large”, 50 is probably “large”, 30 is debatably “large” (depending on what textbook you read), and anything less than 30 is definitely small. I will not test you on the grey areas here.\nThis result has a very special name:\nThe Central Limit Theorem: Given a simple random sample of size \\(n\\) (where \\(n\\) is “large”) from any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of the sample mean will follow a \\(N(\\mu, \\sigma/\\sqrt{n})\\) distribution.\nFor a perfectly normal population, this is true for any \\(n\\). For a population that just a little bit not normal, \\(n\\) must be moderately large. For a very not normal population (e.g. Binomial with \\(p\\) far from 0.5), we need \\(n\\) even larger. Still, as long as the sd of the population is finite, the sampling distribution will be normal for sufficiently large \\(n\\)!\n\nExamples\n\nThe angle of big toe deformation in 38 patients.\n\nThere’s an outlier, but the sampling distribution would still be normal even for relatively small \\(n\\).\n\nThe number of servings of fruit per day for 74 adolescent girls.\n\nThe distribution is clearly (???) skewed17. This makes sense - the number of fruits can only be as low as 0 and there may be many people who don’t eat a lot of fruit, but there will be a few eating many fruits per day!\nThe skewness of the data implies skewness in the population (assuming this is a good sample). No worries, though, the sampling distribution will still be normal! We just might need a larger sample size in future studies.\n\nThe lengths of 56 perch from a Swedish lake.\n\nThis is clearly a bimodal distribution, indicating that there might be two subgroups in these data.\nThe sampling distribution will still be normal (unimodal), but the mean of this sampling distribution will probably be somewhere in between the two peaks. In other words, it won’t be describing either of the apparent subgroups! No amount of beautiful theorems will ever fix errors in sampling.\nIn this case, we would want to find out why there are two subgroups before trying to say anything about the population distributions. If we actually have two types of fish, it’s better to study them separately!\n\n\n\nSource: Baldi & Moore, 4th Edition.\n\n\nNon-Normal Population with Small Sample Size\nThis is governed by the \\(t\\)-distribution, which will be covered later."
  },
  {
    "objectID": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "href": "L10-Sampling_Distributions.html#very-non-normal-the-binomial-distribution",
    "title": "8  Sampling Distributions",
    "section": "8.6 Very Non-Normal: The Binomial Distribution",
    "text": "8.6 Very Non-Normal: The Binomial Distribution\nHere’s some mild deja-vu:\nYou roll a dice. You find the sample proportion of heads, denoted \\(\\hat p\\).18 Is this proportion exactly equal to the population proportion? Probably not.\nWait, did I just say probably not? How probably? What is the probability that the sample proportion is within one standard deviation of the population proportion?\n\nAside: The normal approximation to Binomial\nMost textbooks provide the rule: if both np and n(1-p) are larger than 1019, then the normal distribution is a good approximation to the binomial distribution. I prefer to let you see whether these rules make sense. The app below lets you change n and p, and shows a \\(B(n, p)\\) and an \\(N(np, \\sqrt{np(1-p)})\\)20 distribution.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Apps/normBinom\")\n\nSet n = 20 and find p such that np &lt; 10. Also find p such that n(1-p) &lt; 10. What is the shape of the Binomial distribution in these cases? What do you notice about the normal distribution? Why do both np and n(1-p) need to be greater than 10?21\n\n\nBack to Binomial\nIt turns out that, with large \\(n\\) the sampling distribution of \\(p\\) also follows a normal distribution!22 Even though the population distribution isn’t even continuous,23 the normal distribution approximates it well when there are lots of samples.\nFor each sample, the actual proportion that you calculate is variable. You might get 3 heads out of 10 flips one time, then 8 heads out of 10 flips the next. On average, though, you’ll get 5 heads out of 10 flips. Formally, the mean of the sampling distribution of the sample proportion is \\(p\\).24\nThe variance is a little trickier. In the Binomial lecture notes, I said that the variance increases as n increases. However, when we calculate the proportion, we take the number of successes divided by n. According to some math that is not important for this course, this leads to a variance of the sampling distribution of the sample proportion of p(1-p)/n, which means that the standard deviation25 of the sampling distribution is \\(\\sqrt{p(1-p)/n}\\).\nTo recap: The variance of a Binomial distribution is \\(np(1-p)\\). If we take repeated samples from that Binomial distribution and calculate the proportion of sucesses, the variance will be \\(p(1-p)/n\\).26\n\n\nExample\nSuppose I’m rolling a dice 5 times. The probability of exactly 2 ones is defined by the Binomial distribution: dbinom(2, size = 5, prob = 1/6) = 0.16.27 The variance in the number of ones in 5 rolls is np(1/p) = 5/36.\nThe average number of ones in 5 rolls is np=5/6. The standard deviation of the number of ones in 5 rolls is \\(\\sqrt{np(1-p)} = \\sqrt{5/36}\\).\n\n\nExampling Distribution\nThe following code is not testable - you are not expected to write anything like this. I’m taking repeated samples from a B(50, 0.4) distribution and calculating the proportion of successes for each sample.\n\nset.seed(4)\nn &lt;- 75\np &lt;- 0.4\n\nbinom_proportions &lt;- c() # empty vector, to be filled later\n\nfor(i in 1:1000){ # repeat this 1000 times:\n    # This is confusing: I'm getting *one* sample of size n,\n    # but R labels the number of samples as n\n    new_sample &lt;- rbinom(n = 1, size = n, prob = p)\n    \n    # Add the proportion of successes to the vector\n    binom_proportions[i] &lt;- new_sample/n\n}\n\nhist(binom_proportions, \n    breaks = 10, # what happens if you make this larger?\n    freq = FALSE) # Divide the heights of bars by the number of obs.\ncurve(dnorm(x, mean = p, sd = sqrt(p*(1-p)/n)), add = TRUE, col = 3, lwd = 3)\n\n\n\n\nCopy and paste the code above into a script file and observe what happens when you increase the number of breaks. Why does this happen?28"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "href": "L10-Sampling_Distributions.html#conclusion-statistics-is-the-study-of-variance",
    "title": "8  Sampling Distributions",
    "section": "8.7 Conclusion: Statistics is the Study of Variance",
    "text": "8.7 Conclusion: Statistics is the Study of Variance\nIn both of the sampling distributions above, the mean of the sampling distribution was the mean of the population. The difference between the population and the sampling distribution is the variance. In both sampling distributions, the variance decreases as n increases. If you sample the entire population every time you do a sample, there will be no variance in your estimate!"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#self-study-questions",
    "href": "L10-Sampling_Distributions.html#self-study-questions",
    "title": "8  Sampling Distributions",
    "section": "8.8 Self-Study Questions",
    "text": "8.8 Self-Study Questions\n\nWhen do we use \\(N(\\mu, \\sigma/\\sqrt{n})\\) versus \\(N(\\mu, \\sigma)\\)? When do we use \\(N(p, \\sqrt{p(1-p)/n})\\) versus \\(N(np, \\sqrt{np(1-p)})\\)? This distinction is extremely important.\nIf the population is \\(N(2,4)\\) and we take a sample of size 10, explain why \\(\\frac{\\bar X - 2}{4/\\sqrt{10}}\\) follows a standard normal distribution. This is extremely important.\nWhat does it mean for the sample mean to be the same as the population mean? Will they be the same every time you take a sample?\nPlay around with the “normBinom” app shown above. Why is the normal distribution not appropriate when np&lt;10 or n(1-p)&lt;10?\nIn the “Histogram of binom_proportions”, what happens when you increase the number of breaks? What causes this phenomenon?"
  },
  {
    "objectID": "L10-Sampling_Distributions.html#footnotes",
    "href": "L10-Sampling_Distributions.html#footnotes",
    "title": "8  Sampling Distributions",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nRecall: population refers to the population of interest. The population mean is the true mean of the population.↩︎\nIn statistics, error does not mean mistake.↩︎\nAssuming we have a good sample**.↩︎\nIf you don’t have access to R right now, try this one.↩︎\nYou’ll probably see it in the next stats course you take.↩︎\ni.e. the sampling distribution of the sample mean↩︎\nNote: these numbers actually come from a sample, and we don’t know that the population is normal. We’re making some massive assumptions here.↩︎\nYou will not need to do something like this on a test.↩︎\nRemember the most important fact from probability: Multiplying probabilities only works when they’re independent.↩︎\nTake a moment and make sure you understand this relationship. Write out a description of it. Call a grandparent and try to explain it to them.↩︎\nOr the same app as before, with population set to Exponential.↩︎\nWhich controls how skewed the population distribution is.↩︎\nI will either ask you questions where n &lt; 30 (non-normal sampling distr.) or n &gt; 50 (normal sampling distr.), nothing in between.↩︎\nAlthough, in this case, the normal approximation is biased, but the bias decreases as n increases and you’re not expected to know these details.↩︎\nAnswer: right↩︎\ni.e. \\(\\hat p\\) = number of success divided by number of trials.↩︎\nOr sometimes 15. Again, I won’t test you on the grey areas.↩︎\nRecall that the mean and sd of a Binomial distribution are np and np(1-p), respectively.↩︎\nAnswers: Skewed; positive probability below 0 and above n; symmetric.↩︎\nAgain, we use the rule of thumb that \\(np&gt;10\\) and \\(n(1-p)&gt;10\\).↩︎\nThis is important.↩︎\nNot n*p, since the proportion of heads is x/n.↩︎\nWhich is simply the square root of the variance.↩︎\nNotice how they’re equal when n = 1. When n=1, we’re just taking individuals from the population and calling each individual a sample.↩︎\nIn other words, 2 successes in 5 trials, where a success is defined as “rolling a one”.↩︎\nHint: What are the possible values of \\(\\hat p\\)?↩︎"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "href": "L12-Intro_to_Inference-CI-pvals.html#inference-basics",
    "title": "9  Welcome to Inference!",
    "section": "9.1 Inference Basics",
    "text": "9.1 Inference Basics\n\nProbability vs. Inference\nIn probability, we have distributions and calculate how likely given values are. In inference, we have a value that came from a distribution and try to determine things about that distribution.\nRecall: Sampling Distributions\n\nIf the population is \\(N(\\mu,\\sigma)\\), the sampling distribution of the sample mean is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nAssuming an SRS, 95% of sample means should be within 2\\(\\sigma/\\sqrt{n}\\) of the population mean.3\n\n\n\nFlippin’ it: Confidence intervals\nInstead of asking “What’s the probability that a sample mean is further than 2\\(\\sigma\\) away?”, we can ask “If your sample mean is further than 2\\(\\sigma\\), is it reasonable to say that it comes from that particular population?”\nNotice the subtle shift - we’re now talking about something that we can do with just a sample. The Sampling Distributions section always assumed that the population mean was known and told us about potential sample means. We’re now shifting our perspective: given a sample mean, what are the potential population values?\nThe basic idea in this lecture is as follows: the sample should be similar to the population but a little bit off. What are the potential values of the population mean that are compatible with what we observed?"
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "href": "L12-Intro_to_Inference-CI-pvals.html#confidence-intervals",
    "title": "9  Welcome to Inference!",
    "section": "9.2 Confidence Intervals",
    "text": "9.2 Confidence Intervals\n\nBackground\nGiven data, we want to make an inference about the population. Since \\(P(\\bar X = \\mu) = 0\\), we can’t just calculate the probability that we have the correct population mean. It’s always going to be 0!\nHowever, we can make guesses based on ranges! With confidence intervals, we create a range around our estimate that (hopefully) contains the true population mean. It won’t contain the true mean every time, but if we do things right, we can quantify our confidence that it does.\nAll CI’s that we learn in this class have the form: \\[\n\\text{Estimate} \\pm \\text{Margin of Error}\n\\]\n\n\nThe Margin of Error (MoE)\nIf the population is normal with mean \\(\\mu\\) and sd \\(\\sigma\\), then the Margin of Error is\n\\[\nMoE = (z^*)*(\\sigma/\\sqrt{n}) = \\text{Critical Value}*\\text{Standard Error}\n\\]\n\n\\(z^*\\) is a critical value. This is where we get our “confidence” from. This value is always positive.\n\\(\\sigma/\\sqrt{n}\\) is the standard deviation of the sampling distribution, which is also called the Standard Error.\n\n\n\nCritical Values\nIf \\(z^* = \\infty\\), it means that the confidence interval is infinitely wide. That is, we’re 100% confident that the true population mean is in the interval!\nIf \\(z^* = 0\\), it means the CI is just the point estimate. In other words, we’re 0% confident.\nUsually, we choose a confidence level in between 0 and 100. Values of 90%, 95%, or 97.5% are common. This values strike a nice balance between being useful and being less than infinity.\n\n\nCalculating critical values: 0.95%\nIf \\(X\\sim N(\\mu, \\sigma)\\), then the sampling distribution is \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\).\nWTo make a confidence interval, we want a range of values \\((L, U)\\) such that \\(P(L &lt; \\bar X &lt; U) = 0.95\\).\nThe normal distribution is symmetric. If we want 95% in the middle, then we need 0.025 below L and 0.025 above U. This is equivalent to values such that \\(P(\\bar X &lt; L) = 0.025\\) and \\(P(\\bar X &lt; U) = 0.975\\).\nWe can find \\(P(Z &lt; -z^*) = 0.025\\), then use the formula \\(x = z\\sigma+\\mu\\). However, since we’re using \\(\\bar X\\) instead (which has a standard deviation of \\(\\sigma/\\sqrt{n}\\) instead of \\(\\sigma\\)), this is \\(\\bar x = z^*\\sigma/\\sqrt{n} + \\mu\\).\nWe can do the same with \\(P(Z &lt; z^*) = 0.975\\) and find \\(\\bar x = z^*\\sigma/\\sqrt n + \\mu\\).\n\n\nWhat is \\(z^*\\)?\nFor \\(P(\\bar X &lt; L) = 0.025\\), \\(-z^* = -1.96\\) (almost -2).\nFor \\(P(\\bar X &lt; U) = 0.975\\), \\(z^* = 1.96\\) (almost 2).\nIn other words, it’s symmetric! The two ends of the interval are: \\[\n\\bar x = \\pm z^*\\sigma/\\sqrt{n} + \\mu\n\\]\nHowever, we don’t know the population mean. Instead, we have \\(\\bar x\\).\nA CI is defined as: \\[\n\\mu \\text{ is in the range } \\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\n\n\nSome notation: \\(\\alpha\\)\nA \\((1-\\alpha)\\%\\)CI is is defined as \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\]\nwhere \\(P(Z &lt; z^*) = \\alpha/2\\).\n\nFor a 95%CI, \\(\\alpha = 0.05\\) and \\(\\alpha/2= 0.025\\).\n\n\\(z^*\\) is found by finding the value such that \\(P(Z &lt;z^*) = 0.025\\).\nqnorm(0.025) = r round(qnorm(0.025), 4), so \\(z^* = 1.96\\).\n\nFor a 89%CI, \\(\\alpha = 0.11\\) and \\(\\alpha/2 = 0.055\\).\n\nqnorm(0.055) = r round(qnorm(0.055), 5), so \\(z^* = 1.6\\).\n\n\n\n\nInterpretation\n\nThere is no randomness in a 95% CI. The mean is fixed, the sd is fixed, the population mean is fixed.\nIt is NOT true that “95% of the time, the population mean falls in the CI”.\n\nThis is a classic gotcha.\n\nBy the way the CI is constructed, it will contain the population mean 95% of the time. We have no idea whether any particular one does, but 95% of them do.\n\nOn any given day, there’s a 10% chance of rain. However, it either rained yesterday or it didn’t. There’s not a 10% chance that it rained yesterday - it’s either 0% or 100%.\n\n\n\n\nSummary\nIf \\(X\\sim N(\\mu,\\sigma)\\), then a \\((1-alpha)\\%\\)CI is \\[\n\\bar x \\pm z^*\\sigma/\\sqrt{n}\n\\] where \\(P(Z &lt; z^*) = \\alpha/2\\) can be found with qnorm (or a z-table).\n\nA 95% is based on finding the middle 95% of the sampling distribution, but centering it around \\(\\bar x\\).\n95% of the intervals constructed this way will contain the true population mean.\n\nA given interval has either a 0% chance or a 100% chance\n\nA point of sillyness: This assumes that \\(\\sigma\\) is known."
  },
  {
    "objectID": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "href": "L12-Intro_to_Inference-CI-pvals.html#footnotes",
    "title": "9  Welcome to Inference!",
    "section": "",
    "text": "These things!↩︎\nOr silliness.↩︎\nThis is using the empirical rule - the actual value is closer to 1.96.↩︎"
  },
  {
    "objectID": "L13-p_vals.html#overview-of-tests-of-significance",
    "href": "L13-p_vals.html#overview-of-tests-of-significance",
    "title": "10  Tests of Significance",
    "section": "10.1 Overview of Tests of Significance",
    "text": "10.1 Overview of Tests of Significance\n\nPhilosophy\n\nWe start with a “null” hypothesis, \\(H_0\\), which states that nothing “interesting” is going on.\n\nThe mean is exactly what we guessed, \\(H_0: \\mu = \\mu_0\\)\nThe effect of the drug is the same in both groups.\nSomething something “same as” something something.\n\nWe have an alternative hypothesis - things are different.\n\n\\(H_A: \\mu &gt; \\mu_0\\) (or \\(&lt;\\), or \\(\\ne\\))\n\nWe do our study and get our mean (for now, assume \\(\\sigma\\) known)\nWe check if our observed mean is “too unlikely” under the null.\n\nIf the null hypothesis is true, is our observed mean preposterous?\nThis is where the dreaded p-value comes in.\n\nWe make a decision - reject or don’t reject \\(H_0\\) - based on our p-value.\n\nTo summarize: We make a “guess” about the population. We collect data, and we determine whether or not our data is compatible with our guess. If it isn’t, then it’s the guess that must be wrong; not the data1.\nThe assumptions are the same as the assumptions for CIs:\n\nNormal population (or large sample size)\n\\(\\sigma\\) known\n\nWe will get away from this assumption later; for now it’s nice to ease into the concepts.\n\nSimple Random Sample (Independent Observations)"
  },
  {
    "objectID": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "href": "L13-p_vals.html#p-value-by-example-trailmaking-test-for-fatigue",
    "title": "10  Tests of Significance",
    "section": "10.2 p-value by Example: Trailmaking Test for Fatigue",
    "text": "10.2 p-value by Example: Trailmaking Test for Fatigue\nThe following image shows the output of a “trailmaking” app. Subjects are shown the numbers on a touch screen and are tasked with drawing a line2 starting at 1, then 2, and so on without touching the other numbers. The time is recorded.\n\nIn my research, this app was given to aerial forest fire fighters. Flying a plane is a very challenging task to begin with, made much more challenging when there’s an active fire! The hypothesis is that pilots are measurably fatigued after a fire. However, this hypothesis must be converted into a mathematical construct that we can do something with!\nPilots perform the test many times before a long flight and once after. In samples from the aerial firefighters who were non-fatigued, it was found that completion time follows a normal distribution with mean 15 seconds and standard deviation 1.2 seconds3. We hypothesize that it took longer than that after the flight. \\[\\begin{align*}\nH_0: \\mu &= 15\\\\\nH_A: \\mu &&gt; 15\n\\end{align*}\\] The hypotheses above are created entirely based on the research question. We can (must) write the hypothesis before collecting data. \\(\\bar x\\) does NOT appear in hypotheses. Instead, the “15”s and the “&gt;” come from the hypotheses that fatigued pilots take longer than the population.\n\nResults\nWe caclulated a mean of 15.9 seconds from 16 pilots. Is this slower than 15 seconds? Obviously, these numbers are different, but is this a big difference? To tell whether two numbers are “far apart”, we need some sense of scale. In statistics, scale is given to us in the form of variance.\nThe population standard deviation is given as 1.2 seconds. How many standard deviations away from the hypothesized value is our sample mean? Well, since it’s a SAMPLE MEAN, the standard deviation is \\(1.2/\\sqrt{16} = 0.3\\) (again, this is also called the standard error). Our sample mean of 15.9 is 3 standard deviations4 above the hypothesized means.\nThe p-value for this is the probability of observing a value at least as far from the hypothesized mean, assuming that the hypothesized mean is the true mean5.\nOur p-value is P(Z &gt; 3) = 1 - P(Z &lt; 3) = 0.00136. Is our sample mean “unlikely” assuming that the null hypothesis is true?\nThe definition of “unlikely” will generally need to be given in the question. Usually, a significance level of \\(\\alpha = 0.05\\)7 is used8.\nSince our p-value is 0.0013 &lt; 0.05, our observed mean is “too unlikely.” So our hypothesis must be wrong!9 We conclude that the average time to complete the trail has increased, i.e. \\(\\mu &gt; 15\\)10. In this case, we say our result is statistically significant.\n\n\nSummary\nFrom the question, we got our hypotheses:\n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu &gt; 15\n\\end{align*}\\]\nWe caclulated our test statistic11:\n\\[ z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}  = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 0.9/0.3 = 3\\]\nWe looked up P(Z &gt; \\(z_{obs}\\))12 on our z-table, which gave us the p-value of 0.0013.\nSince this is a small probability (our p-value is less than our significance value of \\(\\alpha = 0.05\\)), we reject the null hypothesis in favour of the alternative.\nThis is the general approach to hypothesis testing: hypotheisize, calculate, find a normal value, then conclude."
  },
  {
    "objectID": "L13-p_vals.html#two-sided-p-values",
    "href": "L13-p_vals.html#two-sided-p-values",
    "title": "10  Tests of Significance",
    "section": "10.3 Two Sided p-values",
    "text": "10.3 Two Sided p-values\nIf your hypotheses are: \n\\[\\begin{align*}\nH_0: &\\mu = 15\\\\\nH_A: &\\mu \\ne 15\n\\end{align*}\\]\nthen you’re going to need to change things. In particular, you need to double the p-value for a one-sided test13. This is where the phrase “at least as extreme” comes in - we would reject anything this far away on either side.\nThe following shiny app demonstrates this. In particular, note what happens when you have a two sided alternative hypothesis and you double the wrong tails14.\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/pvalues\")\n\n\nTwo Sided Example\nGiven \\(\\sigma = 2\\), \\(n = 25\\), and \\(\\bar x = 6.6\\), test the hypothesis that the true population mean is not equal to 6 at the 10% level15.\n\\[\\begin{align*}\nH_0: \\mu = 6\\\\\nH_A: \\mu \\ne 6\n\\end{align*}\\]\ntest stat: \\(z_{obs} = \\frac{6.6 - 6}{2/\\sqrt{5}} = \\frac{0.6}{0.4} = 1.5\\)\nFind on z-table (or using R): P(Z &gt; 1.5) = P(Z &lt; -1.5) = pnorm(-1.5) = 0.0668\np-value = 2*0.0668 = 0.1336\nConclude: p &gt; \\(\\alpha\\), therefore do not reject. The p-value is not significant.\n\n\nCritical Values\nFor a two-sided test at the 5% level, what is the largest test statistic that would not be rejected?\nSince it’s a two-sided test at 5%, we would reject anything in the 2.5% area in either tail. Using the Z-table (or qnorm(0.05/2)), this would come from a test-statistic of 1.96. So if our test stat is 1.97, it would have a p-value below 0.05, and if it’s 1.95 it would have a p-value above 0.05.\nIn hypothesis testing, the critical value denotes the point at which z statistics16 are significant. If your z statistic is larger than 1.96, it will be statistically significant at the 5% level (for a two-sided test). This way, we can test significance without even calculating the p-value. Our conclusion will simply be that \\(p&lt;0.05\\), but this is often sufficient - it’s not important if the p-value is 0.044 versus 0.045.17\n\n\nHard Exam-Style Question\n\nA study reported that their two-sided p-value for \\(H_0:\\mu = 0\\) was significant at the 5% level, but not the 1% level.\nThey reported a mean of 10 and a sample size of 36\n\nWhat values could their standard deviation be?\nSolution:\n\nAt the 5% level, \\(z^* = 1.96\\), so:\n\n\\(1.96 = \\frac{x - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{10 - 0}{\\sigma/6}\\)\nRearranging, \\(\\sigma= \\frac{6*10}{1.96} = 30.61\\)\nSanity check: pnorm(10, 0, 30.61/sqrt(36)) = 0.975, as expected.\n\nAt the 1% level, \\(z^*\\) = -qnorm(0.01/2) = 2.576\n\n\\(\\sigma= \\frac{6*10}{2.576} = 23.292\\)\nSanity check: pnorm(10, 0, 23.292/sqrt(36)) = 0.995, as expected.\n\n\nConclusion: The standard deviation is between 23.3 and 30.6.\nIn this example, notice that a smaller standard deviation means a smaller significance level!\n\n\nCI vs. p-value\nRecall the following two facts:\n\nCI: \\(\\mu\\) is in the interval \\(\\bar x \\pm z^*\\sigma/\\sqrt{n}\\)\nTest statistic: \\(z_{obs} = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt{n}}\\)\n\nAs homework, rearrange the test statistic equation for \\(\\mu_0\\).\nA new definition of confidence intervals: A \\((1-\\alpha)\\)% CI contains every \\(\\mu_0\\) that would NOT be rejected by a test at the \\(\\alpha\\)% significance level.\nThis is why we don’t say that we “accept” the null hypothesis. There are an infinite number of hypothesis values in the CI - we can’t “accept” them all!18"
  },
  {
    "objectID": "L13-p_vals.html#self-study-questions",
    "href": "L13-p_vals.html#self-study-questions",
    "title": "10  Tests of Significance",
    "section": "10.4 Self-Study Questions",
    "text": "10.4 Self-Study Questions\n\nExplain the logic behind hypothesis testing in your own words. Make particular reference to the “at least as extreme as” part of the definition of a p-value.\nExplain why p-values are sample statistics.19\nWhat happens if a sample or study design is biased? In particular, suppose that the sample will systematically result in higher values that the population, and we’re testing \\(H_A:\\mu &gt; \\mu_0\\). What happens to the p-value?20\nFor CIs, I was adamant that we cannot speak of the probability that the population mean is inside the interval. We have now learned about the duality of CI and Hypothesis Testing, but we can speak of probability for test statistics21. What gives?22\nSuppose we are testing \\(H_A:\\mu &gt; 10\\) and we get a sample statistic \\(\\bar x = 10\\). What would the p-value for this be?\nFor a one-sided hypothesis test, what does it mean for our p-value to be larger than 0.5? Does this mean we did something wrong?23"
  },
  {
    "objectID": "L13-p_vals.html#footnotes",
    "href": "L13-p_vals.html#footnotes",
    "title": "10  Tests of Significance",
    "section": "",
    "text": "Unless it’s a bad sample/study design↩︎\n“trail”↩︎\nThese numbers actually come from the data of pre-flight trails, but we’re going to treat them as the population for now.↩︎\n15.9 is 3 steps of 0.3 above 15; (15.9 - 15)/0.3 = 3↩︎\nThis is the definition. The description must always include the part about “assuming that the hypothesized value is the true value”↩︎\nWe can only use a standard normal distrubution because the mean of the sampling distribution is assumed to be \\(\\mu_0\\), our hypothesized mean. If this weren’t the case, then we would not get a standard normal distribution and thus we wouldn’t be able to use this method. This is why the “assuming the null is true” bit is important.↩︎\nThe symbol \\(\\alpha\\) refers to the significance level, but also comes up in a \\((1-\\alpha)\\)%CI. Perhaps this is foreshadowing.↩︎\nPlease read the APA’s statement on p-values, found on OWL. At least one short answer question will be based on this.↩︎\nAgain: if our guess is incompatible with our data, then it’s our guess that’s wrong, not the data.↩︎\nNotice how this conclusion brings back the context of the question.↩︎\nLabelled \\(z_{obs}\\).↩︎\nWe used \\(&gt;\\) rather than \\(&lt;\\) because \\(&gt;\\) appears in our alternate hypothesis.↩︎\nIf you do this and find a p-value that is larger than 1, you used the wrong tails!↩︎\nIn the app, it’s denoted “Use absolute value”. This is because you can find \\(P(Z &gt; |z_{obs}|)\\) so that you always get the upper tail↩︎\nThat is, at the \\(\\alpha=0.1\\) significance level.↩︎\n\\(z_{obs}\\)↩︎\nIf we had taken a different sample, we would have gotten a different p-value - p-values have a sampling distribution as well!!!↩︎\nAlso, our tests only work in reference to the alternate hypothesis. We can only reject/not reject in reference to \\(H_A\\).↩︎\nThis implies that p-values have sampling distributions!↩︎\nWhile you’re at it, what happens to the CI?↩︎\n“p-value” is literally short for “Probability Value”.↩︎\nHint: what are we calculating probabilities for?↩︎\nHint: refer to the previous question.↩︎"
  },
  {
    "objectID": "L14-Inference_Cautions.html#interpreting-p-values",
    "href": "L14-Inference_Cautions.html#interpreting-p-values",
    "title": "11  Special Topics in Inference",
    "section": "11.1 Interpreting p-values",
    "text": "11.1 Interpreting p-values\nA p-value is the probability of a result that is at least as extreme as the one we observed, given that the null hypothesis is true.\nIt’s a measure of evidence against the null. We assume that the null is true, then ask how likely our sample would be. There isn’t a problem with our sample, so if it’s unlikely then it must be our assumption that is wrong.\n\n\n\n\n\n\n“Given that the null hypothesis is true”\n\n\n\nAny interpretation of a p-value that does not assume that the null hypothesis is true is a bad interpretation.\n\n\nThe following interpretations are not valid:\n\nThe probability of getting data like this.\nProbability of our data by chance alone.\n\nWould be correct if we made reference to the null\n\nThe probability that the null hypothesis is true\n\nThis is just so wrong, but it unfortunately appears in a lot of published papers.\n\n\nAlways look for wording that assumes that the null is true, and searches for evidence against it."
  },
  {
    "objectID": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "href": "L14-Inference_Cautions.html#statistical-versus-practical-significance",
    "title": "11  Special Topics in Inference",
    "section": "11.2 Statistical Versus Practical Significance",
    "text": "11.2 Statistical Versus Practical Significance\nSuppose a new drug claims to increase your lifespan significantly. Wow! That sounds great!\nAfter hearing this claim, you dig into the paper that made the claim, and found that the drug increases the average lifespan by 3 hours. How can they claim this was significant???\nThis is the difference between statistical and practical significance. The probability of a result at least as extreme, given that the null hypothesis is true, says nothing of how extreme the results are. A very small effect size can be statistically significant, even if it’s not a noticable change in practice.\nFurthermore, maybe this new drug costs $1,000 per day and has intense nausea as a side-effect. A statistically significant difference says absolutely nothing about the practical effects.\n\n\n\n\n\n\np-values were never meant to be the goal of a study.\n\n\n\nThey are yet another tool in the health researcher’s repertoire, meant to test whether data provide enough evidence against a very particular hypothesis."
  },
  {
    "objectID": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "href": "L14-Inference_Cautions.html#choosing-a-significance-level",
    "title": "11  Special Topics in Inference",
    "section": "11.3 Choosing a Significance Level",
    "text": "11.3 Choosing a Significance Level\nTo build intuition, we start from the extremes:\n\nA significance level of \\(\\alpha = 0\\) will never reject the null hypothesis.\n\nNo amount of evidence will convince you.\n\nA siginficance level of \\(\\alpha = 1\\) will always reject the null hypothesis.\n\nAny evidence will convince you.\n\n\nWhen setting a confidence level, you must consider how much evidence you require. To quote Carl Sagan: “Extraordinary claims require extraordinary evidence.”\nHere are some examples:\n\nA new cancer treatment costs 10 times as much and the patient will never heal back to 100% health. To justify such a procedure, we want to be reeeeeaaaaalllly sure that it works, so we might set a significance level of \\(\\alpha=0.001\\) before we collect any data.\nWe are making a slight change to an advertising strategy that is based on scientific evidence. We’re fairly certain it will work, and the consequences of it not working are very small. A larger significance level, perhaps \\(\\alpha = 0.1\\), might be appropriate.\n\n\\(\\alpha = 0.1\\) is probably the largest significance level you will ever encounter.\n\n👽 Aliens example: There’s an abberation in an image taken by a digital camera. According to the manufacturer, such an abberation would occur in 0.0001% of the pictures. If we think it’s aliens, that’s an extraordinary claim! We’d need some very strong evidence. Do you think that 0.0001% is strong enough? (We’ll return to this later in the chapter.)"
  },
  {
    "objectID": "L14-Inference_Cautions.html#hypothesis-errors",
    "href": "L14-Inference_Cautions.html#hypothesis-errors",
    "title": "11  Special Topics in Inference",
    "section": "11.4 Hypothesis Errors",
    "text": "11.4 Hypothesis Errors\nWhen you test a hypothesis, there are two types of errors: You could reject when the null is true or you could fail to reject when the null is false. The following matrix summarises this:\n\n\n\n\nH_0 is TRUE\nH_0 is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nGood!\n\n\n\nIn other words:\n\nType 1: False Positive\nType 2: False Negative\n\nThere’s another important point here: rejecting the null hypothesis does not mean that it’s actually false! Any number of things might have happened, such as including an outlier or taking a biased sample.\nSimilarly, failing to reject a null does not mean that it’s true. We’ve already talked about this a bit - confidence intervals are all values that would not be rejected by a hypothesis test, so there are many plausible null hypotheses! However, we can also fail to reject the null even though it’s false. This can also happen for multiple reasons:\n\nSample size is too small.\n\nThe distance between the null and the sample mean is calculated relative to the standard error. The standard error decreases with a larger sample, so if our sample isn’t big enough then we might not have collected enough evidence to reject the null, regardless of whether it’s true.\n\nLarge variability in the data.\n\nThis is the other thing that can increase the standard error. With more variation, the distance between the null and the sample mean doesn’t seem as large!\nWe can fix this with better sampling strategies and with better study designs, or by getting a larger sample size!\n\nOur significance level is too high.\n\nThis isn’t really something that we can change after we’ve seen the data.1\n\n\n\nThe Probability of Type 1 Errors\nWhat’s the probability your reject the null, even though it’s true? Let’s say we reject the null if the p-value is, say, less than 5%. This means that any value in the 5% tails of the distribution would lead to us rejecting the null hypothesis - even though it’s true!2 The probability that we do this is 5%, since there’s a 5% chance that we’ll see a value that is “too unlikey” at the 5% level.\nAs usual, I like to demonstrate things via simulation. Here’s the setup:\n\nSet the population parameters as \\(\\mu = 0\\) and \\(\\sigma = 1\\)\nSimulate normal data\nDo a two sided test for \\(H_0: \\mu = 0\\)\n\nNote that this null hypothesis is TRUE\n\nCount how many times we reject the null.\n\n\nset.seed(21); par(mar = c(2,2,1,1)) # unimportant\n## set an empty vector, to be filled with p-values\npvals &lt;- c() \n\nfor(i in 1:10000){ # repeat 10,000 times\n    # Simulate 30 normal values with a population mean of 0 and sd of 1\n    newsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n    # Test whether the population mean is 0\n    newsample_mean &lt;- mean(newsample)\n    newsample_sd &lt;- 1/sqrt(30) # Assuming population sd is known\n    my_z_test &lt;- 2 * (1 - pnorm(abs(newsample_mean), mean = 0, sd = newsample_sd))\n    # record the p-value (the output of t.test has some hidden values)\n    pvals[i] &lt;- my_z_test\n}\n## Testing at the 5% level\nsum(pvals &lt; 0.05) / length(pvals) # should be close to 0.05\n\n[1] 0.0481\n\n\nSince we’re testing at the 5% level, this value is close to 5%! It’s a little tricky to get your head around: If we think 5% is too unlikely, then we reject the null. However, things that have “only” a 5% chance happen about 5% of the time!\nThe histogram below shows all of the p-values we generated. The 5% cutoff isn’t anything special - a test at the 10% level will falsely reject the null 10% of the time. A test at the 90% level will falsely reject the null 90% of the time!\n\n## Fun fact: under the null hyothesis, all p-values are equally likely\n## this fun fact is not relevant to this course.\nhist(pvals, breaks = seq(0, 1, 0.05))\nabline(v = 0.05, col = \"red\", lwd = 3)\n\n\n\n\n\n\nThe Probability of Type 2 Errors\nFor a two-sided test, our hypotheses are: \\[\\begin{align*}\nH_0: \\mu &= \\mu_0\\\\\nH_A: \\mu &\\ne \\mu_0\\\\\n\\end{align*}\\]\nIf the null is actually false3, what’s \\(\\mu\\)? All we know is that it isn’t \\(\\mu_0\\).4 It could be a little above \\(\\mu_0\\), in which case it might be hard to reject \\(\\mu_0\\). It could be a far above \\(\\mu_0\\), in which case it might be easy to reject \\(\\mu_0\\).\n\nEx1: \\(\\mu = 0.001\\), \\(\\sigma = 1\\), and \\(\\mu_0 = 0\\).\n\nHard to reject \\(\\mu_0\\) since it’s so close to \\(\\mu\\) (low power)\nEasy to not reject the false \\(\\mu_0\\) (high probability of type 2 error)\nMost \\(\\bar x\\)’s would be close to \\(\\mu_0\\), relative to the standard error.\n\nEx2: \\(\\mu = 0.001\\), \\(\\sigma = 0.00000001\\), and \\(\\mu_0 = 0\\).\n\nEasy to reject \\(\\mu_0\\) since it’s so far from \\(\\mu\\) (high power)\nHard to not reject the false \\(\\mu_0\\) (low Type 2)\n\n\nPower is our ability to correctly reject a false null hypothesis, and is defined as 1 - P(Type 2 Error)\nNote that these examples are both missing the Standard Error, which incorporates sample size. The power depends on the distance between \\(\\mu\\) and \\(\\mu_0\\) relative to the standard error, not just the population standard deviation. We can partially control the standard error by having a better study design5 and a larger sample size, both of which would give us more power.\n\n\nPower by Simulation (DIY)\nThe following code calculates the power. Run it many times, changing \\(\\mu_0\\), \\(\\sigma\\), and \\(n\\) to see what happens to the power.\n\n## Set parameters\nmu &lt;- 0 # don't change this, but change the other parameters\nmu_0 &lt;- 0.1\nsigma &lt;- 0.5\nn &lt;- 50\n\n## Record p-vals\np_vals &lt;- c()\nfor(i in 1:10000){\n    newsample &lt;- rnorm(n, mu, sigma)\n    p_vals[i] &lt;- 2 * (1 - pnorm(abs(mean(newsample)), 0, sigma/sqrt(n)))\n}\n## The proportion of times the null was (correctly) rejected\nmean(p_vals &lt; 0.05) # Power\n\n[1] 0.0483\n\nmean(p_vals &gt; 0.05) # P(Type 2 Error)\n\n[1] 0.9517"
  },
  {
    "objectID": "L14-Inference_Cautions.html#multiple-comparisons",
    "href": "L14-Inference_Cautions.html#multiple-comparisons",
    "title": "11  Special Topics in Inference",
    "section": "11.5 Multiple Comparisons",
    "text": "11.5 Multiple Comparisons\nSuppose we have a coin that’s heads 5% of the time. What’s the probability of at least one heads in 10 flips?\nAs we saw in previous lectures: P(at least 1 heads in 10 flips) = 1 - P(no heads in 10 flips). We can calculate this in R:\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nWhy did I do go back to flipping coins? Did I forget which chapter I’m in?\nConsider the following problem:\nSuppose you’re testing 10 hypotheses at the 5% level. Assuming all of the null hypotheses are true, what’s the probability that at least one of them is significant?\nSince we’re testing at the 5% level, P(Type 1 Error) = 0.05, so\nP(\\(\\ge\\) 1 rejection in 10 hypotheses) =\n\n1 - dbinom(0, size = 10, prob = 0.05)\n\n[1] 0.4012631\n\n\nIn other words, there’s about a 40% chance that we’d get at least one significant result even though all of the null hypotheses are true.6\n\n\n\n\n\n\nThe Multiple Comparisons Problem\n\n\n\nWhen checking more than one hypothesis, the probability of an error increases!\nThis happens for both Type 1 and Type 2 errors, but is especially important for Type 1 errors. If you test \\(n\\) errors at the \\(\\alpha\\)% level, then the probability of a Type 1 error is \\(1 -(1 - \\alpha)^n\\).\n\n\nSo how do we avoid the multiple comparisons problem? There are generally two ways to do it:\n\nSet a Family-Wise Error Rate, rather than an error rate for individual hypothesis tests.\n\nIf you’re going to check 10 p-values, use a smaller cutoff.\nThere are several ways to do this, with the most popular being the Bonferroni correction: for m values, a cutoff of \\(\\alpha/m\\) will result in rejecting at least one test \\(\\alpha\\)% of the time. For example, if you want a test at the 5% level but you’re testing 10 values, you should reject any individual hypothesis only if the p-value is less than \\(\\alpha/m = 0.05/10\\).\n\nOnly check one p-value!\n\nFor most studies, you should have single, well-defined hypothesis. State this hypothesis ahead of time, do all of your data preparation and get it loaded into R, then only test that hypothesis.\nIf you check a second hypothesis, then your significance level is a lie! Testing two true null hypotheses at the 5% level will result in a significant result 9.75% of the time.\n\n\nFailure to account for multiple hypothesis testing is bad science and it’s a path to the dark side. Consider this fantastic tool by fivethirtyeight. Play around with it - by checking a bunch of hypotheses, you can hack your way into finding one that supports your own point of view! In this particular example, your goal is to prove that either (a) the economy does better when a democrat in the white house or (b) the economy does better when a republican is in the white house. Both of these can be demonstrated with a statistically significant result if you check enough hypotheses!\nLet’s return to the 👽 aliens example. We observed an abberation that only happens in 0.0001% of the pictures taken. However, we took thousands of pictures! Even though this event is rare, it had many chances to happen. This is exactly what multiple hypothesis testing is demonstrating: rare events will happen if you give them enough chances! Rejecting the null when it is actually true is a rare event, but it can easily happen if we check a lot of p-values!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#summary",
    "href": "L14-Inference_Cautions.html#summary",
    "title": "11  Special Topics in Inference",
    "section": "11.6 Summary",
    "text": "11.6 Summary\n\nType 1 Error: Reject a true null\n\nProbability is \\(\\alpha\\)\n\nType 2 Error: Fail to reject a false null\n\nProbability depends on the distance between \\(\\mu\\) and \\(\\mu_0\\), relative to the standard error. In more advanced classes, you will calculate this or have something to calculate it for you.\n\nMultiple comparisons problem: The more hypotheses you test, the more likely it is that at least one of them is falsely labelled significant.\n\nTo prevent this, stop checking so many p-values or adjust your expectations!"
  },
  {
    "objectID": "L14-Inference_Cautions.html#self-study-questions",
    "href": "L14-Inference_Cautions.html#self-study-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.7 Self-Study Questions",
    "text": "11.7 Self-Study Questions\n\nWe set up the null hypotheses as “nothing interesting is going on”. In light of this, explain why power is a good thing.\nIf we’re testing 5 hypotheses, what significance level should we use for each such that P(at least one type 1 error) = 0.05?\nIn simple (non-mathy) terms, explain why increasing sample size increases power.\nIf we have the hypotheses \\(H_0:\\mu = 1\\) versus \\(H_0:\\mu = 2\\), we can directly calculate the power. Run the following code to open the Shiny app, and interpret the results.\n\n\nshiny::runGitHub(repo = \"DBecker7/DB7_TeachingApps\", \n    subdir = \"Tools/SimplePower\")"
  },
  {
    "objectID": "L14-Inference_Cautions.html#participation-questions",
    "href": "L14-Inference_Cautions.html#participation-questions",
    "title": "11  Special Topics in Inference",
    "section": "11.8 Participation Questions",
    "text": "11.8 Participation Questions\n\nQ1\np-values are the probability of observing our data by chance alone.\n\nTrue\nFalse\n\n\n\nQ2\np-values are the probability of getting data at least this extreme under the null hypothesis.\n\nTrue\nFalse\n\n\n\nQ3\np-values are a measure of evidence against the null hypothesis.\n\nTrue\nFalse\n\n\n\nQ4\n\n\n\n\n\\(H_0\\) is TRUE\n\\(H_0\\) is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nHooray!\n\n\n\nIn a particular hypothesis test, rejecting the null means diagnosing a patient with cancer.\nSelect all that are true.\n\nA low significance level means that I require strong evidence before I declare cancer.\nA high power means that I’m more likely to diagnose someone with cancer, assuming they actually have cancer.\nLow type 2 error means that I am less likely to miss true cancer diagnoses.\nThe probability of type 1 error is equal to the significance level \\(\\alpha\\).\n\n\n\nQ5\n\n\n\n\n\\(H_0\\) is TRUE\n\\(H_0\\) is FALSE\n\n\n\n\nDon’t Reject\nGood!\nType 2 Error\n\n\nReject\nType 1 Error\nHooray!\n\n\n\nIn a particular hypothesis test, rejecting the null means diagnosing a patient with cancer.\nWhich of the following is true about Type 1 and Type 2 error?\n\nIncreasing \\(\\alpha\\) means I’m less likely to diagnose someone with cancer.\nHigh power means I’m more likely to diagnose someone with cancer who actually has cancer.\nThe probability that my diagnosis is correct is 1 - P(Type 2 error).\nA smaller significance level means I’ll have less power."
  },
  {
    "objectID": "L14-Inference_Cautions.html#footnotes",
    "href": "L14-Inference_Cautions.html#footnotes",
    "title": "11  Special Topics in Inference",
    "section": "",
    "text": "We can think long and hard about it before seeing the data, but once we see the data we are commited to a particular significance level. Anything else is borderline fraud, depending on the circumstances.↩︎\nRecall: the p-value is the probability of a result that is at least as extreme assuming that the null hypothesis is true!↩︎\nFalse in the population, not just rejected due to a sample.↩︎\nI once saw a bag in a grocery store with a label that said “It’s Not Bacon”. I had no idea what was in that bag. In the video I said it was kale, but that turns out to be false.↩︎\nto reduce \\(s\\)↩︎\nNote that if the hypotheses are all based on the same data then they’re probably not independent.↩︎"
  },
  {
    "objectID": "L15-CI_for_Means.html#recap",
    "href": "L15-CI_for_Means.html#recap",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.1 Recap",
    "text": "12.1 Recap\n\nSilly confidence intervals\nIf \\(X\\sim N(\\mu,\\sigma)\\), where \\(\\sigma\\) is known, then a \\((1-\\alpha)\\)CI for \\(\\mu\\) based on \\(\\bar x\\) is: \\[\n\\bar x \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\n\\] where \\(z^*\\) is found such that \\(P(Z &lt; -z^*) = \\alpha/2\\),\n\nor we could have found \\(z^*\\) such that \\(P(Z &gt; z^*) = \\alpha/2\\),\nor \\(P(Z &lt; z^*) = 1 - \\alpha/2\\),\nor \\(P(Z &gt; -z^*) = 1 - \\alpha/2\\).\n\nA natural question is: why not use \\(s\\), the sample standard deviation?\nTo demonstrate why we can’t just use \\(s\\), I have set up a simulation. I like simulations.\nYou can safely skip the simulations if you’re the type who wants to just memorize a fact and will be sure to perfectly remember it later on. The upshot is this: since we’re estimating the standard deviation, the normal distribution doesn’t apply. Instead we use the \\(t\\) distribution whenever we use \\(s\\).\n\n\nSimulation Setup\n\nTake random values from the standard normal distribution.\nCalculate the mean and sd.\nCalculate the 95% confidence interval with \\(\\sigma\\) and with \\(s\\), both using a \\(z\\) value.\nRecord whether the population mean is in the interval.\nCount how many intervals contain the population mean.\n\nShould be 95% of them!\n\n\nBefore we begin, I want to show some R code for finding confidence intervals. If you’re given that \\(\\bar x = 7.28\\), \\(n=15\\), \\(\\sigma = 1.24\\), and you want to calculate a 95% CI:1\n\nz_star &lt;- abs(qnorm(0.05/2))\nlower_bound &lt;- 7.28 - z_star*1.24/sqrt(15)\nupper_bound &lt;- 7.28 + z_star*1.24/sqrt(15)\nc(lower_bound, upper_bound)\n\n[1] 6.652485 7.907515\n\n\nAlternatively, we can use c(-1, 1) to stand in for “\\(\\pm\\)”. The code is a little weird to get your head around, but trust me - it works!\n\n7.28 + c(-1, 1)*z_star*1.24/sqrt(15)\n\n[1] 6.652485 7.907515\n\n\nSuppose that, unbeknownst to us, the true population mean was 7. To check if this is in our calculated confidence interval, we have to check that it’s larger than the lower bound AND less than the upper bound:\n\n7 &gt; 7.28 - z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n7 &lt; 7.28 + z_star*1.24/sqrt(15) \n\n[1] TRUE\n\n\nThis can be combined into code as follows:\n\n(7 &gt; 7.28 - z_star*1.24/sqrt(15)) & (7 &lt; 7.28 + z_star*1.24/sqrt(15))\n\n[1] TRUE\n\n\nThis is enough to set up the simulation. Basically, we’re going to generate a random data set from a known population, then check if the confidence interval contains the true mean. We’ll do this thousands of times, and check which proportion contain the true mean. We’re hoping it’s 95%!\n\n\nSimulation Code\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05 / 2))\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor (i in 1:100000) { # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1) * z_star * pop_sd / sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1) * z_star * samp_sd / sqrt(n)\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.94887\n\nmean(s_does)\n\n[1] 0.92991\n\n\nThe CI based on \\(s\\) only contains \\(\\mu\\) 93% of the time! This is a pretty big discrepancy. What happens when you increase the sample size, n?2\nThe reason for this discrepancy is shown in the next section:"
  },
  {
    "objectID": "L15-CI_for_Means.html#the-variance-has-variance",
    "href": "L15-CI_for_Means.html#the-variance-has-variance",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.2 The Variance has Variance",
    "text": "12.2 The Variance has Variance\nRecall that the Sampling distribution is all possible values of a statistic when sampling from a population. We’ve covered the sampling distribution for the sample mean: Every time you take a sample, you get a different mean. The distribution of these sample means is \\(N(\\mu,\\sigma/\\sqrt{n})\\).\nThe same idea applies to the sample variance! Every time you take a sample, you get a different variance. The sampling distribution is not a normal distribution. In the next section, we’ll demonstrate this fact.\n\nSimulation: sample statistics\nI’m going to generate a bunch of samples from a \\(N(0, 0.2)\\) distribution. I’ll calculate the mean and variance from each distribution, then plot the histogram.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_means &lt;- c()\nsample_vars &lt;- c()\n\nfor (i in 1:100000) {\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    sample_means[i] &lt;- mean(new_sample)\n    sample_vars[i] &lt;- var(new_sample)\n}\n\npar(mfrow = c(1, 2))\nhist(sample_means, breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Means\")\ncurve(dnorm(x, pop_mean, pop_sd / sqrt(n)), add = TRUE,\n    col = 4, lwd = 2)\n## (n-1)s^2/sigma^2 follows a chi-square distribution on\n## n-1 degrees of freedom. If you understand this, you are\n## far too qualified to be taking this course. This fact\n## is outside the scope of the course.\nhist(sample_vars * (n - 1) / (pop_sd^2), breaks = 25, freq = FALSE,\n    main = \"Sampling Dist of Sample Vars\")\ncurve(dchisq(x, n - 1), add = TRUE, col = 2, lwd = 2)\n\n\n\n\nAs you can tell from the fact that I knew how to draw the correct curve on the plots, the sampling distributions for the mean and variance are well known. Also, the sampling distribution for the variance is skewed, and therefore cannot be normal!\nWhen we use \\(\\bar x+ z^*s/\\sqrt{n}\\), \\(\\bar x\\) has variance, but so does \\(s\\).3 This is why the CI changes. When we know \\(\\sigma\\), the Margin of Error (MoE) is always the same. When the standard deviation changes for each sample, so does the MoE.\n\n\nSimulation: The Distribution of the Margin of Error\nThe sampling distribution of the Margin of Error is interesting to look at. This section is entirely optional - you just need to know that each sample has a different margin of error.\n\nn &lt;- 10\npop_mean &lt;- 0\npop_sd &lt;- 0.2\nsample_MoEs &lt;- c()\nz_star &lt;- abs(qnorm(0.5/2))\n\nfor(i in 1:100000){\n    new_sample &lt;- rnorm(n=n, mean=pop_mean, sd=pop_sd)\n    sample_MoEs[i] &lt;- z_star*sd(new_sample)/sqrt(n)\n}\n\nhist(sample_MoEs, breaks = 25,\n    main = \"Sampling Dist of MoE\")\nabline(v = z_star*pop_sd/sqrt(n), col = 6, lwd = 2)\n\n\n\n\nThe vertical purple line is \\(z^*\\sigma/\\sqrt n\\).4 This is just a re-scaling of the sampling distribution of the sample variance, so it’s also skewed! Furthermore, the average MoE using \\(s\\) is smaller than the MoE using \\(\\sigma\\), even though it’s right-skewed:\n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Average MoE (s)\" = mean(sample_MoEs))\n\n    MoE (sigma) Average MoE (s) \n     0.04265848      0.04148352 \n\nc(\"MoE (sigma)\" = z_star*pop_sd/sqrt(n),\n    \"Median MoE (s)\" = median(sample_MoEs))\n\n   MoE (sigma) Median MoE (s) \n    0.04265848     0.04104300 \n\n\nThis is why the CI using \\(s\\) doesn’t capture the true mean as often - it’s giving us smaller intervals!"
  },
  {
    "objectID": "L15-CI_for_Means.html#removing-the-silliness",
    "href": "L15-CI_for_Means.html#removing-the-silliness",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.3 Removing the Silliness",
    "text": "12.3 Removing the Silliness\nThe distribution of the sample variance is not important.5 Instead, we care about the confidence intervals.\nI’m going to write this yet again: since \\(\\bar X\\sim N(\\mu,\\sigma/\\sqrt{n})\\)), \\[\n\\frac{\\bar X - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\n\\] That is, you take the sample means, subtract the mean of the means, and divide by the standard error6, and you get a standard normal distribution.7\nOn the other hand, if we use \\(s\\) (which has it’s own variance), \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}} \\sim t_{n-1}\n\\] where \\(n-1\\) is the degrees of freedom (or df).8 This is called the \\(t\\) distribution, and is a lot like the normal distribution but it has higher variance.\nBefore we move on, notice how the formula with \\(\\sigma\\) results in N(0,1), which does not require any information for our sample. In the \\(t\\) distribution, we need to know the sample size!\n\nThe t distribution\nThere are two main features of the \\(t\\) distribution that I want you to know:\n\nIt’s centered at 0, just like N(0,1).\nIt’s more variable than the normal distribution.\n\nThe second point is demonstrated in the following plot:\n\n\n\n\n\nThe red line corresponds to a sample size of 2.9 As the colours move through red to blue, we increase the sample size. At \\(df = \\infty\\), the \\(t\\) distribution is exactly the same as the N(0,1) distribution. For anything smaller, the \\(t\\) distribution puts more probability in the tails.\nThis shows up in the critical values:\n\nabs(qnorm(0.05/2)) # z^*\n\n[1] 1.959964\n\nabs(qt(0.05/2, df = 15 - 1)) # t^* n = 15\n\n[1] 2.144787\n\nabs(qt(0.05/2, df = 30 - 1)) # n = 30\n\n[1] 2.04523\n\nabs(qt(0.05/2, df = 50 - 1)) # n = 50\n\n[1] 2.009575\n\n\nNote that, just like how qbinom finds the value such of a binomial distribution such that 0.025% of the distribution is to the left and qnorm finds the z-values such that 0.025 is to the left, qt10 finds the t-value.\n\nn_seq &lt;- seq(2, 100, by = 2)\nt_seq &lt;- abs(qt(0.05/2, df = n_seq-1))\nplot(n_seq, t_seq, type = \"b\",\n    ylab = \"abs(qt(0.05/2, df = n - 1))\",\n    xlab = \"n\",\n    # the code for the title is not important.\n    main = bquote(\"As df -&gt; infinity, t\"^\"*\"*\" -&gt; z\"^\"*\"))\nabline(h = abs(qnorm(0.05/2)), col = 3, lwd = 2)\n## this code just puts a label on the axis - not important\naxis(2, abs(qnorm(0.05/2)), \"z*\", col = 3, font = 2, col.axis = 3)\n\n\n\n\nSince there’s more probability in the tails, you have to go further out to find the point such that 0.025 of the distribution is to the left.11 The \\(t\\) distribution allows for more variance due to the variance of \\(s\\), and it does this by having larger critical values.\n\n\nThe \\(t\\)-distribution\nThe \\(t\\) distribution has higher variance than the Normal distribution due to the extra uncertainty in estimating \\(s\\)."
  },
  {
    "objectID": "L15-CI_for_Means.html#the-t-confidence-interval",
    "href": "L15-CI_for_Means.html#the-t-confidence-interval",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.4 The \\(t\\) Confidence Interval",
    "text": "12.4 The \\(t\\) Confidence Interval\nNow that you understand the reasoning behind using wider confidence intervals, I can show you the formula/ \\[\n\\bar x \\pm t_{n-1}^*s/\\sqrt{n}\n\\]\nwhere \\(t^*_{n-1}\\) comes from abs(qt(alpha/2, df = n-1)).12\nThis has the same interpretation as the Z CI: 95% of the intervals constructed this way will contain the true population mean. This does NOT mean that there’s a 95% chance that the interval contains the true mean.\nWhat’s that? Of course, I can demonstrate by simulation! Thanks for asking! The following code is copied and pasted from above, only the critical value has been changed.\n\n## Set up empty vectors, to be filled with TRUE or FALSE\n## if the population mean is in the interval\nsigma_does &lt;- c() # CI based on sigma does contain mu\ns_does &lt;- c() # CI based on s does contain mu\n\npop_sd &lt;- 1\npop_mean &lt;- 0\nn &lt;- 15 # sample size\n\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, n - 1)) # NEW\n\n## You aren't expected to understand \"for\" loops, but\n## you need to be able to find CIs\nfor(i in 1:100000){ # repeat this code a bunch of times\n    new_sample &lt;- rnorm(n = n, mean = pop_mean, sd = pop_sd)\n    xbar &lt;- mean(new_sample)\n    samp_sd &lt;- sd(new_sample)\n\n    CI_sigma &lt;- xbar + c(-1, 1)*z_star*pop_sd/sqrt(n)\n    CI_s &lt;- xbar + c(-1, 1)*t_star*samp_sd/sqrt(n) # NEW\n    # Do they contain the population mean?\n    # in other words, is the lower bound less than pop_mean\n    # *and* is the upper bound larger than pop_mean?\n    # (Not testable)\n    sigma_does[i] &lt;- (CI_sigma[1] &lt; pop_mean) & (CI_sigma[2] &gt; pop_mean)\n    s_does[i] &lt;- (CI_s[1] &lt; pop_mean) & (CI_s[2] &gt; pop_mean)\n}\n\n## The mean of a bunch of TRUEs and FALSEs is\n## the proportion of TRUEs (TRUE == 1, FALSE == 0)\nmean(sigma_does)\n\n[1] 0.95069\n\nmean(s_does)\n\n[1] 0.95091\n\n\nNow both of them contain the mean 95% of the time!13 The difference between them is that the t CI doesn’t have as much information as the Z CI - the Z CI knows what the population sd is, but the t CI doesn’t. This is kinda magical: using math, we can get the truth with fewer assumptions!"
  },
  {
    "objectID": "L15-CI_for_Means.html#examples",
    "href": "L15-CI_for_Means.html#examples",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.5 Examples",
    "text": "12.5 Examples\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(\\sigma = 0.01\\), find the 92%CI.\n\nThis is a bit of a trick: I gave you \\(\\sigma\\)! This always refers to the population standard deviation, so that’s what it is here. The Z CI can be found with the R code:\n\n\n\n0.4 + c(-1, 1)*abs(qnorm(0.08/2)) * 0.01/sqrt(100)\n\n[1] 0.3982493 0.4017507\n\n\n\n\\(\\bar x = 0.4\\), \\(n = 100\\), \\(s = 0.01\\), will a 92%CI be wider than or smaller than the CI from Example 1?\n\nWe use \\(t\\) to account for the extra variance we have when we estimate \\(s\\). More variance means wider tails! The CI will be wider!\n\n\n\n0.4 + c(-1, 1)*abs(qt(0.08/2, df = 100-1)) * 0.01/sqrt(100)\n\n[1] 0.3982312 0.4017688\n\n\nIt’s only slightly wider. The sample size is large enough that the variance in the estimate is small.14 Try this again with a smaller \\(n\\) and see what happens to the difference!\n\nIf \\(n=16\\) and the 95%CI for \\(\\mu\\) is (10, 15), what’s the variance?\n\nA general form of the CI is \\(\\bar x \\pm t^* s/\\sqrt{n}\\).\n\n\\(\\bar x\\) is in the centre, so \\(\\bar x\\) is 12.5\n\nThe MoE is 2.5, so \\(t^* s/\\sqrt{n} = 2.5\\).\n\n\\(t^*\\) is qt(0.05/2, 16 - 1) = 2.131\n\n\\(2.131s/\\sqrt{16} = 2.5\\), so \\(s = 2.5\\sqrt{16}/2.131 = 4.69\\)\nThe variance is \\(4.69^2 = 21.9961\\)"
  },
  {
    "objectID": "L15-CI_for_Means.html#summary",
    "href": "L15-CI_for_Means.html#summary",
    "title": "12  Confidence Intervals in Practice",
    "section": "12.6 Summary",
    "text": "12.6 Summary\nThis lesson could have been two sentences: The sample standard deviation has variance, so each confidence interval based on \\(s\\) is slightly different. To account for this, we use the \\(t\\) distribution. Then again, when someone tells me their name at a party I immediately forget it. Hopefully this long-winded exploration helps you understand why these facts are true and how they’re relevant to the course.\nNote that all of the best practices for inference still apply! We can still get smaller intervals by taking better samples with larger sample sizes, and we still have to be careful to never speak of a calculated confidence interval in terms of chance.\nThe \\(t\\) confidence interval is actually used in practice. We’ll see some code that calculates the interval for us in the next lecture, and then we’ll never have to use qt() again! (Except possibly to demonstrate knowledge on tests.)"
  },
  {
    "objectID": "L15-CI_for_Means.html#footnotes",
    "href": "L15-CI_for_Means.html#footnotes",
    "title": "12  Confidence Intervals in Practice",
    "section": "",
    "text": "You’ll need to do this sort of thing on a test/assignment.↩︎\nRe-run the code and try it!↩︎\nBoth are random variables.↩︎\nRecall that this never changes since \\(\\sigma\\) is fixed.↩︎\nAnd very complicated.↩︎\nthe standard deviation of the sampling distribution↩︎\nThe word “standard” shows up way too much. Statisticians are bad at naming things.↩︎\nThis is another example of statisticians being bad at naming things.↩︎\nThe degrees of freedom is \\(n-1\\).↩︎\nThe person reading this is a cutie.↩︎\nTry this for other \\(\\alpha\\) values and larger \\(n\\).↩︎\nNote: I’m not even going to bother writing out the \\(P()\\) notation for \\(t^*_{n-1}\\) because you’ll never use it. You’ll only ever need to find \\(t^*_{n-1}\\) in this course.↩︎\nThis means it’s working!↩︎\nRecall: For both the sample mean and the sample proportion, the variance of the sampling distribution decreases as \\(n\\) increases.↩︎"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html",
    "href": "L16-Hypothesis_Tests_for_Means.html",
    "title": "13  t-Tests for a Mean",
    "section": "",
    "text": "14 Recap"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "href": "L16-Hypothesis_Tests_for_Means.html#when-we-use-s-we-use-t",
    "title": "13  t-Tests for a Mean",
    "section": "13.1 When we use \\(s\\), we use \\(t\\)",
    "text": "13.1 When we use \\(s\\), we use \\(t\\)\nWe’ve been over this in confidence intervals, and the same thing applies to hypothesis tests! If the population is normal (or the sample size is large enough) and we have an SRS, then \\[\n\\frac{\\bar X - \\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\]\nAgain, the \\(t\\) distribution is used to account for the extra variability from the estimated standard deviation.1\nThis means our test statistic is \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\]\nSince this is a \\(t\\) distibution, we use pt(t_obs, df = n -1), possibly one minus and/or double, depending on the alternate hypothesis.2\nThat’s it. That’s the big difference. When we estimate the standard deviation, we use the t-distribution.\n\n\n\n\n\n\nThe t-test for a population mean\n\n\n\nGiven a sample mean \\(\\bar x\\) and a sample standard deviation \\(s\\), our test statistic is: \\[\nt_{obs} = \\frac{\\bar x - \\mu}{s/\\sqrt{n}}\n\\] Our hypotheses and calculations of the p-value work the same as they did for the z-test."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#examples",
    "href": "L16-Hypothesis_Tests_for_Means.html#examples",
    "title": "13  t-Tests for a Mean",
    "section": "13.2 Examples",
    "text": "13.2 Examples\n\nPilot Fatigue\nIn the pilot fatigue example from the Understanding p-values lecture, we assumed that we had the population sd. I lied - it was actually a sample statistic! We should have used a t-test, not a z test.\nRecall:\n\n\\(H_0: \\mu = 15\\) versus \\(H_A: \\mu &gt; 15\\) with\n\\(n = 16\\), \\(\\bar x = 15.9\\), \\(s = 1.2\\) (not \\(\\sigma\\)) \\[\nt_{obs} = \\frac{15.9 - 15}{1.2/\\sqrt{16}} = 3\n\\]\n\nUsing the \\(t\\) distribution, our p-value is:\n\n1 - pt(3, df = 16)\n\n[1] 0.00423975\n\n\nThis is larger than our previous p-value of 0.0013. This will always be the case: if the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\n\n\n\n\n\n\np-values from a t-test are larger than a z-test (if you have \\(\\sigma=s\\))\n\n\n\nWe almost never know the population standard deviation, so we have extra uncertainty. With extra uncertainty, we require more evidence! Recall that a p-value is a measure of evidence against a null."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "href": "L16-Hypothesis_Tests_for_Means.html#matched-pairs",
    "title": "13  t-Tests for a Mean",
    "section": "13.3 Matched Pairs",
    "text": "13.3 Matched Pairs\nA matched pairs design allows us to use a one-sample t-test when it looks like we have two samples3. Since the pairs are matched, we can calculate the differences between pairs and treat this like a single vector of observations. It is honkey tonk ridonkulous to say that we know the true population standard deviation for the difference in observations, so a \\(z\\) test could never be appropriate.\nConsider the following example of a matched pairs experiment. Given a sample of brave volounteers, we create a small cut on both hands and put ointment on one of the two cuts4. This study design eliminates the variation in healing times for different people since both cuts are on the same person! For each individual, we observe a difference. That is, one observation per person!\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Ointment\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\nNote: Differences were calculated as “With minus Without”! This will be important for setting up the alternative hypothesis later.\nThe important thing here is that last row of this table now represents our data - we can forget that the other two rows exist! In other words, we have one observation per person, rather than two sets of observations.\nThis is where the assumption that we know the population standard deviation is especially preposterous: we’re looking just at the differences! Even if there’s a true value of the sd for healing time for all people, the standard deviation of the difference between healing times isn’t a reasonable quantity to speak of.\nSince we’re looking at the difference, we no longer have a hypothesized value of \\(\\mu_0\\). Instead, we hypothesize that the average pairwise difference is 0, i.e. \\(\\mu_{with\\; minus\\; without} = \\mu_{diff} = 0\\)5. The alternative is “with” &lt; “without”, i.e. \\(\\mu_{diff} &lt; 0\\).6\n\nx &lt;- c(-0.78, 0.01, -0.33, -0.7, -0.2, 0.61, -0.87, -2.1)\nxbar &lt;- mean(x)\ns &lt;- sd(x)\nn &lt;- length(x)\n\nt_obs &lt;- (xbar - 0)/(s/sqrt(n)) # xbar is with - w/out\n# Notice that we use pt() instead of pnorm()\npt(t_obs, df = n - 1) # Alternative is &lt;\n\n[1] 0.04662624\n\n\nSo our p-value is approximately 0.04. At the 5% level, the null hypothesis would be rejected and we would conclude that the ointment works7. At the 1% level, we would conclude that it doesn’t have a significant effect. This is why it’s important to know the significance level before calculating the p-value - we shouldn’t get to choose whether our results are statistically significant!\n\nt-tests in Practice\nDo you think that researchers in the field are typing test statistics into their calculator? Of course not! We’re finally at the point in this class where the methods are so commonly used that the built-in functions in R can calculate them.\n\nwith_oint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.0, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4  , 6.7, 2.88, 7.8, 6.32)\ndifference &lt;- with_oint - without\nt.test(difference, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.04817\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n         -Inf -0.007063183\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that the output shows a one-sided confidence interval. This isn’t a big leap from what you know: a confidence interval consists of all of the values that would not be rejected by a hypothesis test, and this works for one-sided as well as two-sided alternate hypotheses!\nTo get a two-sided confidence interval, we can either leave alternative at it’s default value or set it to \"two.sided\". We can also change the significance level with the conf.level argument. For an 89%CI:\n\nt.test(difference, alternative = \"two.sided\", conf.level = 0.89)\n\n\n    One Sample t-test\n\ndata:  difference\nt = -1.9199, df = 7, p-value = 0.09635\nalternative hypothesis: true mean is not equal to 0\n89 percent confidence interval:\n -1.04730428 -0.02519572\nsample estimates:\nmean of x \n -0.53625 \n\n\nNotice that this calculated a two-sided p-value, which is twice what we saw before (and no longer significant at the 5% level!)."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#hypothesis-tests-in-general",
    "href": "L16-Hypothesis_Tests_for_Means.html#hypothesis-tests-in-general",
    "title": "13  t-Tests for a Mean",
    "section": "14.1 Hypothesis Tests in General",
    "text": "14.1 Hypothesis Tests in General\n\nDecide on a hypothesis.\n\n\\(H_0: \\mu = \\mu_0\\) versus \\(H_a: \\mu [\\ne,&gt;,\\text{ or }&lt;] \\mu_0\\)\n\nChoose a significance level \\(\\alpha\\).\n\nSmaller leverl = require more evidence to reject the null.\n\nGather data\n\nIndependent observations from same population; random sample.\n\nCalculate the test statistic based on \\(\\bar x\\), \\(s\\), and \\(\\mu_0\\).\n\nSampling distribution is based on the null hypothesis.\n\nCalculate the p-value according to the form of the alternate hypothesis.\n\nIf \\(&lt;\\), then pnorm(z_obs); if \\(&gt;\\), then 1 - pnorm(z_obs); if two sided, double the correct one."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#hypothesis-test-example",
    "href": "L16-Hypothesis_Tests_for_Means.html#hypothesis-test-example",
    "title": "13  t-Tests for a Mean",
    "section": "14.2 Hypothesis Test Example",
    "text": "14.2 Hypothesis Test Example\nNew York is sometimes called “the city that never sleeps”. At the 5% level, do the following data provide evidence that the average New Yorker gets less than 8 hours of sleep per night?\n\n\n\n\\(\\bar x\\)\n\\(s\\)\n\\(n\\)\n\n\n\n\n7.73\n0.77\n25\n\n\n\n\nHypotheses: \\(H_0: \\mu = 8\\), \\(H_a:\\mu &lt; 8\\).\n\\(t_{obs}\\) = \\(\\dfrac{7.73 - 8}{0.77/5} = -1.75\\)\np-value = pt(-1.75, 24) = 0.0464\nConclude: Since p &lt; \\(\\alpha\\), we reject the null hypothesis.\n\nWe have found statistically significant evidence that New Yorkers sleep less than 8 hours per night on average."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#confidence-intervals",
    "href": "L16-Hypothesis_Tests_for_Means.html#confidence-intervals",
    "title": "13  t-Tests for a Mean",
    "section": "14.3 Confidence Intervals",
    "text": "14.3 Confidence Intervals\n\nChoose a confidence level \\(\\alpha\\).\n\n“100(1-\\(\\alpha\\))%CI\n\nCollect data\n\nIndependent observations from same population; random sample.\n\nFind the critical value \\(t^*_{n-1}\\)\n\nWe will not need \\(z^*\\) again, except possibly as comparison.\n\nCalculate \\(\\bar x \\pm t^*s/\\sqrt{n}\\)\nConclude: 95% of the intervals constructed this way will contain the true population mean."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#confidence-interval-example",
    "href": "L16-Hypothesis_Tests_for_Means.html#confidence-interval-example",
    "title": "13  t-Tests for a Mean",
    "section": "14.4 Confidence Interval Example",
    "text": "14.4 Confidence Interval Example\nConstruct a 95% CI for the New York sleep example.\n\n\n\n\\(\\bar x\\)\n\\(s\\)\n\\(n\\)\n\n\n\n\n7.73\n0.77\n25\n\n\n\n\n\\(\\alpha\\) = \n\\(t^*_{n-1}\\) = qt(0.025, 24) = -2.0639.\n\\(\\bar x \\pm t^*_{n-1}s/\\sqrt{n} = 7.73 \\pm 2.0639*0.77/5 = (7.41, 8.05)\\)\nConclude: we are 95% confident that the true average nights sleep in New York is between 7.41 and 8.05.\n\nThis interval includes 8, so 8 would not be rejected by a hypothesis test?!?!"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#participation-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#participation-questions",
    "title": "13  t-Tests for a Mean",
    "section": "14.5 Participation Questions",
    "text": "14.5 Participation Questions\n\nQ1\nWhat is the standard error?\n\n\\(\\sigma/\\sqrt{n}\\)\n\\(\\sqrt{\\frac{p(1-p)}{n}}\\)\n\\(\\sqrt{s_1^2/n_1 + s_2^2/n}\\)\nThe standard deviation of the sampling distribution.\n\n\n\nQ2\nWhat is the standard deviation of the sampling distribution?\n\nThe standard deviation of the population divided by the square root of the sample size (\\(\\sigma/\\sqrt{n}\\)).\nThe standard deviation of the value of a sample statistic across all possible samples from the population.\nThe same as the standard deviation of the population.\nThe average distance to the mean of the population.\n\n\n\nQ3\nWhy does the sampling distribution have a lower variance than the population?\n\nBecause the standard deviation is smaller than the variance.\nBecause the population has a larger number of possible, so the variance is smaller.\nBecause outliers are not as likely in a sample.\nBecause we are summarising many observations from a sample into a single value.\n\n\n\nQ4\nAfter conducting a study, we found a p-value of 0.04. Did we find a statistically significant result?\n\nYes, since the p-value is less than 0.05\nNo, since the p-value is less than 0.05\nWe failed to set the significance lavel ahead of time, so we have to be very careful about concluding significance.\n\n\n\nQ5\nAfter conducting a study, we found a 95% confidence interval for \\(\\mu\\) from -0.1 to 1.9. What can we conclude?\n\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu \\ne 0\\) would not be significant at the 5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu &gt; 0\\) would not be significant at the 5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu \\ne 0\\) would not be significant at the 2.5% level.\nSince 0 is in the interval, a hypothesis test for \\(\\mu = 0\\) versus \\(\\mu &gt; 0\\) would not be significant at the 2.5% level.\n\n\n\nQ6\nUnder which condition does the CLT not apply?\n\nFor \\(\\bar x\\), the sample size is between 3 and 60 but a histogram of the sample appears normal.\nFor \\(\\bar x\\), the sample size is much larger than 60.\nFor \\(\\hat p\\), the sample size is much larger than 60.\nFor \\(\\hat p\\), we have checked \\(np&gt;10\\) and \\(n(1-p)&gt;10\\)"
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "href": "L16-Hypothesis_Tests_for_Means.html#self-study-questions",
    "title": "13  t-Tests for a Mean",
    "section": "14.6 Self-Study Questions",
    "text": "14.6 Self-Study Questions\n\nExplain why: If the \\(z_{obs}\\) test statistic is the same as the \\(t_{obs}\\) test statistic, then the p-value for \\(t_{obs}\\) will be wider.\nIf a test is statistically signficant, does that mean there’s a large effect size? That is, does a hypothesis test tell you anything about the size of the effect?\n\nCompare this to confidence intervals.\n\nCan we interpret a \\(t\\) confidence interval as “all null hypothesis values that would not be rejected”?\nRe-do the ointment example, but using without - with.\n\nDraw a t distribution and mark the two test statistics, then fill in the area that corresponds to the p-value."
  },
  {
    "objectID": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "href": "L16-Hypothesis_Tests_for_Means.html#footnotes",
    "title": "13  t-Tests for a Mean",
    "section": "",
    "text": "Which is used in the caclulation of the Estimated Standard Error.↩︎\nLike pnorm(), it always calculates the probability below the test statistic.↩︎\nWe’ll learn about two-sample t-tests in the next lecture.↩︎\nAnd most likely a bandage on both.↩︎\nIn other words, the healing times are the same for each subject↩︎\nThis is where it’s important to know that we did “with minus without”; we could have done without minus with, but then our alternate hypotheses would need to be “&gt;”.↩︎\nA p-value says nothing about the effect size, so we can’t say whether it’s practically significant↩︎"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "href": "L17-Two_Sample_hypothesis_Tests.html#how-much-can-one-more-sample-complicate-things",
    "title": "14  Two-Sample t-Tests",
    "section": "14.1 How much can one more sample complicate things?",
    "text": "14.1 How much can one more sample complicate things?\n\nNotation: Subscripts everywhere!\nWe now have two samples.\n\\(\\bar X_1\\sim N(\\mu_1, \\sigma_1/\\sqrt{n_1})\\), where \\(s_1\\) is the estimated standard deviation of a given sample.\n\\(\\bar X_2\\sim N(\\mu_2, \\sigma_2/\\sqrt{n_2})\\) \nGoal: Are the means the same? I.e., is \\(\\mu_1 = \\mu_2\\)?\nWith two samples, the difference in means has a sampling distribution. What is that distribution? It’s difficult!\nThe easy part is the mean of the difference. The mean of the difference is the difference in means. \\[\n\\bar X_1 - \\bar X_2 \\sim N(\\mu_1 - \\mu_2, ???)\n\\]\nThe hard part is the standard deviation of the difference. Take a moment and think about what we’re talking about here. What does it actually mean for the difference in means to have variance?\nIt’s the same as it was before, it just seems a little more complicated. When we take a pair of samples then find their difference, we have calculated a statistic! For every pair of samples, we’ll get a different statistic. The variance that we seek is the variance of all of these statistics.\n\n\nThe standard deviation of a difference\nAgain, I’m going to use a simulation to demonstrate what happens if we take a bunch of pairs of samples, then find their difference.\nIn this case, I’m sampling x1 as 22 values from a normal distribution with a mean of 0 and a standard deviation of 2, whereas x2 has 33 observations and comes from a distribution with a mean of 0 and a standard deviation of 3. I’m finding their means, then finding their differences. I repeat this 10,000 times, keeping track of what the difference in means was.\n\n## approximating the sampling distribution\ndifferences &lt;- c()\nfor(i in 1:10000){\n    m1 &lt;- mean(rnorm(22, 0, 2))\n    m2 &lt;- mean(rnorm(33, 0, 3))\n    differences[i] &lt;- m1 - m2\n}\nsd(differences)\n\n[1] 0.6802144\n\n\n… it’s not at all obvious where this number comes from.\nBoth had a mean of 0, so the difference should be 0. But the standard deviation of the differences isn’t obvious. There is a nice formula for the mean - \\(\\sigma/\\sqrt{n}\\) - but it’s not obvious how this works for two samples with different variances and different sample sizes!\nIt turns out that the following equation is the correct one for the standard deviation of the differences. Recall that the standard deviation of a sampling distribution is known as the Standard Error (SE).\n\\[\nSE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nAnd to verify that our simulation matches this idea:\n\n## Check\nsd(differences)\n\n[1] 0.6802144\n\nsqrt(4/22 + 9/33) # Close enough\n\n[1] 0.6741999\n\n\n\n\nPutting it Together\nAltogether, this means that the difference between means has the following sampling distribution:\n\\[\n\\bar X_1 - \\bar X_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\right)\n\\]\nFrom this, we get the same general ideas as before. The hypothesis tests are based on the observed test statistic: \\[\nt_{obs} = \\frac{\\text{sample statistic} - \\text{hypothesized value}}{\\text{standard error}} = \\frac{(\\bar x_1 - \\bar x_2) - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] where usually the hypothesized value is 0 so that we’re testing whether the true population means are the same.\n\n\nTwo-Sample Hypotheses\nThe usual hypotheses invlove the equality of the means, i.e.: \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_0: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\nThe confidence interval is also the same idea: \\[\n\\text{sample statistic}\\pm\\text{critical value}*\\text{standard error} = (\\bar x_1 - \\bar x_2)\\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nIn both of these, we still need to know the degrees of freedom! As we saw last lecture, the \\(t\\)-distribution requires some information about the sample size. In the one sample case, this was \\(n-1\\). However, we now have two potentially different sample sizes! What do we do?\nRecall that the \\(t\\)-distribution gets closer and closer to the normal distribution as \\(n\\) increases. The whole point of the \\(t\\) is to get us a little further from the normal in order to account for the variance in the sample standard deviation. For the two-sample case, there is a “correct” formula, but it’s big and scary and everything we’re doing now is approximate anyway. Instead, we use a more conservative approach to ensure that we’re not underestimating the variance.1\n\nTwo-Sample \\(t\\) degrees of freedom\nWe use the smallest sample size, then subtract 1.\n\nIn the simulation we did earlier, the two samples had sizes of 22 and 33. In both a CI and a hypothesis test, we would use 21 as the value in qt() or pt() (which are the same idea as qnorm() and pnorm()).\n\nAside: “Pooled Variance”\nThere’s another formula out there that uses a so-called “pooled variance” for the standard error of the differences. This assumes that both populations have the exact same variance, and tries to use information from both to estimate the variance. It essentially treats the two samples as one big sample from the same population in order to calculate the standard deviation.\nThis also implicitly assumes that both populations are normal, and this is not based on the CLT. Instead, the populations need to be normal. This is a huge assumption - we can use normality from the CLT because the math checks out. Assuming normality of the population is just a wild guess that we can’t really check.\nIt is also very, very unlikely that the two populations have the same standard deviation.\nIf the two assumptions are met, then the pooled standard deviation is the “correct” formula. However, the SE that we saw before still works very well! If the assumptions are not met, then the pooled SE works poorly and the SE we’ve seen is still very good!\nExcept in exceptional cases, the SE that we’ve learned should be used. The idea of a pooled variance is a vestige of another age."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#summary",
    "href": "L17-Two_Sample_hypothesis_Tests.html#summary",
    "title": "14  Two-Sample t-Tests",
    "section": "14.2 Summary",
    "text": "14.2 Summary\n\nTwo-Sample t-test and CI Overview\nWe are usually testing for the difference in means, i.e. \\[\\begin{align*}\nH_0: \\mu_1 = \\mu_2 &\\Leftrightarrow H_0:\\mu_1 - \\mu_2 = \\mu_d = 0\\\\\nH_a: \\mu_1 &lt; \\mu_2 &\\Leftrightarrow H_a:\\mu_1 - \\mu_2 = \\mu_d &lt; 0\\\\\n\\end{align*}\\]\n\\[\nt_{obs} = \\frac{\\bar x_1 - \\bar x_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\] \n\\[\n\\text{p-value} = P(T &lt; t_{obs}) = \\texttt{pt(t\\_obs, df = min(n1, n2) - 1)}\n\\]\n\\[\n\\text{A $(1-\\alpha)$ CI for $\\mu_d$ is }\\bar x_1 - \\bar x_2 \\pm t^*\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\nwhere \\(t^*\\) is based on the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\)"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#examples",
    "href": "L17-Two_Sample_hypothesis_Tests.html#examples",
    "title": "14  Two-Sample t-Tests",
    "section": "14.3 Examples",
    "text": "14.3 Examples\n\nExample 1: Two-Sample versus Matched\nFrom the Ointment example:\n\n\n\n\nSubject 1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nWith Oint\n6.44\n6.06\n4.22\n3.3\n6.5\n3.49\n7.01\n4.22\n\n\nWithout\n7.22\n6.05\n4.55\n4\n6.7\n2.88\n7.88\n6.32\n\n\nDifference\n-0.78\n0.01\n-0.33\n-0.7\n-0.2\n0.61\n-0.87\n-2.1\n\n\n\nFirst we’ll do matched pairs. In this example, this is the correct test to use.\n\nwithoint &lt;- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.01, 4.22)\nwithout &lt;- c(7.22, 6.05, 4.55, 4, 6.7, 2.88, 7.88, 6.32)\ndiff &lt;- withoint - without\n\nt.test(x = diff, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -1.9421, df = 7, p-value = 0.04663\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n        -Inf -0.01332314\nsample estimates:\nmean of x \n   -0.545 \n\n\nWe could instead have done a two-sample test, which ignores the fact that the observations are paired. Since we know the pairings, this test is leaving out valuable information.\n\nt.test(x = withoint, y = without, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  withoint and without\nt = -0.67584, df = 13.736, p-value = 0.2552\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n      -Inf 0.8772719\nsample estimates:\nmean of x mean of y \n    5.155     5.700 \n\n\nThis result should not be trusted since it misses a key aspect of the data.\n\n\nExample 2: Body Mass of Penguins\nIn this example, we’ll look at the difference in body mass between male and female penguins.\nIn this case, there is no clear pairing between the penguins. If they were monogomous couples, then the differences in body mass might tell us about couples, but doesn’t say much about male and female penguins in general.\nThis example gives us the opportunity to learn more notation in R! The previous example used t.test(x=..., y=...) to denote the two samples. If the data are neatly formatted in a data frame, then we can use the ~ notation to demonstrate that the body mass is split into different groups for male and female:\n\nlibrary(palmerpenguins)\nt.test(body_mass_g ~ sex, data = penguins)\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -8.5545, df = 323.9, p-value = 4.794e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -840.5783 -526.2453\nsample estimates:\nmean in group female   mean in group male \n            3862.273             4545.685 \n\n\nFrom this output, we get a two-sided p-value as well as a two-sided CI, both confirming that the difference in body mass is different from 0. You can also see that it’s using “female minus male”, rather than “male minus female”. This is because R will put them in alphabetical order, so female comes first.\nYou may also be happy to hear that no, you will never have to manually enter the standard error formula! You may need to interpret it for tests, though!"
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#conclusion",
    "href": "L17-Two_Sample_hypothesis_Tests.html#conclusion",
    "title": "14  Two-Sample t-Tests",
    "section": "14.4 Conclusion",
    "text": "14.4 Conclusion\n\nIf you can have matched pairs, you should use a matched pairs test.\nMost of the time, you’ll need to use a two-sample t-test.\n\nDon’t get fooled by equal sample sizes!\n\nA two-sample t-test is based on the difference in means\n\nThe standard error is tricky.\nThe degrees of freedom is the smallest sample size minus 1.\n\nUsed for the p-value for hypothesis tests and the critical value for confidence intervals.\n\nThe null hypothesis is usually 0, and the alternate dependes on the order in which you subtract the means."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#two-sample-t-test-example",
    "href": "L17-Two_Sample_hypothesis_Tests.html#two-sample-t-test-example",
    "title": "14  Two-Sample t-Tests",
    "section": "14.5 Two Sample t-test Example",
    "text": "14.5 Two Sample t-test Example\n\n\nDo mothers who smoke give birth to children with a lower birthweight than mothers who don’t?\nTest this at the 5% level using the data on the right.\n\n\n\n\n\nSmoker\nNon-Smoker\n\n\n\n\nmean\n6.78\n7.18\n\n\nsd\n1.43\n1.60\n\n\nn\n50\n100\n\n\n\n\n\n\nThe null hypothesis is \\(H_0: \\mu_{smoke} = \\mu_{non}\\), which can be written as \\(H_0: \\mu_{non} - \\mu_{smoke} = \\mu_{n - s} = 0\\).\nThe alternate hypothesis is \\(H_a: \\mu_{smoke} &lt; \\mu_{non}\\), which could be written as\n\n\\(H_a: \\mu_{n - s} &gt; 0\\), or\n\\(H_a: \\mu_{s - n} &lt; 0\\)\n\n\nIt doesn’t matter which we choose, but we have to calculate the corresponding p-value! Let’s use \\(\\mu_{n-s}\\).\nThe sample mean difference is \\(\\bar x_{n - s} = 7.18 - 6.78 = 0.4\\). The standard error is: \\[\nSE = \\sqrt{\\dfrac{s_s^2}{n_s} + \\dfrac{s_n^2}{n_n}} = \\sqrt{\\dfrac{1.43^2}{50} + \\dfrac{1.60^2}{100}} = 0.2578721\n\\]\nSince we used \\(\\bar x_{n - s}\\), our alternate tells us to find the p-value for a t-statistic larger than what we got.\n\\[\nt_{obs} = \\dfrac{\\bar x_{n-s} - \\mu_{n-s}}{SE} = \\dfrac{0.4 - 0}{0.2578721} = 1.55\n\\]\nSince we’re doing a right-tailed test, we calculate our p-value as:\n\n1 - pt(1.55, 49)\n\n[1] 0.06378844\n\n\nTo summarise:\n\n\\(H_0: \\mu_{n - s} = 0\\) versus \\(H_0: \\mu_{n - s} &gt; 0\\)\n\\(t_{obs} = 1.55\\)\np-value is 0.06\n\nSince our p-value is larger than 0.5, we do not have a statistically significant result. We fail to reject the hypothesis that smokers have lower birthweights. We conclude that we do not have enough evidence to say that smoking is associated with lower birthweights."
  },
  {
    "objectID": "L17-Two_Sample_hypothesis_Tests.html#footnotes",
    "href": "L17-Two_Sample_hypothesis_Tests.html#footnotes",
    "title": "14  Two-Sample t-Tests",
    "section": "",
    "text": "In general, we would much, much, much rather overestimate the variance. The whole point of statistics is to avoid overconfidence in our estimates.↩︎"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#refresher",
    "title": "15  Large sample test for a proportion",
    "section": "15.1 Refresher",
    "text": "15.1 Refresher\nIn the lecture on sampling distributions, we learned that the sampling distribution of a sample proportion can be found as follows:\nIf \\(X\\sim B(n,p)\\) and \\(np&gt;10\\) and \\(n(1-p)&gt;10\\), then \\[\n\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\n\\]\nThis relies on the population proportion to find the standard error, but this is never available.1"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#hypothesis-tests-for-proportions",
    "title": "15  Large sample test for a proportion",
    "section": "15.2 Hypothesis tests for proportions",
    "text": "15.2 Hypothesis tests for proportions\nAs before, we write our hypotheses: \\[\nH_0:p = p_0 \\text{ vs. } H_A: p \\{&gt;or&lt;or\\ne\\} p_0\n\\]\nWe always write \\(H_0:p = p_0\\) and then fill in the value for \\(p_0\\), then we use that same value in the alternate hypothesis but use either \\(&gt;\\), \\(&lt;\\), or \\(\\ne\\) based on the wording of the question.\nAs before, we use the sampling distribution to find our p-value. In this case, though, we have a hypothesized value for the population proportion. In fact, we must assume that the null is true.2 If this is the case, we do have the standard error!\nI swear, this is the last time I introduce a new standard error for the sampling distribution of the sample proportion.3 Assuming \\(H_0\\) is true (and the conditions are met), \\[\n\\hat p \\sim N\\left(p_0, \\sqrt{\\frac{p_0(1-p_0)}{n}}\\right)\n\\]\n\nThe Test Statistic\nAs you can guess from the sampling distribution, the test statistic is:\n\n\n\n\n\n\nTest Statistic for a Test for Proportions\n\n\n\n\\[\nz_{obs} = \\frac{\\text{observed} - \\text{hypothesized}}{\\text{standard error}} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\\]\n\n\nand then we can use the normal distribution as usual: \\[\nP(Z \\{&gt;or&lt;or\\text{ further away than}\\} z_{obs}) = \\dots\n\\]\nwhere we use \\(&gt;\\) if the alternate hypothesis uses \\(&gt;\\), \\(&lt;\\) if the alternate hypothesis uses \\(&lt;\\), and we look at the two tails if the alternate hypothesis is \\(\\ne\\).\nA common question is: which \\(p\\) do we use to check normality? We’re supposed to check \\(np\\) and \\(n(1-p)\\), but do we use \\(\\hat p\\) or \\(p_0\\)?\nFor a hypothesis test, we assume the null is true, i.e. \\(p=p_0\\). We should use this assumption everywhere! For a hypothesis test about a proportion, we check whether \\(np_0&gt;10\\) and \\(n(1-p_0)&gt;10\\)4.\nFrom here, we proceed as usual. We check the observed test statistic against a normal distribution and see whether our data are too extreme to come from the distribution assumed in the null hypothesis."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#example",
    "title": "15  Large sample test for a proportion",
    "section": "15.3 Example",
    "text": "15.3 Example\n\nMendelian Genetics\nTo test his theory that 75% of plants would inheret a dominant gene, Gregor Mendel cross bred pure breeds of pea plants. Out of 7324 plants, 5474 showed the dominant trait. At the 4.5% level, is this compatible with the hypothesis of 75% dominant?\nSolution:\n\nCheck: \\(np_0 = 7324*0.75 &gt; 10\\) and \\(n(1-p_0) = 7324*0.25 &gt; 10\\).\n\\(z_{obs} = \\frac{\\hat p - p_0}{\\sqrt{p_0(1-p_0)/n}} = \\frac{0.747 - 0.75}{\\sqrt{0.75*0.25/7324}} = -0.513\\)\n\\(p-val = 2 *P(Z &lt; z_{obs})\\) = 2*pnorm(-0.513) = 0.608\n\nWe doubled the \\(P(Z &lt; z_{obs})\\) because we want both tails. If you do this and your p-value is larger than 1, do \\(1 - P(Z &lt; z_{obs})\\) first and then double it.\n\nConclusion: Since \\(p-val &gt; \\alpha\\), we do not reject the null. The hypothesis that 75% of plants inherent the dominant trait is compatible with the data.\n\nThe last step is important: always word your conclusion in the context of the study.\nThese methods are extremely widespread, so of course they’re implemented in R. Here’s a verification of our results:\n\nprop.test(x = 5474, n = 7324, p = 0.75)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  5474 out of 7324, null probability 0.75\nX-squared = 0.24923, df = 1, p-value = 0.6176\nalternative hypothesis: true p is not equal to 0.75\n95 percent confidence interval:\n 0.7372578 0.7572926\nsample estimates:\n        p \n0.7474058 \n\n\nThe output should look familiar - it’s very similar to the t-test output.\nWe can see that the p-value (be careful not to mix up the p-value and the estimate of \\(p\\), labelled p - these are very different things!) is a little different. Maybe it’s because of rounding errors? We calculated the z test statistic to the nearest 3 decimal places, maybe that wasn’t enough?\n\nx &lt;- 5474\nn &lt;- 7324\nphat &lt;- x / n\nse &lt;- sqrt(0.75 * (1 - 0.75) / n)\n2*pnorm((phat - 0.75) / se)\n\n[1] 0.6081484\n\n\nNope, it’s not a rounding problem!\nThe actual answer is that R uses a continuity correction factor (which isn’t going to be on the test for this course). The correction factor “shifts” the data so that the normal distribution aligns with the center of the bar, rather than the edge. See the following plot for why.\n\n\n\n\n\nAs you can see, the normal distribution aligns with the side of the bar. For values below the mean (in this case, \\(n=10\\) and \\(p=0.4\\), so the mean is 4), the normal distribution is overestimating the areas to the left, whereas above the mean it’s underestimating the areas to the left. The correction factor shifts the normal distribution to the right by 0.5 so that it’s a better estimate of the areas below the curve.\nIf we run prop.test() without the correction factor, we get the exact same p-value that we saw before.\n\nprop.test(x = 5474, n = 7324, p = 0.75, correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  5474 out of 7324, null probability 0.75\nX-squared = 0.26288, df = 1, p-value = 0.6081\nalternative hypothesis: true p is not equal to 0.75\n95 percent confidence interval:\n 0.7373269 0.7572253\nsample estimates:\n        p \n0.7474058 \n\n\nThese details are not important, just be aware that almost all tests for proportions are run with the continuity correction factor.\n\n\nMendelian Genetics Confidence Interval\nRecall from last lecture the duality of the CI and the hypothesis test. For this question, a 95.5%5\nIn order to find the confidence interval, we again need the standard error! In the hypothesis test, we assumed that \\(p_0\\) was the true population proportion in order to proceed with the test. However, we don’t make this assumption for confidence intervals.\nWhat can we do? We don’t have \\(p\\) or \\(p_0\\), so we’re left with \\(\\hat p\\), the sample proportion that we calculated. In the t-test, this meant that we needed to switch to the \\(t\\) distribution. However, that was because there was really good theory to say that the \\(t\\) distribution is the correct distribution to use. There’s no such theory here.\n\n\n\n\n\n\nCIs for Proportions Only Work When the CLT Applies\n\n\n\nThe \\(t\\)-distribution allows us to do hypothesis tests and make CIs even for smaller samples when we’re not sure that the CLT applies. For proportions, we need a “large” sample.\n\n\nNow that we know all this, the CI can be found as: \\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}} = 0.747 \\pm 2.005\\sqrt{\\frac{0.747(1-0.747)}{7324}}\n\\]\nwhich results in the CI (0.737, 0.758). This matches the CI shown in the output of prop.test() above (double check this!).\n\n\n\n\n\n\nDuality of Hypotheses and CIs\n\n\n\nFor proportions, it is not true that the CI contains all hypotheses that would not be rejected because the CI and the hypothesis test use different standard errors."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#exact-test-for-binomial",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#exact-test-for-binomial",
    "title": "15  Large sample test for a proportion",
    "section": "15.4 Exact Test for Binomial",
    "text": "15.4 Exact Test for Binomial\nIn this course, we use the normal approximation to the binomial in order to do hypothesis tests. This is not the only way to do it: we don’t always need to use the approximation! There’s something called the “exact binomial test”, which is a hypothesis test that uses the binomial distribution rather than the normal approximation (this will not be on tests).\nThere are two main reasons why we might prefer the approximation, rather than using the exact test:\n\nIf we have a large sample, then the approximation and the exact test are very very close. The approximation is computationally simpler.\n\nIf we have a small sample, neither tests can accurately approximate the variance of the population, and thus the estimated standard error isn’t well estimated either.\n\nBecause the binomial distribution is discrete, the p-values for many different test statistics will be the same. By setting \\(\\alpha\\), we might not actually be getting \\(\\alpha\\).\n\nThis is a technical point that can be safely ignored when studying for tests.\n\n\nThe exact test can be performed using the binom.test() function in R."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#summary",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#summary",
    "title": "15  Large sample test for a proportion",
    "section": "15.5 Summary",
    "text": "15.5 Summary\nCIs and Hypothesis Tests work exactly as they did before, but now we’re dealing with proportions. Just like the change from one sample to two sample \\(t\\) tests, the standard error is important and difficult.\n\nFor hypothesis tests, the standard error uses \\(p_0\\).\nFor CIs, the standard error uses \\(\\hat p\\).\n\nInterpreting these confidence intervals and hypothesis tests is very similar to before, but you must keep in mind that they’re proportions. This mainly affects how you describe the end results.\nThere are two other wrinkles to consider when using proportions:\n\nThe default test for proportions is the normal approximation with continuity correction.\n\nIt is possible, but not recommended, to not use continuity correction.\n\nThere is also an exact test, but for large samples the approximation is faster and easier."
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#self-study-questions",
    "title": "15  Large sample test for a proportion",
    "section": "15.6 Self-Study Questions",
    "text": "15.6 Self-Study Questions\n\nWhen do we use \\(\\hat p\\) in the standard error? When do we use \\(p_0\\)?\nExplain why we don’t estimate the standard error in a hypothesis test about a proportion.\nExplain in your own words why there’s no \\(t\\) version of a hypothesis test for proportions.\nWrite a good summary of the Mendelian genetics example. What did we conclude, and how is this knowledge useful?"
  },
  {
    "objectID": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "href": "L18-Hypothesis_Tests_for_Proportions.html#footnotes",
    "title": "15  Large sample test for a proportion",
    "section": "",
    "text": "If it was, then why are we doing inference?↩︎\nWe want to be strict about this so that it’s more convincing if we prove it wrong.↩︎\nI’m lying.↩︎\nAs before, both conditions must be true; it’s not enough for just \\(np_0&gt;10\\) alone.↩︎\n\\(\\alpha = 0.045\\), so \\(1 - \\alpha = 0.955\\).↩︎"
  },
  {
    "objectID": "L19-CI_for_Proportions.html#introduction",
    "href": "L19-CI_for_Proportions.html#introduction",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\n\nThis is the same as the last video.\n\nBased on our data, we make an interval that we think describes the population.\nIn this case, we just have a different population distribution?\n\n\n\nAssumptions\nIn stats, assumptions give us power but only if they hold.\nAssumptions for a CI for \\(p\\) are the same as the assumptions for the binomial distribution, with the addition of an SRS."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#the-ci-for-p",
    "href": "L19-CI_for_Proportions.html#the-ci-for-p",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.2 The CI for \\(p\\)",
    "text": "16.2 The CI for \\(p\\)\n\nSampling Distribution of \\(\\hat p\\)\nAs we saw before the midterm, if the population is \\(B(n,p)\\), then under certain conditions,\n\\[\\hat p \\sim N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\\]\n\n\nDeja-Vu\nSince \\(\\hat p \\sim N(p, \\sqrt{\\frac{p(1-p)}{n}})\\),\n\\[\n\\frac{\\hat p - p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]\nAgain, we can use the form \\(z = (x-\\mu)/\\sigma\\), but replace \\(x\\), \\(\\mu\\), and \\(\\sigma\\) with the correct values.\nA \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\]\n\n\nWe don’t know the variance, why not \\(t_{n-1}^*\\)?\n\nWe used \\(t_{n-1}^*\\) because we had to estimate \\(\\sigma\\)\nThere’s no \\(\\sigma\\) to estimate!\nThe variance of the Binomial distribution is entirely determined by \\(p\\)!\n\nBinom be crazy.\n\n\n\n\n… but Devan, we still don’t know \\(p\\)!\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{p(1-p)}{n}}\n\\] which needs \\(p\\) in the second part of the equation.\nWhy not just plug in \\(\\hat p\\)?\nOkay fine. \n\\(\\sqrt{\\hat p(1-\\hat p)/n}\\) is called the estimated standard error, since its the sd of the sampling distribution, but it’s based on an estimate.\n\n\nFinal_Version_V2_Update_LastTry_Srsly.docx.pdf\nThe \\((1-\\alpha)\\)CI for \\(p\\) is:\n\\[\n\\hat p \\pm z^*\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n\\]\nwhere \\(z^*\\) is chosen such that \\(P(Z &lt; -z^*) = \\alpha/2\\).\n\n\nDevan Style: Simulation\n\nn &lt;- 100\np &lt;- 0.7\nSE_true &lt;- sqrt(p*(1-p)/n)\np_does &lt;- c()\nphat_does &lt;- c()\nthat_does &lt;- c()\nz_star &lt;- abs(qnorm(0.05/2))\nt_star &lt;- abs(qt(0.05/2, df = n-1))\n\n\n\nDevan Style: Simulation\n\nfor(i in 1:10000){\n    new_sample &lt;- rbinom(n=1, size=n, prob=p)\n    phat &lt;- new_sample/n\n    SE_est &lt;- sqrt(phat*(1-phat)/n)\n    \n    pCI &lt;- phat + c(-1,1)*z_star*SE_true\n    phatCI &lt;- phat + c(-1,1)*z_star*SE_est\n    thatCI &lt;- phat + c(-1,1)*t_star*SE_est\n    \n    p_does[i] &lt;- pCI[1] &lt; p & pCI[2] &gt; p\n    phat_does[i] &lt;- phatCI[1] &lt; p & phatCI[2] &gt; p\n    that_does[i] &lt;- thatCI[1] &lt; p & thatCI[2] &gt; p\n}\n\n\n\nSimulation Results\n\nmean(p_does)\n\n[1] 0.9371\n\nmean(phat_does)\n\n[1] 0.9502\n\nmean(that_does)\n\n[1] 0.9502\n\n\nUsing the population proportion is… worse?\nDIY: Change \\(p\\) so that the normal approximation doesn’t apply."
  },
  {
    "objectID": "L19-CI_for_Proportions.html#examlpes-and-cautions",
    "href": "L19-CI_for_Proportions.html#examlpes-and-cautions",
    "title": "16  Confidence Intervals for a Proportion",
    "section": "16.3 Examlpes and Cautions",
    "text": "16.3 Examlpes and Cautions\n\nExample 1\nIt was found that 591 out of 700 people sampled supported a certain political position. Find a 91%CI.\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nn &lt;- 70\nphat &lt;- 68/70\nse_est &lt;- sqrt(phat*(1-phat)/n)\nz_star &lt;- abs(qnorm(0.09/2))\n\nphat + c(-1, 1)*z_star*se_est\n\n[1] 0.9376692 1.0051879\n\n\n… so it would be reasonable to say that the popluation proportion is larger than 1???\n\n\nExample 2\nIt was found that 68 out of 70 people sampled supported a certain political position. Find a 91%CI.\n\nprop.test(x = 68, n = 70)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  68 out of 70, null probability 0.5\nX-squared = 60.357, df = 1, p-value = 7.912e-15\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.8913981 0.9950358\nsample estimates:\n        p \n0.9714286"
  },
  {
    "objectID": "L20-Two_Proportions.html#diy-confidence-intervals",
    "href": "L20-Two_Proportions.html#diy-confidence-intervals",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.1 DIY Confidence Intervals",
    "text": "17.1 DIY Confidence Intervals\nThis lesson is going to be a little different from the rest. I’m not going to give you the answers, I’m going to give you the tools.\n\nStandard Error for a Single Mean\nAs we’ve seen many times, this is \\[\nSE(\\bar X) = \\frac{\\sigma}{\\sqrt{n}} = \\sqrt{\\frac{\\sigma^2}{n}},\n\\] but we often use \\[\n\\hat{SE}(\\bar X) = \\frac{S}{\\sqrt{n}} = \\sqrt{\\frac{S^2}{n}},\n\\] which is the estimated standard error (the “hat” on top of the letters “SE” indicates that it’s estimated). For the rest of this lecture, we’ll always use the estimated standard error.\n\n\nStandard Error for the Difference in Means\nEven though we were subtracting means, we added their variances and then take their square root. \\[\n\\hat{SE}(\\bar X_1 - \\bar X_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\] It is an important fact that we add the variances and then take the square root.\nFrom this lesson, also note that we had to make the assumptions:\n\nThe individuals within a group are independent of other individuals in that group.\n\nFor example, if we sample people in our own family then the samples are not independent. People in the same family tend to have similar characteristics, so knowledge of the characteristics of one family member are informative about the others.1\n\nThe groups are independent.\n\nFor example, if we’re looking at the difference in mean heights between men and women, but we have spousal pairs. Spousal pairs have a smaller difference in height than the average difference in height.2\n\n\n\n\nStandard Error for a Single Proportion\nThis is nothing new, I’m just repeating it here: \\[\n\\hat{SE}(\\hat p) = \\sqrt{\\frac{\\hat p(1 - \\hat p)}{n}}\n\\]\n\n\nStandard Error for the Difference in Proportions\nThis is up to you to find! Keed these in mind:\n\nThe standard error cannot be negative, so you probably can’t subtract things.\nVariances can be added if we assume things are independent.\n\nMake these assumptions explicit!\n\n\n\n\nConfidence Interval Example\nThis question is from OpenIntro Introductory Statistics for the Life and Biomedical Sciences, First Edition.\nThe way a question is phrased can inﬂuence a person’s response. For example, Pew Research Center conducted a survey with the following question:\n\nAs you may know, by 2014 nearly all Americans will be required to have health insurance. [People who do not buy insurance will pay a penalty] while [People who cannot afford it will receive financial help from the government]. Do you approve or disapprove of this policy?\n\nFor each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the order of the two statements was reversed. The table below shows the results of this experiment. Calculate and interpret a 90% conﬁdence interval of the difference in the probability of approval of the policy.\n\n\n\n\nSample size \\(n_i\\)\nApprove %\n\n\n\n\nOriginal Ordering\n771\n47\n\n\nReversed Ordering\n732\n34\n\n\n\nSolution\nLet \\(p_1\\) be the proportion who approve when given the original ordering with sample size \\(n_1\\), and \\(p_2\\) be the proportion who approve when given the reversed ordering with sample size \\(n_2\\). This question is asking us to calculate a confidence interval for \\(p_1 - p_2\\).\nWe first check the conditions required to use the normal approximation.\n\nPew Research Center is basically the world expert on opinion polling, so the samples are probably good.\nWe can safely assume that the samples are independent.\nThe two statements were randomly assigned, so it’s safe to say that the two groups are independent.\n\\(n_1 * 0.47 = 771 * 0.47 = 362.37\\)\n\nThere are three other calculations to check. Check them!\n\n\nNow that that’s covered, we can make a confidence interval. The general form is: \\[\n\\text{Point Estimate}\\pm\\text{Critical Value}*\\text{Standard Error}\n\\] From your homework above, verify that you can calculate the standard error as 0.025.3\nOur point estimate is \\(\\hat p_1 - \\hat p_2 = 0.47 - 0.34 = 0.13\\). Since we doing a difference in proportions, our critical value comes from the normal distribution:\n\nqnorm((1 - 0.9)/2)\n\n[1] -1.644854\n\n\nSo our confidence interval is: \\[\n0.13 \\pm 1.65*0.025 = (0.09, 0.17)\n\\]\nWe are 90% confident that the true mean difference is between 0.09 and 0.17. This provides evidence that the two proportions are indeed different."
  },
  {
    "objectID": "L20-Two_Proportions.html#hypothesis-testing",
    "href": "L20-Two_Proportions.html#hypothesis-testing",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.2 Hypothesis Testing",
    "text": "17.2 Hypothesis Testing\nHere’s where things are a little less obvious - I’m not going to get you to find the standard error yourself!\nWe are generally looking at a hypothesis test for whether two proportions are equal, that is, \\[\nH_0: p_1 = p_2\\implies p_1 - p_2 = 0\n\\] with an alternative that they are not equal, or that one is bigger than the other. In other words, we’re looking at the hypotheses:4 \\[\\begin{align*}\nH_0: &p_{1-2} = 0\\\\\nH_A: &p_{1-2} \\ne 0\\text{ or }p_{1-2} &gt; 0\\text{ or }p_{1-2} &gt; 0\\\\\n\\end{align*}\\]\nIn the lesson on proportions, we saw that the standard error depended on the null hypothesis being true, since we calculate p-values under the assumption that the null hypothesis is true. How do we do that here?\n\nThe Pooled Proportion\nUnder the null hypothesis, \\(p_1 = p_2\\). That’s like saying that we observed a bunch of successes and failures from a single group, instead of two. Let \\(x_1\\) be the number of successes in the first group, and \\(x_2\\) the number for the second. Then \\[\n\\hat p = \\frac{x_1 + x_2}{n_1 + n_2}\n\\] That is, we observed \\(x_1 + x_2\\) successes out of \\(n_1 + n_2\\) trials.\nFor example, if we assume that two coins have the same probability of heads, the getting 5 heads in 9 flips for one coin and 3 heads out of 6 flips for the other. The two coins are assumed to be identical, so it’s like we flipped one coin 15 times and got 8 heads.\nAs before, the assumption that the null hypothesis is true is used everywhere. This means it’s true for testing whether the normal approximation is appropriate. We must test \\(n_1\\hat p\\), \\(n_1(1 - \\hat p)\\), \\(n_2\\hat p\\), and \\(n_2(1 - \\hat p)\\).\nFrom this, we might assume that our standard error is something like: \\[\n\\hat{SE}(\\hat p_1 - \\hat p_2) = \\sqrt{\\frac{\\hat p(1 - \\hat p)}{???}}\n\\] The ??? might seem like it should be \\(n_1 + n_2\\), but some advanced math shows that this doesn’t quite work. Again, this is from the problem of adding variances, but working with standard deviations. Instead, the standard error is: \\[\n\\hat{SE}(\\hat p_1 - \\hat p_2) = \\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\n\\]\nThis standard error is based on the null hypothesis, specifically the assumption that the groups are identical, so it’s as if we took two samples from the same population.\nAs before, the test statistic is \\[\n\\frac{\\text{sample statistic} - \\text{hypothesized value}}{\\text{standard error}} = \\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\\] and this is compared to the normal distribution.\n\n\nHypothesis Test Example\nUsing the same example as before, we can set up our null hypothesis as \\(p_1 = p_2\\), and we’ll choose the alternate hypothesis \\(p_1 \\ne p_2\\).5 We’ll use the 5% level.\nThe “pooled” estimate is based on \\(x_1\\) and \\(x_2\\), which we can find based on \\(\\hat p_1\\) and \\(n_1\\). Since \\(\\hat p_1 = x_1/n_1\\), we can find \\(x_1 = \\hat p_1 n_1 = 771 * 0.47 = 362.37\\), which we’ll round to 362. Similarly, we’ll use \\(x_2\\) as 249. \\[\n\\hat p = \\frac{362 + 249}{771 + 732} = 0.4065\n\\]\nThe test statistic is calculated as \\[\n\\frac{(\\hat p_1 - \\hat p_2) - 0}{\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} = \\frac{(0.47 - 0.34) - 0}{\\sqrt{0.4065(1-0.4065)(1/771 + 1/732)}} = 5.12\n\\]\nWe all remember the all-important value of 1.96, right? The total area under the normal curve above 1.96 plus the area below -1.96 adds to 5%. If we get a z-score above 1.96 or below -1.96, we know that the p-value is smaller than 5%. Intuitively, 5.12 is a massive z-score, and thus will have a miniscule p-value.\n\n2 * (1 - pnorm(5.12))\n\n[1] 3.055357e-07\n\n\nThat’s a p-value of approximately 0.0000003. We can safely reject the null hypothesis.\nThis isn’t surprising, the original proportions were 0.47 and 0.35, with sample sizes of 771 and 732. Given the sample size, we expect a pretty small standard error and thus we shouldn’t be surprised that a difference of 0.13 counts as a “big” difference!"
  },
  {
    "objectID": "L20-Two_Proportions.html#example",
    "href": "L20-Two_Proportions.html#example",
    "title": "17  Inference for the Difference in Proportions",
    "section": "17.3 Example",
    "text": "17.3 Example\nThe following example comes from OpenIntro Statistics for Health and Life Sciences.\nThe use of screening mammograms for breast cancer has been controversial for decades because the overall benefit on breast cancer mortality is uncertain. Several large randomized studies have been conducted in an attempt to estimate the effect of mammogram screening. A 30-year study to investigate the effectiveness of mammograms versus a standard non-mammogram breast cancer exam was conducted in Canada with 89,835 female participants. During a 5-year screening period, each woman was randomized to either receive annual mammograms or standard physical exams for breast cancer. During the 25 years following the screening period, each woman was screened for breast cancer according to the standard of care at her health care center.\nAt the end of the 25 year follow-up period, 1,005 women died from breast cancer. The results by intervention are summarized below.\n\n\n\n\nDied\nSurvived\n\n\n\n\nMammogram\n500\n44,425\n\n\nControl\n505\n44,405\n\n\n\nAssess whether the normal model can be used to analyze the study results.\nSince the participants were randomly assigned to each group, the groups can be treated as independent, and it is reasonable to assume independence of patients within each group. Participants in randomized studies are rarely random samples from a population, but the investigators in the Canadian trial recruited participants using a general publicity campaign, by sending personal invitation letters to women identified from general population lists, and through contacting family doctors. In this study, the participants can reasonably be thought of as a random sample.\nThe pooled proportion \\(\\hat{p}\\) is\n\\[\n\\hat{p} = \\dfrac{x_{1} + x_{2}}{n_{1} + n_{2}} = \\dfrac{500 + 505}{500 + 44425 + 505 + 44405} = 0.0112\n\\]\nChecking the success-failure condition for each group: \\[\\begin{align*}\n\\hat{p} \\times n_{mgm} &= 0.0112 \\times \\text{44,925} = 503\\\\\n(1 - \\hat{p}) \\times n_{mgm} &= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\\n\\hat{p} \\times n_{ctrl} &= 0.0112 \\times \\text{44,910} = 503\\\\\n(1 - \\hat{p}) \\times n_{ctrl} &= 0.9888 \\times \\text{44,910} = \\text{44,407}\n\\end{align*}\\] All values are at least 10.6\nThe normal model can be used to analyze the study results.\nWe can use this information to do a hypothesis test for the equality of proportions.\nThe standard error is still: \\[\n\\sqrt{\\hat p(1 - \\hat p)\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)} = 0.000702\n\\] That’s quite small, but this is to be expected with such a large sample size.\nThe test statistic is \\(\\hat p_1 - \\hat p_2 / 0.000706 = -0.17\\). Again, using our intuition, this is way lower than our 1.96 value, so this is very much not a significant result.\nWe conclude that there is insufficient evidence to reject the null hypothesis; the observed difference in breast cancer death rates is reasonably explained by sampling error when the two proportions are equal.\nEvaluating medical treatments typically requires accounting for additional evidence that cannot be evaluated from a statistical test. For example, if mammograms are much more expensive than a standard screening and do not offer clear benefits, there is reason to recommend standard screenings over mammograms. This study also found that a higher proportion of diagnosed breast cancer cases in the mammogram screening arm (3250 in the mammogram group vs 3133 in the physical exam group), despite the nearly equal number of breast cancer deaths. The investigators inferred that mammograms may cause over-diagnosis of breast cancer, a phenomenon in which a breast cancer diagnosed with mammogram and subsequent biopsy may never become symptomatic. The possibility of over-diagnosis is one of the reasons mammogram screening remains controversial."
  },
  {
    "objectID": "L20-Two_Proportions.html#footnotes",
    "href": "L20-Two_Proportions.html#footnotes",
    "title": "17  Inference for the Difference in Proportions",
    "section": "",
    "text": "Recall that independence means that knowledge of one outcome gives you a better guess at other outcomes.↩︎\nIn this example, we could find the difference in heights between spouses, then use this collection of differences in a one-sample t-test, which gives us different information, but it’s also interesting.↩︎\nThis is to ensure you have the correct caclulation, you won’t need to do this on a test.↩︎\nThere may be a time in your life where you test whether \\(p_1 - p_2 = 0.25\\) or something like that, and you’ll need to modify the methods a little bit.↩︎\nYou might have also chosen \\(p_1 &gt; p_2\\) if you thought, before seeing the results of the study, that the original order would lead to more agreement.↩︎\nIt is worth noting that these values are very close to the original values we were given. If we were doing a confidence interval where we don’t use the pooled proportion, we could have just checked the values in the given table!↩︎"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#differences-in-proportions-independence",
    "href": "L21-ChiSquare_Multiple_Proportions.html#differences-in-proportions-independence",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.1 Differences in Proportions; Independence",
    "text": "18.1 Differences in Proportions; Independence\nRecall the following example from the lesson on multiple proportions.1 We were interested in whether getting a mammogram lead to fewer deaths due to breast cancer, and were presented the following data:\n\n\n\n\nDied\nSurvived\nTotal\n\n\n\n\nMammogram\n500\n44,425\n44,925\n\n\nControl\n505\n44,405\n44,910\n\n\nTotal\n1,005\n88,830\n89,835\n\n\n\nIn that lesson, we asked whether the proportion of people who died was the same in the mammogram group and the control group. This is a very specific approach, and in this lesson we will generalize it to many situations.\nThe question can be re-worded as “Does knowing that the patient got a mammogram tell us more about whether they survived?” This phrasing should sound familiar - it’s a question about independence! Instead of asking about the difference in proportions, we can ask about whether the survival of the patient is independent of the method of screening.\nIn essence, we’re checking all of the potential conditional probabilities. This includes P(Died | Mammogram) \\(\\stackrel{?}{=}\\) P(Died) as well as P(Mammogram | Died) \\(\\stackrel{?}{=}\\) P(Mammogram). Technically, these two statements are equivalent, so we can think about it whichever way is more useful. The test we’re about to describe also tests for whether P(Died | Control) \\(\\stackrel{?}{=}\\) P(Died) at the same time.\n\n\n\n\n\n\nTest for Independence of Columns and Rows\n\n\n\nThe Chi-Square test that we are about to learn is a test of whether the rows of a two-way table are independent of the columns. This works no matter how many rows/columns there are.\n\\[\nH_0: \\text{The rows are indepenent of the columns} vs. \\text{There is some form of dependence}\n\\] The Chi-Square test gives a significant result if there is any deviation from independence, even if it’s just one cell in the two-way table that doesn’t fit the pattern.\n\n\nThe interpretation of the test is that all of the rows look the same as each other; the counts in the rows are random deviations from the same distribution. The same interpretation applies to columns.\nIn this example, the test for a difference in proportions is the exact same idea as a test for independence of rows and columns, but this will generalize the same idea to any number of rows/columns."
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#expected-counts",
    "href": "L21-ChiSquare_Multiple_Proportions.html#expected-counts",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.2 Expected Counts",
    "text": "18.2 Expected Counts\nJust like in the tests for two proportions, we’re going to see what would have happened if there was actually no difference. That is, what would the table above look like if the outcome was independent of the screening method?\nAs you’ll clearly recall, we can multiply probabilities if they are independent. That is, \\[\nP(A\\text{ and }B) \\stackrel{indep}{=}P(A)P(B),\n\\] where, again, I stress that this is only true if events A and B are independent.\nFor hypothesis tests, we calculate things assuming that the null hypothesis is true. In this case, we assume that the events are independent and thus we can multiply their probabilities. So the proportion of people we expect to see in the “mammogram and died” group is: \\[\nP(\\text{mammogram and died}) = P(\\text{mammogram})P(\\text{died}) = \\left(\\frac{44925}{89835}\\right)\\left(\\frac{1005}{89835}\\right) \\approx 0.5662\n\\] Where did these numbers come from? \\(P(\\text{mammogram})\\) is the number of people in the mammogram row divided by the total number of people. That is, this is the proportion of people who were screened via mammogram, regardless of whether they survived. Similarly, 10005 is the number of people who did not survive, regardless of whether they were screened via mammogram.2 Note that this the probability of mammogram and died, not the probability of death given that they were screened via mammogram.\nThis is the proportion of patients, so the expected count is just \\(np\\), the sample size times the proportion. Notice what happens to the calculation when we include this number: \\[\n88935 * P(\\text{mammogram and died}) = 88935 \\left(\\frac{44925}{89835}\\right)\\left(\\frac{1005}{89835}\\right) = \\frac{1005*44295}{89835} = 502.6\n\\]\n\n\n\n\n\n\nExpected Counts for a Two-Way Table\n\n\n\n\\[\n\\text{Expect count for the cell in row }i\\text{, column }j = \\frac{(\\text{row }i\\text{ total})(\\text{column }j\\text{ total})}{\\text{table total}}\n\\]\n\n\nThe following table shows the actual counts and expected counts in the format actual(expected). For practice, double check the calculations!\n\n\n\n\nDied\nSurvived\nTotal\n\n\n\n\nMammogram\n500 (502.6)\n44,425 (44,422.4)\n44,925\n\n\nControl\n505 (502.4)\n44,405 (44,407.6)\n44,910\n\n\nTotal\n1,005\n88,830\n89,835\n\n\n\n\nThe Chi-Square Test Statistic\nUp until this exact moment, all our test statistics have been of the form (observed - hypothesized)/standard error. This ends here. Here we’ll introduce the Chi-Square test statistic, often written as \\(\\chi^2\\), which is greek letter “chi”, pronounced “kai”.\n\n\n\n\n\n\nThe \\(\\chi^2\\) Test Statistic\n\n\n\nAfter gathering the observed counts and calculating the expected counts, the “Chi-Square” test statistic is: \\[\n\\chi^2 = \\sum_{\\text{all cells}}\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\]\n\n\nThere are a couple important features of this value:\n\nThe numbers are squared so that negatives don’t cancel out wiht positives.\nWe divide by expected counts, which means that a large deviation is okay if it’s for a large count.\n\nFor example, 500 is 5 away from 505 and 44425 is 20 away from 44405, but the 500 and the 505 “feel” like they’re closer together because the counts are small. With large counts, we’re more forgiving of observed minus expected.\n\n\nThis test statistic is based on the normal approximation to the binomial distribution, so you’d better believe that there are some conditions before we can do a hypothesis test!\n\nEach individual must be independent of each other individual.\n\nThis is very different from assuming that the clomn variable is independent of the row variable.\nFor example, random sampling will ensure independence of individuals in the study.\n\nEach expected cell count must be larger than 10.\n\nSome textbooks use the looser rule that at most 1/5th of the expected counts are less than 5. This gets confusing, and you really just need to ensure that you have a large enough sample in each cell of the two-way table.\n\n\nFor the mammogram example, these conditions are satisfied. Verify that the \\(\\chi^2\\) test stat is 0.02.\nThe p-value for a \\(z\\) test statistic is calculated from the normal distribution, the p-value for a \\(t\\) test statistic is calculated from a \\(t\\) distribution, and the \\(\\chi^2\\) test statistic is calculated from a \\(\\chi^2\\) distribution!\nThe null hypothesis for this test is simply that the rows and columns are independent, with the alternate hypothesis being that this is false. Because of the way the \\(\\chi^2\\) statistic is calculated, any difference between observed and expected increases the test statistic. In other words, we only really care about the upper tail.\nBefore we can calculate a p-value, we need to know the degrees of freedom. Again, this is a confusing concept that is often best memorized. For a two-way table, the df is \\[\ndf = (r-1)(c-1)\n\\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns.\nWe can calculate the right-tailed p-value as follows:\n\n1 - pchisq(0.02, df = 1)\n\n[1] 0.8875371\n\n\nThat’s nearly 1, so there’s no reasonable significance level for which this test would be significant. We conclude that it’s reasonable to think that the rows and columns are independent3, and so we can say that there’s no difference in outcome across different methods of screening.4\n\n\n\n\n\n\nThe \\(\\chi^2\\) Test\n\n\n\nThe \\(\\chi^2\\) test calculates the difference in observed counts and what would be expected if the rows and columns were independent, then finds a one-tailed p-value to tell whether the observed and expected counts are too different.\nA significant p-value means there is some sort of dependence, even if it’s just one cell that is sufficiently different.\n\n\n\n\nExample in R\nThe following data come from the help file for the chisq.test() function in R.\n\nparty_by_gender &lt;- as.table(rbind(c(762, 327, 468), c(484, 239, 477)))\n# The following line is just to make sure we get pretty output\n# It is NOT something you'd be expect to reproduce\ndimnames(party_by_gender) &lt;- list(gender = c(\"F\", \"M\"),\n    party = c(\"Democrat\",\"Independent\", \"Republican\"))\nparty_by_gender\n\n      party\ngender Democrat Independent Republican\n     F      762         327        468\n     M      484         239        477\n\n\nWe want to know whether the party affiliation is independent of the gender. By eye, it looks like there are more women in the democratic party, slightly more in the Independent party, and about the same in the republican party. However, there are more women in general in this study, so it’s not immediately obvious that this is a difference in party affiliation or a difference in sample sizes across groups. This is where the \\(\\chi^2\\) test works best!\nLet’s use the built-in R function to save us some work.5\n\nchisq.test(party_by_gender)\n\n\n    Pearson's Chi-squared test\n\ndata:  party_by_gender\nX-squared = 30.07, df = 2, p-value = 2.954e-07\n\n\nWe can see that the \\(\\chi^2\\) test statistic is 30.07, the degrees of freedom is (2-1)*(3-1)=1*2=2, and the resultant p-value is about 3 times ten to the negative 7. This is definitely a statistically significant relationship, and we can conclude that there’s a difference in party affiliation across genders.\nNow that we know there’s a statistically significant difference, we can see where this difference is. We can look at which observed values are furthest from the expected values. Like in linear regression, we are looking at the residuals.\n\n\n\n\n\n\nResiduals for a \\(\\chi^2\\) Test\n\n\n\nFor the cell in row \\(i\\) and column \\(j\\), the residual is defined as: \\[\n\\frac{\\text{observed} - \\text{expected}}{\\sqrt{\\text{expected}}}\n\\] This is just the square root of their contribution to the \\(\\chi^2\\) test statistic, which preserves the sign (expected counts that are too small are still negative).\n\n\n\n# Rounding the values for nicer display\nround(chisq.test(party_by_gender)$residuals, 2)\n\n      party\ngender Democrat Independent Republican\n     F     2.20        0.41      -2.84\n     M    -2.50       -0.47       3.24\n\n\nThe main thing that sticks out to me is that the count for republican women and republican men was about the same, but this is actually way more men than expected due to the sample size!"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#confidence-intervals",
    "href": "L21-ChiSquare_Multiple_Proportions.html#confidence-intervals",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.3 Confidence Intervals",
    "text": "18.3 Confidence Intervals\nLet’s not.6"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#chi-square-for-goodness-of-fit",
    "href": "L21-ChiSquare_Multiple_Proportions.html#chi-square-for-goodness-of-fit",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "18.4 Chi-Square for “Goodness of Fit”",
    "text": "18.4 Chi-Square for “Goodness of Fit”\nIn the lesson so far, the “expected” counts were the counts that would be expected if the null hypothesis were true, that is, if the rows and columns were independent. We can define the expected counts differently and still use the \\(\\chi^2\\) test!\nIn particular, we can check whether a hypothesized distribution works for a given set of data.7 For example, we can check whether the demographics of a study are the same as the demographics in the population. The following example comes from the OpenIntro textbook, where it discusses a study called the “FAMuSS” study.\n\n\n\n\nAfrican American\nAsian\nCaucasian\nOther\nTotal\n\n\n\n\nFAMuSS\n27\n55\n467\n46\n595\n\n\nUS Census\n0.128\n0.01\n0.804\n0.058\n1\n\n\nExpected\n79.16\n5.95\n478.38\n34.61\n595\n\n\n\nIn this example, we know the true distribution of ethnicities in the population, and we’re testing whether the demographics in the study follow this distribution.\nThe “Expected” counts are simply the census proportions times the sample size. We can see visually that there’s a difference, but are these differences big compared to sampling error? A hypothesis test will save us!\nWe can calculate the \\(\\chi^2\\) statistic in the exact same way: \\[\n\\chi^2 = \\sum_{\\text{all cells}}\\frac{(\\text{observed} - \\text{expected})^2}{\\text{expected}}\n\\] and compare this to a \\(\\chi^2\\) distribution. As before, I’m too lazy to do this by hand and I want R to do it for me. Let’s use the usual 5% significance level.\n\nobserved &lt;- c(27, 55, 467, 46)\nhypothesized &lt;- c(0.128, 0.01, 0.804, 0.058)\nchisq.test(x = observed, p = hypothesized)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 440.18, df = 3, p-value &lt; 2.2e-16\n\n\nAccording to R, the demographics are significantly different!"
  },
  {
    "objectID": "L21-ChiSquare_Multiple_Proportions.html#footnotes",
    "href": "L21-ChiSquare_Multiple_Proportions.html#footnotes",
    "title": "18  Chi-Square Test for Multiple Proportions",
    "section": "",
    "text": "Adapted from OpenIntro BioStats.↩︎\nIn other words, they’re the row probabilities regardless of column and the column probability regardless of row.↩︎\nWe’re searching for evidece against the null, we can never conclude that the null is true!↩︎\nWe can also say that there’s no difference in levels of screening across outcomes, but this isn’t meaningful given the context of the data.↩︎\nFor practice try to calculate these by hand!↩︎\nThere’s not really a single statistic that’s worth making a CI for. We could make one for each expected count, but that’s silly.↩︎\nThe name “Goodness of Fit” is often used for this, but it’s a bad name. The null hypothesis is that the observed data fit with the given distribution, but we never confirm the null so we can never say that it’s a “good” fit.↩︎"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#return-to-regression",
    "href": "L22-Inference_for_Regression.html#return-to-regression",
    "title": "19  Inference for Regression",
    "section": "19.1 Return to Regression",
    "text": "19.1 Return to Regression\nIn the lessons on regression, there was a recurrent theme: if the correlation was 0, then the slope was 0 (and vice-versa, since \\(b = rs_y/s_x\\)). However, in real data the correlation is never exactly 0. How do we know if it’s “close enough” to 0 to say that there’s no correlation between \\(x\\) and \\(y\\)? By comparing it to a standard error, of course!\nAfter the previous lesson (the Chi-Square test), this lesson is a return to form. We’re going back to t-tests! Hooray! But first, let’s do a quick recap on regression."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#regression-recap",
    "href": "L22-Inference_for_Regression.html#regression-recap",
    "title": "19  Inference for Regression",
    "section": "19.2 Regression Recap",
    "text": "19.2 Regression Recap\nIn regression, we’re trying to find parameters \\(a\\) and \\(b\\) in the equation \\(y = a + bx\\) to make sure that the fitted line is as “close” to the observed data as possible.\nTo find the line of best fit, we minimize the sum of squared errors, \\(\\sum(y_i - \\hat y_i)^2\\), where \\(\\hat y_i\\) is the height of the line that we get if we plug the \\(x\\) value into the model. The fact that we minimize the squares is not important, but it is important that it’s based on the quantity \\(y_i - \\hat y_i\\), called the residuals.\nFor example, consider the penguins data that we looked at earlier. In these data, we’re trying to predict the body mass of a penguin based on their flipper length. This is useful to field researchers, since measuring flipper length is much easier than weighing a penguin and still gives them some idea of how much that penguin might weigh.\n\n\n\n\n\nThe blue line is the line of best fit, which was estimated as: \\[\nbody\\_mass_i = -6787.28 + 54.62*flipper\\_length_i\n\\]\nThe slope is 54.62, meaning that the weight of the penguin increases, on average, by 54.62 grams for each 1mm increase in the flipper length. In other words, if we look at all pairs of penguins that had flipper lengths that were 1mm apart, the average difference in their body masses would be something like 54.62.1\nThe slope was calculated using our formulas from before. The correlation between flipper length and body mass is 0.7027, the standard deviation of flipper length is 6.4850, and the sd of body mass is 504.1162. Putting these together, the slope is \\(0.7072 * 504.1162 / 6.4850 = 54.9747\\). This is slightly off because of rounding - I calculated this one by hand, but the slope in the equation above was calculated with R.\nThe intercept of this model is -6787.28, which could be interpreted as saying that a penguin with a flipper length of 0 should have a body mass of about -7kg, but this isn’t how we should interpret this value.2 This intercept simply exists to shift the line up or down in order to best fit the cloud of points.\nThis interpretation of the intercept as a “nuisance” parameter3 can be seen in the way we calculate it. The calculation is \\(a = \\bar y - b\\bar x\\), i.e., the intercept is calculated to ensure we have a line with slope \\(b\\) that goes through the point \\((\\bar x, \\bar y)\\) on the plot.\nThe red line represents the residual for one of the penguins. This particular penguin had a flipper length of 207, leading to a predicted body mass of \\(-6787.28 + 54.62*207 = 4519.06\\). This particular penguin had an actual body mass of 5050, giving a residual of \\(5050 - 4519.06 = 530.94\\)."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#inference-for-the-slope-parameter",
    "href": "L22-Inference_for_Regression.html#inference-for-the-slope-parameter",
    "title": "19  Inference for Regression",
    "section": "19.3 Inference for the Slope Parameter",
    "text": "19.3 Inference for the Slope Parameter\nYou may have noticed a word show up several times in that recap: “average”. The intercept passes through the average of x and the average of y, and the slope is the average increase in \\(y\\) for a one-unit increase in \\(x\\). Linear regression is essentially just a 2 dimensional average!\nAs you might guess from this fact, we’re going back to t-tests! We still have the exact same test statistic: \\[\nt_{obs} = \\frac{\\text{Observed} - \\text{Hypothesized}}{\\text{Standard Error}}\n\\] We just have to decide on the hypotheses and calculate the standard error!\nAs noted above,4, the slope is 0 when the correlation is 0. In general, we are checking the hypotheses: \\[\\begin{align*}\nH_0: \\beta = 0\\text{ vs. }H_a: \\beta\\ne 0\\\\\n\\end{align*}\\] We are now equipped to fill out the test statistic: \\[\nt_{obs} = \\frac{b - 0}{\\text{Standard Error}}\n\\] We can just plug that into a calculator and get a result, right? Wait, something might be missing.\nIn this class, we won’t even write out the equation of the standard error. This is the sort of thing software was designed to do for you. By now, you should be able to explain the concept of the standard error to a grandparent; it’s a central part of everything we’ve done since the midterm. You should also know why it decreases with a larger sample size, and how this affects the test statistic and p-value! However, it’s fine to skip over the actual value for now and simply trust that statisticians are smart.\nWith the Standard Error being calculated by software, this hypothesis test works exactly the same as the test for a mean.\n\nAssumptions\nAs always, statistics is built on making some assumptions about the population that allow us to make inferences from a sample. The assumptions should be pretty obvious.\n\n\n\n\n\n\nLinear Regression Assumptions\n\n\n\n\nThere is some true relationship \\(y = \\alpha + \\beta x\\)\n\nThat is, the model is actually a straight line relationship with no curves.\n\nThe deviations above and below this line are normally distributed.\n\nThat is, the height of the line at a given \\(x\\) value is normal, with the mean being the height of the line.\nPut another way: The residuals are normal.\n\nThe individuals are independent of each other.\nThe variance above and below the line doesn’t depend on the \\(x\\) value.\n\n\n\n\nViolating Assumption 1\nThere is one way for a line to be straight, and an infinite number of curved lines. Basically, the plot of \\(y\\) against \\(x\\) should look linear.\n\n\n\n\n\nIn the plot above, there is clearly not a linear relationship. The math works out just fine and we can calculate a straight line that minimizes the sum of squared error, but it doesn’t tell us anything about the population.\n\n\nViolating Assumptions 2 - 4\nAgain, there are many ways to violate these assumption. A good example might be the stock price of Apple Computers (or any stock).\n\nThe price on one day is going to be close to the price the day before.\n\nNot independent!\n\nWhen Apple holds a press conference, there will be a lot more variability in the stock price depending on what they announce.\n\nThe variance depends on the \\(x\\) value!\nThis also violates the assumption of normality. Large stock price changes are to be expected, but the normal distribution doesn’t allow for this!"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#regression-in-r",
    "href": "L22-Inference_for_Regression.html#regression-in-r",
    "title": "19  Inference for Regression",
    "section": "19.4 Regression in R",
    "text": "19.4 Regression in R\nCalculating things by hand helps you conceptualize what’s going on, but it’s impractical for actual practice. As a “Statistical Programming Language”, R has so many useful functions built in.\nFor this example, we’ll use the mtcars data that is built into R, so we don’t have to worry about loading in new data. These data include various measurements of a sample of cars in the 1970s. For our purposes, we’re going to determine the relationship between fuel efficiency (\\(y\\)) as measured in miles per gallon (mpg), and weight (\\(x\\)) measured in units of 1,000 lbs.\nWe’ll start by checking a plot. I sometimes use “base R” plotting, and sometimes use “ggplot”. Neither will be tested on the final exam, but I like pointing out the distinction. Base R plotting has notation that matches the syntax of linear modelling, so it’s useful to include here.\n\ndata(mtcars) # built-in data in R\n\n# Base R plot: y ~ x, data = ...\nplot(mpg ~ wt, data = mtcars)\n\n# Linear Model (lm): y ~ x, data = ...\nmylm &lt;- lm(mpg ~ wt, data = mtcars)\n\n# Add the line to the existing plot\nabline(mylm)\n\n\n\n\nAs always: check assumptions first!\n\nThe plot above looks pretty linear.\n\nThere might be a slight curve to the line, though. The points at the left and right are mostly above the line, but the points in the middle are mostly below the line. This doesn’t appear to be a strong pattern, but it’s something to note when making a conclusion.\n\nExtrapolation is definitely not possible, but a linear model might explain the data in this range.\n\n\nThere aren’t any obvious outliers, but we’ll need to look at a different plot to really check this assumption.\nFrom the sampling strategy, I feel comfortable saying that the observations are independent.\n\nThere’s no possible test for this, it’s all about having a good sampling strategy!\n\nThis is probably fine, but again we should check other plots before we make a conclusion.\n\nThere are two assumptions that we were not able to test by looking at a plot of mpg against weight. R has some built-in plotting methods that help us with these assumptions.\n\n# Create a plotting space with 2 rows and 2 columns\n# \"mfrow\" = Multiple Figures, filled in ROW-wise\npar(mfrow = c(2,2))\n\n# The basic plot function for the output of lm\n# makes 4 different plots.\nplot(mylm)\n\n\n\n\nSome notes on these plots:\n\nResiduals versus Fitted: This is usually better to look at than the \\(y\\) versus \\(x\\). When you get into more than one \\(x\\) variable, it can be difficult to look at all of the plots, and this tells us more information anyway.\n\nFor this example, we can see the pattern again: points above the line on the left and right, and below the line in the middle. The red line helps highlight this.\n\nNormal Q-Q: This plot checks whether the residuals are normal. We’ll skip the details of how this plot is made, but it’s useful to have an intuition about these plots. Essentially, if the residuals are normal then everything should fall exactly on the dotted line. Due to random sampling it won’t, so we’re mainly looking for systematic deviations from the line. I’ve added some code at the end of this lesson for you to check this.\n\nThese data look okay, but not perfect. The residuals are possibly heavy tailed.5\n\nScale-Location: This is essentially the absolute value of the residuals, which shows whether the variance is the same for all values of \\(x\\). We want this to look like there is no pattern.\n\nThe red line wiggles a bit, but this is to be expected. It looks pretty good to me!\n\nResiduals versus Leverage: This plot is awkward to read, but shows us if any of the points are affecting the line by a lot. Essentially, we’re looking for any points on the wrong side of the dotted lines (“Cook’s Distance”). Above the 0.5 dotted line is something to look into, and something above 1 is bad for the model. Again, I’ve added an appendix about “leverage”.6\n\nNothing outside of that 0.5 dotted line, so this should be good!\n\n\nNow that we’ve looked at the plots to check our assumptions7, we can look at our estimates and our p-values.\nThe output of the lm() function isn’t very user-friendly, but the summary() function makes it nicer.\n\nsummary(mylm)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nLet’s walk through this output!\n\nThe Call: is the R code we used to make this model.\nThe Residuals show the five number summary for the residuals. If they’re normal with a mean of 0 as we assume, then they should have a minimum that is the negative of the maximum, a Q1 that is the negative of the Q3, and a median of 0.\n\nThis is a quick check to see whether the residuals are symmetric.\nFor this example, these results aren’t ideal but we also have a small data set so we can be a little forgiving.\n\nThe Coefficients table is where the magic happens.\n\n(Intercept) is our estimate of \\(a\\). Again, this is a nuisance parameter that we’re not super interested in right now.\nwt corresponds to our estimate of \\(b\\). The slope is estimated as \\(b=-5.3445\\), and it provides the standard error and the t test statistic for us!\n\nThe p-value is (Estimate - 0)/Std. Error, where the “- 0” comes from the null hypothesis that \\(\\beta = 0\\).\nThe three stars at the end of the line show significance level. *** means significant at the 0.1% level, ** is significant at the 1% level, * is 5%, and . is 10%. You should always set your significance level before looking at this table, but it gives a nice quick visual check for significance.8\n\n\nThe last block of text shows some important quantities.\n\nThe “Multiple R-squared” value is what we learned previously, whereas the “Adjusted R-squared” is something you will need to learn about when moving into multiple linear regression. For practice, try to figure out which (if any) is equal to the square of the correlation between mpg and wt!\nThe F-statistic row is also going to be very important when you move into multiple linear regression.\n\n\nLooking at this output, we can see that the slope parameter associated with wt, the weight of the car, is significantly different from 0. This means that there is statistically significant correlation between the fuel efficiency of a car and it’s weight. This isn’t surprising, but it’s always nice to have a scientific confirmation of what we hypothesized to be true."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#confidence-intervals",
    "href": "L22-Inference_for_Regression.html#confidence-intervals",
    "title": "19  Inference for Regression",
    "section": "19.5 Confidence Intervals",
    "text": "19.5 Confidence Intervals\nSince we know it’s a t-test, we’re still looking at confidence intervals of the form: \\[\n\\text{point estimate}\\pm\\text{critical value}*\\text{standard error}\n\\] where we use the SE given in the table. However, we have changed the degrees of freedom! Recall that the df can be seen as the number of parameters we can estimate from the data9, and we need to estimate the intercept. For this reason, we’ve lost another degree of freedom, so the \\(t\\) critical value is based on \\(n-2\\) degrees of freedom.\nA 95% CI for the slope in the mtcars example can be calculated as follows. We know:\n\nThe point estimate is -5.3445.\nWe’re finding a 95% CI, so we use qt((1 - 0.95)/2).\nThe SE is given from the R output as 0.5591\n\nWe just use this value - we don’t have to divide by \\(\\sqrt{n}\\) or anything like that!\n\n\nThe following R code uses + c(1, -1) to add and subtract (\\(\\pm\\)).\n\n-5.3445 + c(1, -1) * qt((1 - 0.95)/2, 32 - 2) * 0.5591\n\n[1] -6.486335 -4.202665\n\n\nSo a 95% CI for the slope is (-6.49, -4.20). This has the usual10 interpretation that, if we repeated this study many many times, then 95% of the intervals that we construct this way would contain the true population slope.\nUnlike the tests for proportions, this CI is back to having the interpretation of “contains every hypothesized value that would not be rejected by a two-sided hypothesis test”. We already did a test for \\(H_0:\\beta = 0\\) versus \\(H_a:\\beta \\ne 0\\) in which the null was rejected, and indeed 0 is not in this interval."
  },
  {
    "objectID": "L22-Inference_for_Regression.html#conclusion",
    "href": "L22-Inference_for_Regression.html#conclusion",
    "title": "19  Inference for Regression",
    "section": "19.6 Conclusion",
    "text": "19.6 Conclusion\nSo that’s it! It’s a t-test based on a standard error that we’re not going to calculate by hand. Other than the interpretation of the slope and one less degree of freedom, this is basically inference for a single mean!\nYou will still need to keep all of the assumptions of regression in mind. We need a linear relationship, independence, normality of the residuals, and constant variance across values of \\(x\\) for this test to be valid. We must check these assumptions by looking at some plots and critiquing the data collection.\nFor the exam, you’ll be expected to know:\n\nThe assumptions, and how to check them.\n\nIncluding interpreting QQ plots to say whether they’re good or bad, and interpreting whether a point looks like it might be high leverage. (In other words, the Appendices in this chapter are not just optional bonus topics.)\n\nThe interpretation of the hypothesis test and the CI\n\nStated in the context of the problem.\n\n\nA nice exam question might show you the results of plot(mylm) and summary(mylm) and ask you to make a conclusion in the context of the study.11"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#appendix---leverage",
    "href": "L22-Inference_for_Regression.html#appendix---leverage",
    "title": "19  Inference for Regression",
    "section": "19.7 Appendix - Leverage",
    "text": "19.7 Appendix - Leverage\nThe word “leverage” comes from the word “lever”, which is intentional. If you think of the line of best fit as a see-saw, a high leverage point is a point that either pushes the see-saw down or pulls it up.\nAn outlier is a point that doesn’t really fit into the pattern. There isn’t a single way to define what an “outlier” is in 2 dimensions12, so we have to be smart about it. Usually, we refer to an outlier as a point that’s far from the mean of x and y.\nNot all outliers are high leverage, though! The following plots demonstrate this idea. Both plots show the same data, but with an extra outlier. The first plot has an outlier at an x value of \\(\\bar x - 6\\) and a y value at \\(\\bar y + 15\\). The second plot has an outlier with the same \\(x\\) value, but the \\(y\\) value is \\(\\bar y - 15\\).\nThese two potential outliers are the exact same distance from the mean of \\(x\\) and the mean of \\(y\\), but have very different effects on the line! Including the red point changes the line a little, while the green point changes the line a lot! Even though they’re the same distance from the mean, the green point has higher leverage.\nThe definition of leverage is much more well-defined than the definition of an outlier. The leverage of a point is a measure of how much the line of best fit would change if that point were not in the data.13 It is possible to have outliers with low leverage. Outliers are points that are far from your data; leverage provides a measure of how well a point fits into the pattern.\n\nlm0 &lt;- lm(y ~ x)\n\nx1 &lt;- c(x, mean(x) - 6)\ny1 &lt;- c(y, mean(y) + 15)\nlm1 &lt;- lm(y1 ~ x1)\n\nx2 &lt;- c(x, mean(x) - 6)\ny2 &lt;- c(y, mean(y) - 15)\nlm2 &lt;- lm(y2 ~ x2)\n\nn1s &lt;- rep(1, n)\n\npar(mfrow = c(1,2))\nplot(x1, y1, col = c(n1s, 2), \n    pch = 16, cex = c(n1s, 2))\nabline(h = mean(y), col = \"grey\")\nabline(v = mean(x), col = \"grey\")\naxis(2, at = mean(y), labels = expression(bar(y)), las = 1)\naxis(1, at = mean(x), labels = expression(bar(x)), las = 1)\nabline(lm0)\nabline(lm1, col = 2)\nlegend(\"topright\", legend = c(\"Without Outlier\", \"With Outlier\"), col = 1:2, lty = 1)\n\nplot(x2, y2, col = c(n1s, 3), \n    pch = 16, cex = c(n1s, 2))\nabline(h = mean(y), col = \"grey\")\nabline(v = mean(x), col = \"grey\")\naxis(2, at = mean(y), labels = expression(bar(y)), las = 1)\naxis(1, at = mean(x), labels = expression(bar(x)), las = 1)\nabline(lm0)\nabline(lm2, col = 3)\nlegend(\"topright\", legend = c(\"Without Outlier\", \"With Outlier\"), col = c(1,3), lty = 1)"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#appendix---interpreting-qq-norm",
    "href": "L22-Inference_for_Regression.html#appendix---interpreting-qq-norm",
    "title": "19  Inference for Regression",
    "section": "19.8 Appendix - Interpreting QQ norm",
    "text": "19.8 Appendix - Interpreting QQ norm\nCopy the following code and past it into RStudio. Run it over and over again to get a feel for QQ plots. Change n as indicated.\nSome things you might notice:\n\nTruly normal data is never perfectly on the line!\n\nWhen interpreting QQ, it’s okay to allow for a little bit of variance!\nMost methods are fairly robust to deviations from normality.\n\nHeavy tailed data (e.g., more variance than expected by the normal distribution) result in one kind of shape, right tailed data result in a different shape.\n\nWhat do you expect left tailed data to look like?\n\n\n\n# Change this to 20, 100, 500, and 10000 to see how much changes\nn &lt;- 500\n\nnormal_sample1 &lt;- rnorm(n, mean = 0, sd = 1)\nnormal_sample2 &lt;- rnorm(n, mean = 0, sd = 1)\nt_sample &lt;- rt(n, df = 5) # Low df to highlight difference from normal\nchisq_sample &lt;- rchisq(n, df = 5)\n\npar(mfrow = c(2,2))\nqqnorm(normal_sample1, main = \"Normal Data 1\")\nqqline(normal_sample1)\nqqnorm(normal_sample2, main = \"Normal Data 2\")\nqqline(normal_sample2)\nqqnorm(t_sample, main = \"t Data - Heavy Tailed\")\nqqline(t_sample)\nqqnorm(chisq_sample, main = \"Chi Square - Right-Tailed\")\nqqline(chisq_sample)"
  },
  {
    "objectID": "L22-Inference_for_Regression.html#footnotes",
    "href": "L22-Inference_for_Regression.html#footnotes",
    "title": "19  Inference for Regression",
    "section": "",
    "text": "This isn’t exactly how it works, but it’s a useful analogy.↩︎\nEvaluating the height of the line at an x-value that is outside the range of our observations is called extrapolation, and should generally be avoided.↩︎\nA nuisance parameter is something we must calculate in order for the model to work but something we’re not planning on interpreting.↩︎\nAnd many times in previous lectures.↩︎\nYou’re not expected to be able to guess this on the exam.↩︎\nThis concept will be on the exam, but not the calculations.↩︎\nNotice how we have to fit the model before we can check the assumptions. The p-values are already calculated, but you should be very careful not to think about them before you’ve checked the assumptions!↩︎\nBad statisticians who violate the issues in the “Inference Cautions” lecture are accused of “chasing stars”.↩︎\nWith one data point, we can estimate the mean but not sd. With two, we can calculate the mean which we need for the sd. And so on.↩︎\nHighly specific, and wrong if you miss any part of it.↩︎\nPossibly with one of the assumptions violated, which you’ll have to catch on your own!↩︎\nThere’s no way to do Q1 - 1.5IQR in both \\(x\\) and \\(y\\).↩︎\nThere is an exact calculation, but we’re just concerned with the concept for now.↩︎"
  },
  {
    "objectID": "L23-ANOVA-oi.html#analysis-of-variance-anova-and-the-f-test",
    "href": "L23-ANOVA-oi.html#analysis-of-variance-anova-and-the-f-test",
    "title": "20  Comparing means with ANOVA",
    "section": "20.1 Analysis of variance (ANOVA) and the \\(F\\)-test",
    "text": "20.1 Analysis of variance (ANOVA) and the \\(F\\)-test\nThe famuss dataset was introduced in Chapter 1, Section 1.2.2. In the FAMuSS study, researchers examined the relationship between muscle strength and genotype at a location on the ACTN3 gene. The measure for muscle strength is percent change in strength in the non-dominant arm (). Is there a difference in muscle strength across the three genotype categories (CC, CT, TT)?\n\nHypotheses\nThe null hypothesis under consideration is the following: \\(\\mu_{\\texttt{CC}} = \\mu_{\\texttt{CT}} = \\mu_{\\texttt{TT}}\\). Write the null and corresponding alternative hypotheses in plain language.\n\n\nSolution\n\n\\(H_0\\): The average percent change in non-dominant arm strength is equal across the three genotypes. \\(H_A\\): The average percent change in non-dominant arm strength varies across some (or all) groups.]\n\n\n\nAssumption Checking\nThe table below provides summary statistics for each group. A side-by-side boxplot for the change in non-dominant arm strength is shown in Figure 5.24; Figure 5.25 shows the Q-Q plots by each genotype.2\n\nIt is reasonable to assume that the observations are independent within and across groups; it is unlikely that participants in the study were related, or that data collection was carried out in a way that one participant’s change in arm strength could influence another’s.\nBased on the Q-Q plots, there is evidence of moderate right skew; the data do not follow a normal distribution very closely, but could be considered to ‘loosely’ follow a normal distribution.3\nNotice from the table that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied for using ANOVA.\n\n\n\n\n\nCC\nCT\nTT\n\n\n\n\nSample size (\\(n_i\\))\n173\n261\n161\n\n\nSample mean (\\(\\bar{x}_i\\))\n48.89\n53.25\n58.08\n\n\nSample SD (\\(s_i\\))\n29.96\n33.23\n35.69\n\n\n\n\n\n\n\n\nFigure 2.24: Side-by-side box plot of the change in non-dominant arm strength for 595 participants across three groups.\n\n\n\n\n\n\n\n\n\nFigure 5.25: Q-Q plots of the change in non-dominant arm strength for 595 participants across three groups.\n\n\n\n\n\n\nDifference in Means\nThe largest difference between the sample means is between the and groups. Consider again the original hypotheses:\n\n\\(H_0\\): \\(\\mu_{\\texttt{CC}} = \\mu_{\\texttt{CT}} = \\mu_{\\texttt{TT}}\\)\n\\(H_A\\): The average percent change in non-dominant arm strength (\\(\\mu_i\\)) varies across some (or all) groups.\n\nWhy might it be inappropriate to run the test by simply estimating whether the difference of \\(\\mu_{\\texttt{CC}}\\) and \\(\\mu_{\\texttt{TT}}\\) is statistically significant at a 0.05 significance level?\n\n\nSolution\n\nIt is inappropriate to informally examine the data and decide which groups to formally test. This is a form of **data fishing*; choosing the groups with the largest differences for the formal test will lead to an increased chance of incorrectly rejecting the null hypothesis (i.e., an inflation in the Type~I error rate). Instead, all the groups should be tested using a single hypothesis test.\n\n\n\nMean Squared Error - Within and Between\nAnalysis of variance focuses on answering one question: is the variability in the sample means large enough that it seems unlikely to be from chance alone? The variation between groups is referred to as the mean square between groups (\\(MSG\\)); the \\(MSG\\) is a measure of how much each group mean varies from the overall mean. Let:\n\n\\(\\overline{x}\\) represent the mean of outcomes across all groups\n\nIn other words, it’s the mean of the data when it’s not split into groups.\n\n\\(\\overline{x}_i\\) is the mean of outcomes in a particular group \\(i\\)\n\\(n_i\\) is the sample size of group \\(i\\).\n\nThe mean square between groups is: \\[\nMSG = \\frac{1}{k-1}\\sum_{i=1}^{k} n_{i}\\left(\\overline{x}_{i} - \\overline{x}\\right)^{2} = \\frac{1}{df_{G}}SSG,\n\\] where \\(SSG\\) is the sum of squares between groups, \\(\\sum_{i=1}^{k} n_{i}\\left(\\overline{x}_{i} - \\overline{x}\\right)^{2}\\), and \\(df_{G}=k-1\\) is the degrees of freedom associated with the \\(MSG\\) when there are \\(k\\) groups.\nNotice what’s happening here: \\(MSG\\) is kind of like a variance, but not the variance of individual values.\n\n\n\n\n\n\nMSG is the variance of the means\n\n\n\nInstead of individual data, we’re looking at the mean of each group. With 3 groups, we have 3 different means. The \\(MSG\\) is like calculating the variance based on these 3 values (disregarding the variation within each group).\n\n\nUnder the null hypothesis, there is no real difference between the groups. In other words, the null hypothesis assumes that the groupings are non-informative, such that all observations can be thought of as belonging to a single group. If the null is true, then it the variability between the group means should be equal to the variability observed within a single group. The mean square error (\\(MSE\\)) is a pooled variance estimate with associated degrees of freedom \\(df_E=n-k\\) that provides a measure of variability within the groups. The mean square error is computed as: \\[\nMSE = \\frac{1}{n-k}\\sum_{i=1}^{k} (n_i-1)s_i^{2} = \\frac{1}{df_{E}}SSE,\n\\] where the \\(SSE\\) is the sum of squared errors, \\(n_i\\) is the sample size of group \\(i\\), and \\(s_i\\) is the standard deviation of group \\(i\\).\n\n\n\n\n\n\nThe MSE is the variance as if there are no groups\n\n\n\nThe MSE is almost the same as the variance of the data, assuming we lumped everything together and forgot about the groupings. The only difference is the degrees of freedom!\n\n\nUnder the null hypothesis that all the group means are equal, any differences among the sample means are only due to chance; thus, the \\(MSG\\) and \\(MSE\\) should also be equal. ANOVA is based on comparing the \\(MSG\\) and \\(MSE\\). The test statistic for ANOVA, the F-statistic, is the ratio of the between-group variability to the within-group variability: \\[\nF = \\frac{MSG}{MSE}\n\\]\nThis is a new distribution that we’re not going to talk about too much. The main thing to note is that i’s similar to the Chi-Square distribution: The test statistic is based on adding squared things, so we’re only interested in a right-tailed test.\n\n\n\n\n\n\nThe \\(F\\)-Statistic\n\n\n\nThe F-statistic can be seen as the ratio of the variance between groups and the variance within groups. A large \\(F\\)-stat means that there’s a lot of variance between groups compared to the variance of the data.\nPut another way, the data tells us how much variance to expect so that we have context for the variance of the group means.\n\n\n\n\nCalculating the \\(F\\)-statistic\nCalculate the \\(F\\)-statistic for the famuss data summarized in Figure 5.23. The overall mean \\(\\overline{x}\\) across all observations is 53.29.\n\n\nSolution: By hand\n\nFirst, calculate the \\(MSG\\) and \\(MSE\\). \\[\\begin{align*}\nMSG =& \\frac{1}{k-1}\\sum_{i=1}^{k} n_{i}\\left(\\bar{x}_{i} - \\bar{x}\\right)^{2} \\\\\n=& \\frac{1}{3-1} [(173)(48.89 - 53.29)^{2} \\\\&+ (261)(53.25 - 53.29)^{2} + (161)(58.08 - 53.29)^{2} ]\\\\\n=& 3521.69\n\\end{align*}\\]\nNotice how the MSG is essentially the variance of three observations.\n\\[\\begin{align*}\nMSE =& \\frac{1}{n-k}\\sum_{i=1}^{k} (n_i-1)s_i^{2} \\\\\n=& \\frac{1}{595-3}[(173-1)(29.96^2) + (261-1)(33.23^2) \\\\&+ (161-1)(35.69^2)] \\\\\n=& 1090.02\n\\end{align*}\\]\nThe MSE is almost the same as the variance of all observations if we were to ignore which group they were in!\nThe \\(F\\)-statistic is the ratio: \\[\ndfrac{MSG}{MSE} = dfrac{3521.69}{1090.02} = 3.23\n\\]\n\n\n\nSolution: R\n\nThe following R code will save us many calculations.\nAs in the linear regression section, I use the summary() function to get nicer output.\n\nlibrary(oibiostat)\ndata(famuss)\n\n# The data have uninformative names.\n# ndrm.ch is percentage change in non-dominant arm strength\n# actn3.r577x is the grouping variable (genotype)\nsummary(aov(ndrm.ch ~ actn3.r577x, data = famuss))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nactn3.r577x   2   7043    3522   3.231 0.0402 *\nResiduals   592 645293    1090                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice how the R output givus us - \\(df_G = 592\\), \\(MSG = 3522\\) (rounded from 3521.69) - \\(df_E = 2\\), \\(MSE = 1090\\) (rounded from 1090.02) - \\(MSG / MSE = 3.231\\)\nI will not expect you to do the calculations by hand on any test or assignment.\n\n\n\np-values\nA \\(p\\)-value can be computed from the \\(F\\)-statistic using an \\(F\\)-distribution, which has two associated parameters: \\(df_{1}\\) and \\(df_{2}\\). For the \\(F\\)-statistic in ANOVA, \\(df_{1} = df_{G}\\) and \\(df_{2}= df_{E}\\). An \\(F\\) distribution with 2 and 592 degrees of freedom corresponds to the \\(F\\)-statistic for the genotype and muscle strength hypothesis test,.\nThe larger the observed variability in the sample means (\\(MSG\\)) relative to the within-group variability (\\(MSE\\)), the larger \\(F\\) will be. Larger values of \\(F\\) represent stronger evidence against the null hypothesis. The upper tail of the distribution is used to compute a \\(p\\)-value, which is typically done using statistical software.\n\n\nConclusion\nFrom the R output, the \\(p\\)-value corresponding to the test statistic is equal to about 0.04. Does this provide strong evidence against the null hypothesis at significance level \\(\\alpha = 0.05\\)?\n\n\nSolution\n\nThe \\(p\\)-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis at a significance level of 0.05. The data suggest that average change in strength in the non-dominant arm varies by participant genotype.\n\n\n\n\n\n\n\nThe \\(F\\)-statistic and the \\(F\\)-test\n\n\n\nAnalysis of variance (ANOVA) is used to test whether the mean outcome differs across two or more groups. ANOVA uses a test statistic \\(F\\), which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If \\(H_0\\) is true and the model assumptions are satisfied, the statistic \\(F\\) follows an \\(F\\) distribution with parameters \\(df_{1}=k-1\\) and \\(df_{2}=n-k\\). The upper tail of the \\(F\\)-distribution is used to calculate the \\(p\\)-value.\nThe null hypothesis is false if at least of the means is sufficiently different from the others, relative to the variance in the data."
  },
  {
    "objectID": "L23-ANOVA-oi.html#multiple-comparisons-and-controlling-type-1-error-rate",
    "href": "L23-ANOVA-oi.html#multiple-comparisons-and-controlling-type-1-error-rate",
    "title": "20  Comparing means with ANOVA",
    "section": "20.2 Multiple comparisons and controlling Type 1 Error rate",
    "text": "20.2 Multiple comparisons and controlling Type 1 Error rate\nRejecting the null hypothesis in an ANOVA analysis only allows for a conclusion that there is evidence for a difference in group means. In order to identify the groups with different means, it is necessary to perform further testing. For example, in the famuss analysis, there are three comparisons to make: \\(\\texttt{CC}\\) to \\(\\texttt{CT}\\), \\(\\texttt{CC}\\) to \\(\\texttt{TT}\\), and \\(\\texttt{CT}\\) to \\(\\texttt{TT}\\). While these comparisons can be made using two sample \\(t\\)-tests, it is important to control the Type 1 error rate. One of the simplest ways to reduce the overall probability of identifying a significant difference by chance in a multiple comparisons setting is to use the Bonferroni correction procedure.\nIn the Bonferroni correction procedure, the \\(p\\)-value from a two-sample \\(t\\)-test is compared to a modified significance level, \\(\\alpha^\\star\\); \\(\\alpha^\\star = \\alpha/K\\), where \\(K\\) is the total number of comparisons being considered. For \\(k\\) groups, \\(K=\\frac{k(k-1)}{2}\\). When calculating the \\(t\\)-statistic, use the pooled estimate of standard deviation between groups (which equals \\(\\sqrt{MSE}\\)); to calculate the \\(p\\)-value, use a \\(t\\)-distribution with \\(df_2\\). It is typically more convenient to do these calculations using software.\n\n\n\n\n\n\nBonferroni correction\n\n\n\nThe Bonferroni correction suggests that a more stringent significance level is appropriate when conducting multiple tests: \\[\\begin{align*}\n\\alpha^\\star = \\alpha / K\n\\end{align*}\\] where \\(K\\) is the number of comparisons being considered. For \\(k\\) groups, \\(K=\\frac{k(k-1)}{2}\\).\n\n\n\nBut which group is different?\nThe ANOVA conducted on the famuss dataset showed strong evidence of differences in the mean strength change in the non-dominant arm between the three genotypes. Complete the three possible pairwise comparisons using the Bonferroni correction and report any differences.\nUse a modified significance level of \\(\\alpha^\\star = 0.05/3 = 0.0167\\). The pooled estimate of the standard deviation is \\(\\sqrt{MSE} = \\sqrt{1090.02} = 33.02\\).\nGenotype CC versus Genotype CT: \\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{48.89 - 53.25}{33.02 \\sqrt{\\frac{1}{173} + \\frac{1}{261}}} = -1.35\n\\] This results in a \\(p\\)-value of 0.18 on \\(df =592\\). This \\(p\\)-value is larger than \\(\\alpha^\\star = 0.0167\\), so there is not evidence of a difference in the means of genotypes CC and CT.\nGenotype CC versus Genotype TT: \\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{48.89 - 58.08}{33.02 \\sqrt{\\frac{1}{173} + \\frac{1}{161}}} = -2.54.\n\\]\nThis results in a \\(p\\)-value of 0.01 on \\(df =592\\). This \\(p\\)-value is smaller than \\(\\alpha^\\star = 0.0167\\), so there is evidence of a difference in the means of genotypes CC and TT.\nGenotype CT versus Genotype TT:\n\\[\nt = \\frac{\\overline{x}_1 - \\overline{x}_2}{s_{\\text{pooled}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n= dfrac{53.25 - 58.08}{33.02 \\sqrt{\\frac{1}{261} + \\frac{1}{161}}} = -1.46\n\\]\nThis results in a \\(p\\)-value of 0.14 on \\(df =592\\). This \\(p\\)-value is larger than \\(\\alpha^\\star = 0.0167\\), so there is not evidence of a difference in the means of genotypes CT and TT.\nIn R, these can be calculated using the pairwise.t.test() function as follows:\n\npairwise.t.test(famuss$ndrm.ch, famuss$actn3.r577x,\n    p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  famuss$ndrm.ch and famuss$actn3.r577x \n\n   CC    CT   \nCT 0.537 -    \nTT 0.034 0.433\n\nP value adjustment method: bonferroni \n\n\nAs you can guess from the command and its output:\n\nThere is more than one way of adjusting p-values! There is some debate about the best way, but the important thing is to recognize when they’re necessary and use them!\nThis test is using a pooled standard deviation. Thast is, it calculates the sd as if all of the data have the same variance.\n\nFor the t-test, it’s easy enough to use different variances for each group. For multiple comparisons, using the pooled variance makes a little bit more sense because it will tend to be larger, and therefore we’re adding extra variance and not being overconfident in our results.\n\n\nFrom these results, we can see that CC and TT are significantly different from each other. CC and CT are not, and neither are CT and TT.4"
  },
  {
    "objectID": "L23-ANOVA-oi.html#summary",
    "href": "L23-ANOVA-oi.html#summary",
    "title": "20  Comparing means with ANOVA",
    "section": "20.3 Summary",
    "text": "20.3 Summary\nIn summary, the mean percent strength change in the non-dominant arm for genotype CT individuals is not statistically distinguishable from those of genotype CC and TT individuals. However, there is evidence that mean percent strength change in the non-dominant arm differs between individuals of genotype CC and TT are different."
  },
  {
    "objectID": "L23-ANOVA-oi.html#footnotes",
    "href": "L23-ANOVA-oi.html#footnotes",
    "title": "20  Comparing means with ANOVA",
    "section": "",
    "text": "This is 1 minus the probability of no significant results in three tests. The probability of a result that is not significant is \\(1-0.05\\), so we do this three times and then do 1 minus the result. This is similar to the probability of not getting a 6 in 3 dice rolls.↩︎\nThe previous lesson has some code to help you interpret Q-Q plots.↩︎\nIn a more advanced course, it can be shown that the ANOVA procedure still holds with deviations from normality when sample sizes are moderately large. Additionally, a more advanced course would discuss appropriate transformations to induce normality.↩︎\nNote that this is not transitive! CC and CT are not, CT and TT are not, but CC and TT are different!↩︎"
  },
  {
    "objectID": "L24-Review.html#the-broad-topics",
    "href": "L24-Review.html#the-broad-topics",
    "title": "21  Post-Midterm Review",
    "section": "21.1 The Broad Topics",
    "text": "21.1 The Broad Topics\nThe following topics have applied to almost everything since the midterm.\n\nStandard Errors\n\nSince our sample was random, we could have gotten different data. With different data, we’d get a different mean and sd.\nThe standard error of the mean represents uncertainty around the mean. With more data, we are more certain about the value.\n\nEffects of outliers diminish, and the overall variation “averages out”.\nThe same thing happens to the sd, but it’s not a normal distribution.\n\nBe ready to explain how standard errors relate to various concepts.\n\nFor example, could you explain why test statistics and p-values have a standard error (even though you don’t know the formula for it)?\n\n\nAssumption Checking\n\nAll of the methods we use require assumptions about the population.\nThere’s always some sort of independence assumption, which can be satisfied by having a good sampling strategy.\nThere’s usually either an assumption of normality, or some conditions for which the normal approximation applies.\n\nFor means, we need a big enough sample for the Central Limit Theorem to apply (\\(n &gt; 30\\) or \\(n&gt;60\\) or something like that). This applies to all of the groups for t-tests and ANOVA.\nFor proportions, we need some form of \\(np&gt;10\\) and \\(n(1-p)&gt;10\\) so that the normal approximation applies. Be careful whether you need the observed proportion, hypothesized proportion, or pooled proportion!\nFor regression, we assume that the residuals are normal, and we can assess this with the QQ Plot.\n\nThere are other assumptions that may be method-specific.\n\nFor example, linear regression requires that the plot looks linear!\n\n\nHypothesis Testing\n\nNull and Alternative hypotheses: Convert the word problem into math.\n\nNull: Nothing is going on. Very often, this means \\(\\mu = 0\\) or \\(p = 0.5\\) or \\(\\mu_1 - \\mu_0 = 0\\). However, we could also test things like \\(p_1 - p_2 = 0.1\\), i.e., whether \\(p_1\\) is 10 percentage points higher than \\(p_2\\).\nAlternative: generally involves a \\(\\ne\\), \\(&gt;\\), or \\(&lt;\\). The wording of the statement will indicate whether it’s a two sided (\\(\\ne\\)), right-tailed (\\(&gt;\\)), or left-tailed (\\(&lt;\\)) p-value.\n\nSet the significance level.\n\nThis determines how “strong” the evidence must be in order for us to reject the null. \\(\\alpha = 0.1\\) means we’ll accept weak evidence, whereas \\(\\alpha = 0.001\\) means we need to be very sure of our results before the null is rejected.\n\nTest statistic: measures the distance between our observed data and the hypothesized value, relative to the standard error.\n\nRelies on the value from the null hypothesis, as well as the sample size!\n\np-value: ASSUMING THE NULL HYPOTHESIS IS TRUE, the p-value measures the probability of getting data at least as extreme as the data we got.\n\n“At least as extreme” = this far away from the null values or further. The direction is determined by the alternative hypothesis.\nA small p-value is strong evidence. If \\(p &lt; \\alpha\\), we reject the null and claim that our result is “statistically significant”.\nA small p-value does not mean that there’s a large difference in the observed and hypothesized! It’s a large difference relative to the standard error, which decreases with better study designs and larger sample sizes. For example, a 0.01% increase in cancer risk is not something that a person really needs to worry about, but a large enough sample might find a statistically significant result!\n\n\nConfidence Intervals\n\nIf we were to repeat the exact same study many, many times with new samples each time, \\((1-\\alpha)\\)% of the intervals we create will contain the true population parameter.\nGenerally, a CI has the form “point estimate \\(\\pm\\) critical value * standard error”\n\nPoint estimate: the mean, for example.\nCritical value: the value from the relevant distribution that makes it a \\((1-\\alpha)\\)% interval. Essentially, this controls the width of the interval so that the interval actually does contain the true parameter as often as we claim that it does.\nStandard error: see above.\n\nA 95% CI is essentially the middle 95% of the sampling distribution, if it were centered on the observed sample mean.\n\nIn other words, we use the standard error to determine an interval that we hope covers the middle 95% of all possible mean values.\n\nFor a 95% CI, we want the middle 95%. This means we want 2.5% on either side, which is why we see \\(\\alpha/2\\) a lot when dealing with confidence intervals.\n\nConclusion in the context of the problem\n\nThe beauty of stats is that it’s rigorous math with applications to the real world - always remember that the data come from somewhere and the results might be meaningful to real people!\n\nType 1 and 2 Error\n\nType 1 Error: rejecting a null when it’s true.\n\nWe reject if \\(p &lt; \\alpha\\) because our p-value is “too unlikely” under the null. However, unlikely things still happen!\nIf \\(\\alpha=0.05\\), then we reject anything with a p-value less than 5%. However, things with a p-value of 5% still happen 5% of the time.\nP(Type 1 Error) = \\(\\alpha\\).\n\nIn other words, we control P(Type 1 Error) when we choose a significance level.\n\n\nType 2 Error: Fail to reject the null when it’s false\n\nEven when the null is actually false, we may not have strong enough evidence to reject it.\n\nBetter evidence comes from either a better study design or a larger sample size.\n\nPower: 1 - P(Type 2 Error), which is generally difficult to calculate.\n\nIt is not \\(1-\\alpha\\).\n\nA study with low power might have p-values larger than \\(\\alpha\\), but the authors would not be able to say whether the null is false.\n\n\nMultiple Comparisons Problem\n\nSuppose the null is true. We still have a 5% chance of rejecting a true null, and therefore a 95% chance of correctly not rejecting it.\nIf we have two nulls, both of which are true, then the probability that we correctly don’t reject either is 0.95*0.95 = 0.9025. This means we have a 9.75% chance of rejecting at least one of them, even though they’re both true.\nThe multiple comparisons problem states that, when we check a lot of p-values, the Type 1 Error increases."
  },
  {
    "objectID": "L24-Review.html#means",
    "href": "L24-Review.html#means",
    "title": "21  Post-Midterm Review",
    "section": "21.2 Means",
    "text": "21.2 Means\n\nOne-sample t-tests and CIs for a Mean\n\nAssumptions: The population is normal, or that the sample is large enough for the CLT to apply. Independence among observations.\nNull Hypothesis: \\(H_0:\\mu = \\mu_0\\), where \\(\\mu_0\\) is the hypothesized value given in the question.\nTest Statistic: \\(t_{obs} = \\dfrac{\\bar x - \\mu_0}{s/\\sqrt{n}}\\)\np-value:\n\npt(\\(t_{obs}\\)) for left-tailed (\\(&lt;\\))\n1 - pt(\\(t_{obs}\\)) for right-tailed (\\(&gt;\\))\n2*(1 - pt(|\\(t_{obs}\\)|)) for two-tailed (\\(\\ne\\))\n\nCI: \\(\\bar x \\pm t_{n-1}^*\\dfrac{s}{\\sqrt{n}}\\)\n\nFor a \\((1-\\alpha)\\)% interval, \\(t^*_{n-1}\\) comes from qt(alpha/2)\n\nE.g., for a 95% CI, \\(\\alpha = 0.05\\) and we would use qt(0.025).\nThis could also be written as qt((1 - 0.95)/2).\n\n\n\nExample: The EPA claims that the average fuel mileage of cars is 19 mpg. Does the mtcars data set support this claim?\nA small difference from 19 would not be important to us, so we’re only going to reject this claim if there is strong evidence. For this reason, we’ll use a significance level of 0.01.\n\n\nThe plot below does not quite look normal, but for this size sample it is potentially normal “enough”.\n\n\n\n\n\n\nThe question asks whether our data support the claim, but does not ask if the true mpg is larger than or less than. We use a two-sided hypothesis test.\n\\[\nH_0: \\mu = 19\\text{ vs. }H_A:\\mu \\ne 19\n\\]\nWe’ll let R do the calculations for us.\n\n\n\nt.test(mtcars$mpg, mu = 19, \n    alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 1.0237, df = 31, p-value = 0.3139\nalternative hypothesis: true mean is not equal to 19\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n\nOur p-value is 0.31, which is much larger than our significance level of 0.05. We conclude that we cannot reject the EPA’s claim.1\n\n\nSecond Example\n\nThe ggplot2 package also loads in a data set called mpg, do these data agree with our previous conclusion?\n\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n\n\nt.test(mpg$overall, mu = 19)\n\n\n    One Sample t-test\n\ndata:  mpg$overall\nt = 2.528, df = 233, p-value = 0.01213\nalternative hypothesis: true mean is not equal to 19\n95 percent confidence interval:\n 19.18104 20.45998\nsample estimates:\nmean of x \n 19.82051 \n\n\nThe p-value for this test is 0.012, which is above our significance level of 0.01. However, it’s only slightly above!\nIf these are two well-collected samples from the population, then there’s a \\(1 - (1 - 0.01)^2 = 0.0199\\approx 2\\%\\) chance of at least one of them being significant by chance.\nAs mentioned in class, the mtcars data set is from 1973. This data set is from 1999-2008, so they’re not exactly separate samples from the same population!\n\n\n\nMatched Pairs t-test for a Mean Difference\nA matched pairs study design is one where each subject is matched with another, only one of which gets the treatment and the other gets the control. See the oitment example from the class.\nA matched pairs test is actually just a one-sample t-test for the differences. In other words, we treat each difference as if it’s the value we’re calculating. The one-sample t-test assumptions and conclusions apply.\n\n\nTwo-Sample t-tests\n\nAssumptions: Both populations are normal, or both the samples are large enough for the CLT to apply. Independence among observations within and between groups.2\nNull Hypothesis: Generally, we have the null hypothesis \\(H_0:\\mu_{diff} = 0\\), where \\(\\mu_{diff}\\) is the difference in the means (either \\(\\mu-1 - \\mu_2\\) or \\(\\mu_2 - \\mu_1\\)3).\nTest Statistic: \\(t_{obs} = \\dfrac{\\bar x_1 - \\bar x_2}{SE}\\)\n\n\\(SE\\) comes from the square root of the sum of their variances.\n\n\\(SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{2_s^2}{n_2}}\\)\n\n\np-value: Same as 1-sample, where we use a \\(t\\)-distirbution.\nCI: \\(\\bar x1 - \\bar x_2 \\pm t_{n-1}^*SE\\)\n\nExample: In the 1-sample procedure, I included a second example using a different sample of cars. The first example used a dataset called mtcars, which measured a sample of cars from 1973-1974, while the second used a data set called mpg which measures a sample of cars from 1999-2008. In this example, let’s test whether the fuel efficiency of cars has improved (i.e., the mpg has gone up).4\nIn symbols, our hypotheses are \\(H_0:\\mu_{mtcars} - \\mu_{mpg} = 0\\) versus \\(H_A: \\mu_{diff} &lt; 0\\).5\n\n\n\n\n\n\n\nFrom these two plots, it looks like there is a possible slight difference.\n\n\nWe are assuming that both data sets are based on good samples, although this may not actually be appropriate and we should acknowledge this in our conclusions.\nIt is clear that there is not dependence between these two data sets - they were collected completely separately!\n\n\n\n\nt.test(mtcars$mpg, mpg$overall, alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  mtcars$mpg and mpg$overall\nt = 0.24252, df = 36.979, p-value = 0.5951\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 2.149167\nsample estimates:\nmean of x mean of y \n 20.09062  19.82051 \n\n\nIt looks like there is not a significant difference.\nI’m always a little bit skeptical when something is doing calculations for me, so I just want to double check whether R is doing mtcars minus mpg or the other way around.\n\nmean(mtcars$mpg); mean(mpg$overall)\n\n[1] 20.09062\n\n\n[1] 19.82051\n\n\nSo it looks like R is labelling “x” as mtcars and “y” as mpg, and doing mtcars minus mpg. This means that we’re correct in using “alernative = \"less\".\nNow that we’ve double checked that the calculations were correct, we can make our conclusion. It does not appear that the two data sets have significantly different fuel efficiencies. However, this conclusion is highly sensitive to whether the data sets have “good” sampling strategies.\nThe mtcars data were sample by Motor Trend Magazine based on what they thought their audience would like, whereas the mpg data were taken from the Environmental Protection Agency without any preference. It is very important to note that neither data set represents the average mpg of cars on the road. For instance, the mpg data contain one row for the manual Chevrolet Corvette and one row for the automatic Corvette, as well as one for the Honda Civic (automatic and manual). This is not representative of cars in general, as well as not representative of the actual cars on the road.\n\n\nANOVA for Multiple Means\n\nAssumptions: The population is normal, or that the sample is large enough for the CLT to apply. Independence among observations. Independence between groups. Groups have the same variance.\nNull: All means are equal, i.e. \\(\\mu_1 = \\mu_2 = ... = \\mu_k = 0\\).\n\nThis is false if any or all of the means differ; ANOVA does not tell us which mean is significantly different from the others.\n\nTest Statistic: \\(MSG/MSE\\), which is interpreted as the variance of the group means (considering their sample sizes) divided by the variance in the data if we were to ignore the groups.\np-value: Always right-tailed, since we’re only testing whether the variance of group means is too large relative to the data varaince. The p-value is calculated from an F distribution.\nConclusions and further steps: If we reject the null and conclude that at least one group is statistically significant, we can then do a post-hoc analysis to determine which mean(s) is(are) statistically significantly different from the others.\n\nWe use special techniques to control the Type 1 error!\n\n\nExample: The EPA’s claim of 19 mpg as the average does not take into account the number of cylinders. When asked, the spokesperson said that the average mpg is the same regardless of the number of cylinders that a car has.\nLet’s test this claim! Again, we want strong evidence before we reject this claim, so we’ll set the significance level to 0.01.\n\n\nFrom the plot below, it looks like there will almost certainly be a difference!\n\n\n\n\n\n\nThe table shows that one of the assumptions of ANOVA is not satisfied (guess which before looking at the footnote6), so we should be careful when interpreting the results.\n\n\n\n\n\ncyl\nmean\nsd\nsize\n\n\n\n\n4\n26.66\n4.51\n11\n\n\n6\n19.74\n1.45\n7\n\n\n8\n15.10\n2.56\n14\n\n\n\n\n\n\n\n\nsummary(aov(mpg ~ factor(cyl), data = mtcars))\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals   29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "L24-Review.html#proportions",
    "href": "L24-Review.html#proportions",
    "title": "21  Post-Midterm Review",
    "section": "21.4 Proportions",
    "text": "21.4 Proportions\n\nOne-Sample Test for Proportions\n\nAssumptions: \\(np &gt; 10\\) and \\(n(1 - p)&gt;10\\) so that the normal approximation applies. Indpedence/good sampling.\n\nFor hypothesis tests we use \\(p_0\\) (the hypothesized proportion), for confidence intervals we use \\(\\hat p\\) (the estimated proportion).\n\nHypotheses: \\(p = p_0\\).\nTest Statistic: We assume normality, and under the null we have a standard error of \\(\\sqrt{p_0(1 - p_0)/n}\\). The test statistic is \\(z = (\\hat p - p_0)/SE\\)\np-value: Since we’re not estimating the standard deviation, we don’t get a \\(t\\) distribution. The p-value comes from the normal distribution.\nConfidence Intervals: For a CI, we don’t have a hypothesis and so we use \\(\\hat p\\) in the standard error.\n\n\\(100(1-\\alpha)\\%CI: \\hat p \\pm z^*\\sqrt{\\hat p(1 - \\hat p)/n}\\)\n\n\nExample: It is suggested that, if male penguins are more likely to be the hunters, then there should be more females than males (the hunters will get hunted by orcas). Assuming that the Palmer Penguins data are a random sample, do we have evidence at the 5% level that males are the primary hunters?\n\n\nWe are told to assume it’s a random sample, and we can see from the plots that \\(n\\) is large enough for \\(np_0&gt;10\\) and \\(n(1-p_0)&gt;10\\).\nOur hypothesis is that the proportion of males is less than 0.5: \\[\nH_0: p = 0.5\\text{ vs. }p &lt; 0.05\n\\]\n\n\n\n\n\n\n\n\nWe can look at the data as follows:\n\ntable(penguins$sex == \"male\")\n\n\nFALSE  TRUE \n  165   168 \n\nprop.test(table(penguins$sex == \"male\"), p = 0.5, alternative = \"less\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  table(penguins$sex == \"male\"), null probability 0.5\nX-squared = 0.012012, df = 1, p-value = 0.4564\nalternative hypothesis: true p is less than 0.5\n95 percent confidence interval:\n 0.0000000 0.5419071\nsample estimates:\n        p \n0.4954955 \n\n\nSince the p-value is 0.4564, we do not reject the null hypothesis. There is no evidence to suggest that there are fewer males than females, which means that males do not appear to be predated at a higher rate.\n\n\nTwo-Sample Test for Proportions\n\nAssumptions: \\(n_1\\hat p &gt; 10\\) and \\(n_1(1 - \\hat p)&gt;10\\), similar \\(n_2\\), so that the normal approximation applies. Indpedence/good sampling.\n\nFor hypothesis tests, \\(\\hat p\\) is the pooled proportion. For confidence intervals, we require that \\(n_i\\hat p_i&gt;10\\) and \\(n_i(1-\\hat p_i)&gt;10\\) for \\(i=1\\) and \\(i=2\\).\n\nHypotheses: Generally, we’re testing \\(p_1 = p_2\\), which amounts to testing \\(p_{diff} = 0\\), where \\(p_{diff} = p_2 - p_1\\) of \\(p_1 - p_2\\).\n\nIt is also possible to test, e.g., whether \\(p_2\\) is 10 percentages higher than \\(p_1\\), which would be \\(H_0:p_{diff} = 0.1\\).\n\nTest Statistic: Since this is based on a normal approximation, this works almost exactly the same as the two-sample t-test approach.\n\n\\(z = \\frac{\\hat p_{diff} - p_{diff}}{SE(\\hat p_{diff})}\\).\n\np-value: See two-sample t-tests.\nCI: See two-sample t-tests.\n\nExample: In the penguins data, penguins are sampled from three different islands: Biscoe, Dream, and Torgersen. For this example, suppose a researcher is interested in whether the islands Biscoe and Dream have the same proportion of males and females. We’ll test this at the 10% level.\nWe can treat the penguins from Biscoe as one sample, find the proportion of males, and do the same for Dream.\n\n# Code to specify the islands\ndb &lt;- penguins[penguins$island %in% c(\"Dream\", \"Biscoe\"),]\n# Ensure that R ignores the third island)\ndb$island &lt;- factor(db$island)\n# Create a two-way table\ntable(db$island, db$sex)\n\n        \n         female male\n  Biscoe     80   83\n  Dream      61   62\n\n\nFrom the two-way table, I’m guessing that we won’t reject the null! Those proportions look pretty close, and the sample size is pretty small.\nWe can use prop.test() to do the calculations for us. If we don’t specify the alternative hypothesis, it will assume two sided. This is appropriate, since we’re just looking for a difference in proportions.\n\nprop.test(table(db$island, db$sex))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(db$island, db$sex)\nX-squared = 1.7776e-30, df = 1, p-value = 1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.1273096  0.1170348\nsample estimates:\n   prop 1    prop 2 \n0.4907975 0.4959350 \n\n\nThe p-value for the two-sided alternated hypothesis is 1! This is definitely larger than our significance level, so we absolutely cannot reject the null. We conclude that there is no evidence of a difference in the proportion of males to females.\n\n\nChi-Square Test for Two-Way Tables\n\nAssumptions: Independence among individuals, large enough samples for the normal approximation to apply.\nHypotheses: The Column and the Row variable are independent.\nTest Statistic: The squared difference between the observed count and the expected count.\n\nThe expected count is what we would get if the rows and columns were independent.\n\np-value: Always right-tailed, since the test stat is a squared difference.\n\nExample: Instead of just looking at Dream and Biscoe, let’s look at all three islands.\n\ntable(penguins$island, penguins$sex)\n\n           \n            female male\n  Biscoe        80   83\n  Dream         61   62\n  Torgersen     24   23\n\n\n\nchisq.test(table(penguins$island, penguins$sex))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$island, penguins$sex)\nX-squared = 0.057599, df = 2, p-value = 0.9716\n\n\nAs we might have expected, there’s no evidence that there’s a difference in the “sex” variable in different islands.\n\n\nSecond Example\n\nWhat about a difference in the “island” variable for different sexes?\n\nchisq.test(table(penguins$sex, penguins$island))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(penguins$sex, penguins$island)\nX-squared = 0.057599, df = 2, p-value = 0.9716\n\n\nThe p-value is identical! If one is independent of the other, then the other is independent of the one!"
  },
  {
    "objectID": "L24-Review.html#practice-problems",
    "href": "L24-Review.html#practice-problems",
    "title": "21  Post-Midterm Review",
    "section": "21.5 Practice Problems",
    "text": "21.5 Practice Problems\n\nfamuss Study\nIn the famuss study, we focused on the ndrm.ch(non-dominant arm percent change after 12 weeks of strength training) and the actn3.r577x (genotype at a particular position on the genome). We used this for the lecture on Chi-Square test for proportions.\nHere are the other variables available in the data set:\n\n# install.packages(oibiostat) # Need to run *once*, then the other code will run.\nlibrary(oibiostat)\n\ndata(famuss)\nhead(famuss)\n\n  ndrm.ch drm.ch    sex age      race height weight actn3.r577x    bmi\n1      40     40 Female  27 Caucasian   65.0    199          CC 33.112\n2      25      0   Male  36 Caucasian   71.7    189          CT 25.845\n3      40      0 Female  24 Caucasian   65.0    134          CT 22.296\n4     125      0 Female  40 Caucasian   68.0    171          CT 25.998\n5      40     20 Female  32 Caucasian   61.0    118          CC 22.293\n6      75      0 Female  24  Hispanic   62.2    120          CT 21.805\n\n\nThese data are not necessarily a random sample of the population, so questions like “does weight change with age?” won’t result in conclusions that apply to the general population. However, you can do a couple of tests just to practice your skills.\nFor each example below, answer all of these questions:\n\nTest yourself on the assumptions we’re making.\n\nAre they satisfied in the example?\n\nWrite the hypotheses in the appropriate symbols.\n\nDo the statements in the examples make sense?\n\nExplain why the test is appropriate for the data.\nInterpret the conclusions in the context of the problem.\n\nI’ve provided the research questions and the code to calculate the p-value, as well as some plots to help you address the assumptions.\n\nAre 50% of the people in this study female?\n\nsum(famuss$sex == \"Female\")\nnrow(famuss)\nprop.test(sum(famuss$sex == \"Female\"), nrow(famuss), p = 0.5)\n\nIs the change in dominant arm strength larger than 0?\n\nmean(famuss$drm.ch)\nt.test(famuss$drm.ch, mu = 0, alternative = \"greater\")\n\nThe data contain a column for non-dominant and dominant arm strength. Are the means of these columns different?7\n\nt.test(famuss$drm.ch - famuss$drm.ch, mu = 0, alternative = \"two.sided\")\nAlternative question: is the mean of ndrm.ch larger than ndrm.ch by more 50? This is the same as asking if the dominant arm strength is 50 percentage points higher than the non-dominant by over 50% percentages.8 This is a small change to the code, but it adds a lot to the interpretation!\nInterpret the confidence interval.\n\nIs there a difference between change in non-dominant arm strength for men and women in this study?\n\nt.test(ndrm.ch ~ sex, data = famuss, alternative = \"two.sided\")\n\nIs there an association between race and genotype?\n\ntable(famuss$race, famuss$actn3.r577x)\nchisq.test(table(famuss$race, famuss$actn3.r577x))\n\nInterpreting the results here must be done carefully, but is important!\n\nIf the results are significant, how might you figure out which races are different?\n\nIs the mean change in non-dominant arm strength the same across races?\n\nanova(aov(ndrm.ch ~ race, data = famuss))\n\nIs there a correlation between dominant and non-dominant arm strength?9\n\nsummary(lm(drm.ch ~ ndrm.ch, data = famuss))\nInterpret the slope in the context of the problem, being careful to refer to any shortcomings of the problem.\n\nAlso, did it matter which I used as the \\(y\\) variable?\n\nIdentify any potentially influential outliers."
  },
  {
    "objectID": "L24-Review.html#footnotes",
    "href": "L24-Review.html#footnotes",
    "title": "21  Post-Midterm Review",
    "section": "",
    "text": "This does not mean that 19 is the true value, we just don’t have evidence against this value. We don’t confirm the null; it’s set up in a such a way that we seek evidence against it, not for it.↩︎\nThere is a version of the two-sample t-test that also assumes equality of variances in the two groups, but it’s generally best to avoid that assumption unless it’s abundantly clear that it holds.↩︎\nEither is fine, but you must be careful about using right or left-sided p-values↩︎\nThis example cannot be seen as a matched pairs procedure - there’s nothing to match!↩︎\nTake a moment to ensure that these make sense to you!↩︎\nEqual variance within groups↩︎\nJustify why this is a matched pairs test.↩︎\nTechnical note: we are not testing if it’s double, e.g. \\(\\mu_{dom} = 2*\\mu_{non}\\); we have not learned the machinery for this. in particular, there are some extra steps for the standard error.↩︎\nDefinitely check the plots for this one!!!↩︎"
  },
  {
    "objectID": "L24-Review.html#regression",
    "href": "L24-Review.html#regression",
    "title": "21  Post-Midterm Review",
    "section": "21.3 Regression",
    "text": "21.3 Regression\n\nAssumptions: Independence, no patterns in the residual plots\n\nResiduals vs. Fitted should be a straight line with no patterns\nQQ plot should look like a line\nScale-location should be a straight line\nNothing outise the dotted lines in the Leverage plot.\n\nHypothesis: The slope is 0 (which implies that the correlation is 0)\nTest Statistic: Like a test for a one-sample t-test, but with a more complicated standard error.\n\nR will give the standard error.\nDegrees of freedom is \\(n-2\\).\n\np-value: Generally two sided, but not always\n\nWe may be asking about a positive association, e.g. \\(H_A:\\beta &gt; 0\\).\n\nConfidence Intervals: See one-sample t-test.\n\nExample: Is there a linear relationship between the fuel efficiency of the car and the car’s weight? Test at the 5% level.\nTo test the assumptions, we have to actually fit the model first! This is a good test of self control - the p-values are just sitting there waiting to be interpreted, but we’ve got a lot of work before we should even look at them!\n\nmtcars_lm &lt;- lm(mpg ~ wt, data = mtcars)\n\npar(mfrow = c(2, 2)) # sets up plotting region for 4 plots\nplot(mtcars_lm)\n\n\n\n\n\nResiduals vs. Fitted: There’s a slight “U”-shaped pattern. This is not good!\nNormal Q-Q: Doesn’t look too bad.\nScale-location: A bit of a pattern, but overall not too bad.\nResiduals vs. Leverage: One point outside the “0.5” dotted line, which should be investigated but isn’t too bad.\n\nSo the assumptions are not really satisfied, mainly because of the “U”-shaped pattern in the first plot.\nWe’ll look at the output just for practice, but the actual pattern might not be linear.\n\nsummary(mtcars_lm)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nAccording to the output, the slope is significant. There appears to be a significant correlation between mpg and weight of the car, but the actual pattern might not be linear."
  }
]