---
title: "Two-Sample t-Tests"
author: "Devan Becker"
date: "22/07/2020"
output: 
    beamer_presentation:
        colortheme: Western2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(2112)
```

## How much can one more sample complicate things?

### Notation: Subscripts everywhere!

We now have two samples.

$\bar X_1\sim N(\mu_1, \sigma_1/\sqrt{n_1})$, where $s_1$ is the estimated standard deviation of a given sample.

$\bar X_2\sim N(\mu_2, \sigma_2/\sqrt{n_2})$ \pause

\quad

*Goal:* Are the means the same? I.e., is $\mu_1 = \mu_2$?

With two samples, the difference in means has a sampling distribution. What is that distribution? It's difficult!


The easy part is the mean of the difference. The mean of the difference is the difference in means.
$$
\bar X_1 - \bar X_2 \sim N(\mu_1 - \mu_2, ???)
$$

The hard part is the standard deviation of the difference. Take a moment and think about what we're talking about here. What does it actually mean for the difference in means to have variance?

It's the same as it was before, it just seems a little more complicated. When we take a pair of samples then find their difference, we have calculated a statistic! For every *pair* of samples, we'll get a different statistic. The variance that we seek is the variance of all of these statistics.



### The standard deviation of a difference {.t}

Again, I'm going to use a simulation to demonstrate what happens if we take a bunch of pairs of samples, then find their difference.

In this case, I'm sampling x1 as 22 values from a normal distribution with a mean of 0 and a standard deviation of 2, whereas x2 has 33 observations and comes from a distribution with a mean of 0 and a standard deviation of 3. I'm finding their means, then finding their differences. I repeat this 10,000 times, keeping track of what the difference in means was.

```{r}
## approximating the sampling distribution
differences <- c()
for(i in 1:10000){
    m1 <- mean(rnorm(22, 0, 2))
    m2 <- mean(rnorm(33, 0, 3))
    differences[i] <- m1 - m2
}
sd(differences)
```

... it's not at all obvious where this number comes from.

Both had a mean of 0, so the difference should be 0. But the standard deviation of the differences isn't obvious. There is a nice formula for the mean - $\sigma/\sqrt{n}$ - but it's not obvious how this works for two samples with different variances *and* different sample sizes!

It turns out that the following equation is the correct one for the standard deviation of the differences. Recall that the standard deviation of a sampling distribution is known as the Standard Error (SE).

$$
SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
$$

And to verify that our simulation matches this idea:

```{r}
## Check
sd(differences)
sqrt(4/22 + 9/33) # Close enough
```


### Putting it Together

Altogether, this means that the difference between means has the following sampling distribution:

$$
\bar X_1 - \bar X_2 \sim N\left(\mu_1 - \mu_2, \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}\right)
$$

From this, we get the same general ideas as before. The hypothesis tests are based on the observed test statistic:
$$
t_{obs} = \frac{\text{sample statistic} - \text{hypothesized value}}{\text{standard error}} = \frac{(\bar x_1 - \bar x_2) - 0}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
$$
where usually the hypothesized value is 0 so that we're testing whether the true population means are the same.

::: {.callout-information}
### Two-Sample Hypotheses

The usual hypotheses invlove the *equality* of the means, i.e.:
\begin{align*}
H_0: \mu_1 = \mu_2 &\Leftrightarrow H_0:\mu_1 - \mu_2 = \mu_d = 0\\
H_0: \mu_1 < \mu_2 &\Leftrightarrow H_0:\mu_1 - \mu_2 = \mu_d < 0\\
\end{align*}
:::

The confidence interval is also the same idea:
$$
\text{sample statistic}\pm\text{critical value}*\text{standard error} = (\bar x_1 - \bar x_2)\pm t^*\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
$$

In both of these, we still need to know the degrees of freedom! As we saw last lecture, the $t$-distribution requires some information about the sample size. In the one sample case, this was $n-1$. However, we now have two potentially different sample sizes! What do we do?

Recall that the $t$-distribution gets closer and closer to the normal distribution as $n$ increases. The whole point of the $t$ is to get us a little further from the normal in order to account for the variance in the sample standard deviation. For the two-sample case, there is a "correct" formula, but it's big and scary and everything we're doing now is approximate anyway. Instead, we use a more conservative approach to ensure that we're not underestimating the variance.^[In general, we would much, much, much rather *overestimate* the variance. The whole point of statistics is to avoid overconfidence in our estimates.]

::: {.callout-information}
### Two-Sample $t$ degrees of freedom

We use the smallest sample size, then subtract 1.
:::

In the simulation we did earlier, the two samples had sizes of 22 and 33. In both a CI and a hypothesis test, we would use 21 as the value in `qt()` or `pt()` (which are the same idea as `qnorm()` and `pnorm()`).



### Aside: "Pooled Variance"

There's another formula out there that uses a so-called "pooled variance" for the standard error of the differences. This assumes that both populations have the exact same variance, and tries to use information from both to estimate the variance. It essentially treats the two samples as one big sample from the same population in order to calculate the standard deviation. 

This also implicitly assumes that both populations are normal, and this is not based on the CLT. Instead, the populations need to be normal. This is a huge assumption - we can use normality from the CLT because the math checks out. Assuming normality of the population is just a wild guess that we can't really check.

It is also very, very unlikely that the two populations have the same standard deviation. 

If the two assumptions are met, then the pooled standard deviation is the "correct" formula. However, the SE that we saw before still works very well! If the assumptions are not met, then the pooled SE works poorly and the SE we've seen is still very good!

Except in exceptional cases, the SE that we've learned should be used. The idea of a pooled variance is a vestige of another age.


## Examples

### Example 1: Two-Sample versus Matched

From the Ointment example:

|   | Subject 1    | S2   | S3   | S4  | S5  | S6   | S7   | S8   |
|---|--------------|------|------|-----|-----|------|------|------|
| With Oint | 6.44 | 6.06 | 4.22 | 3.3 | 6.5 | 3.49 | 7.01 | 4.22 |
| Without   | 7.22 | 6.05 | 4.55 | 4   | 6.7 | 2.88 | 7.88 | 6.32 |
| Difference| -0.78 | 0.01|-0.33|-0.7| -0.2 | 0.61 | -0.87 | -2.1 |

First we'll do matched pairs. In this example, this is the correct test to use.

```{r}
withoint <- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.01, 4.22)
without <- c(7.22, 6.05, 4.55, 4, 6.7, 2.88, 7.88, 6.32)
diff <- withoint - without

t.test(x = diff, alternative = "less")
```

We could instead have done a two-sample test, which ignores the fact that the observations are paired. Since we know the pairings, this test is leaving out valuable information.

```{r}
t.test(x = withoint, y = without, alternative = "less")
```

This result should not be trusted since it misses a key aspect of the data.

### Example 2: Body Mass of Penguins

In this example, we'll look at the difference in body mass between male and female penguins. 

In this case, there is no clear pairing between the penguins. If they were monogomous couples, then the differences in body mass might tell us about couples, but doesn't say much about male and female penguins in general.

This example gives us the opportunity to learn more notation in R! The previous example used `t.test(x=..., y=...)` to denote the two samples. If the data are neatly formatted in a data frame, then we can use the `~` notation to demonstrate that the body mass is split into different groups for male and female:

```{r}
library(palmerpenguins)
t.test(body_mass_g ~ sex, data = penguins)
```

From this output, we get a two-sided p-value as well as a two-sided CI, both confirming that the difference in body mass is different from 0. You can also see that it's using "female minus male", rather than "male minus female". This is because R will put them in alphabetical order, so female comes first. 

You may also be happy to hear that no, you will never have to manually enter the standard error formula! You may need to interpret it for tests, though!

## Conclusion

- If you can have matched pairs, you should use a matched pairs test.
- Most of the time, you'll need to use a two-sample t-test.
    - Don't get fooled by equal sample sizes!
- A two-sample t-test is based on the difference in means
    - The standard error is tricky.
    - The degrees of freedom is the *smallest* sample size minus 1.
        - Used for the p-value for hypothesis tests and the critical value for confidence intervals.
    - The null hypothesis is usually 0, and the alternate dependes on the order in which you subtract the means.


## Two Sample t-test Example {.t}

:::: {.columns}
::: {.column width="60%"}
Do mothers who smoke give birth to children with a lower birthweight than mothers who don't? 

\pspace

Test this at the 5% level using the data on the right.

:::
::: {.column width="40%"}
\vspace{-7mm}
| | Smoker | Non-Smoker |
|---|---|---|
| mean | 6.78 | 7.18 |
| sd | 1.43 | 1.60 |
| n | 50 | 100 |
:::
::::

- The null hypothesis is $H_0: \mu_{smoke} = \mu_{non}$, which can be written as $H_0: \mu_{non} - \mu_{smoke} = \mu_{n - s} = 0$.\pause\lspace
- The alternate hypothesis is $H_a: \mu_{smoke} < \mu_{non}$, which could be written as
    - $H_a: \mu_{n - s} > 0$, or
    - $H_a: \mu_{s - n} < 0$

It doesn't matter which we choose, but we have to calculate the corresponding p-value! Let's use $\mu_{n-s}$.


The sample mean difference is $\bar x_{n - s} = 7.18 - 6.78 = 0.4$. The standard error is:
$$
SE = \sqrt{\dfrac{s_s^2}{n_s} + \dfrac{s_n^2}{n_n}} = \sqrt{\dfrac{1.43^2}{50} + \dfrac{1.60^2}{100}} = 0.2578721
$$


Since we used $\bar x_{n - s}$, our alternate tells us to find the p-value for a t-statistic larger than what we got.

$$
t_{obs} = \dfrac{\bar x_{n-s} - \mu_{n-s}}{SE} = \dfrac{0.4 - 0}{0.2578721} = 1.55
$$

Since we're doing a right-tailed test, we calculate our p-value as:

```{r}
#| echo: true
#| eval: true
1 - pt(1.55, 49)
```


To summarise:

- $H_0: \mu_{n - s} = 0$ versus $H_0: \mu_{n - s} > 0$
- $t_{obs} = 1.55$
- p-value is 0.06


Since our p-value is larger than 0.5, we do not have a statistically significant result. We fail to reject the hypothesis that smokers have lower birthweights. We conclude that we do not have enough evidence to say that smoking is associated with lower birthweights.
