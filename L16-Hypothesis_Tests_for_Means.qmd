---
title: "t-Tests for a Mean"
author: "Dr. Devan Becker"
date: "July 10, 2023"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

## When we use $s$, we use $t$


We've been over this in confidence intervals, and the same thing applies to hypothesis tests! If the population is normal (or the sample size is large enough) and we have an SRS, then
$$
\frac{\bar X - \mu}{s/\sqrt{n}}\sim t_{n-1}
$$

Again, the $t$ distribution is used to account for the extra variability from the estimated standard deviation.^[Which is used in the caclulation of the Estimated Standard Error.]

This means our test statistic is
$$
t_{obs} = \frac{\bar x - \mu}{s/\sqrt{n}}
$$


Since this is a $t$ distibution, we use `pt(t_obs, df = n -1)`, possibly one minus and/or double, depending on the alternate hypothesis.^[Like `pnorm()`, it always calculates the probability below the test statistic.]

That's it. That's the big difference. When we estimate the standard deviation, we use the t-distribution.

::: {.callout-warning}
### The t-test for a population mean

Given a sample mean $\bar x$ and a sample standard deviation $s$, our test statistic is:
$$
t_{obs} = \frac{\bar x - \mu}{s/\sqrt{n}}
$$
Our hypotheses and calculations of the p-value work the same as they did for the z-test.
:::

## Examples

### Pilot Fatigue

In the pilot fatigue example from the Understanding p-values lecture, we assumed that we had the population sd. I lied - it was actually a sample statistic! We should have used a t-test, not a z test.


Recall: 

- $H_0: \mu = 15$ versus $H_A: \mu > 15$ with 
- $n = 16$, $\bar x = 15.9$, $s = 1.2$ (not $\sigma$)
$$
t_{obs} = \frac{15.9 - 15}{1.2/\sqrt{16}} = 3
$$

Using the $t$ distribution, our p-value is:

```{r}
1 - pt(3, df = 16)
```


This is *larger* than our previous p-value of 0.0013. This will always be the case: if the $z_{obs}$ test statistic is the same as the $t_{obs}$ test statistic, then the p-value for $t_{obs}$ will be wider. 

::: {.callout-warning}
### p-values from a t-test are larger than a z-test (if you have $\sigma=s$)

We almost never know the population standard deviation, so we have extra uncertainty. With extra uncertainty, we require more *evidence*! Recall that a p-value is a measure of evidence against a null.
:::


## Matched Pairs

A matched pairs design allows us to use a one-sample t-test when it looks like we have two samples^[We'll learn about two-sample t-tests in the next lecture.]. Since the pairs are matched, we can calculate the differences between pairs and treat this like a single vector of observations. It is honkey tonk ridonkulous to say that we know the true population standard deviation for the difference in observations, so a $z$ test could never be appropriate.

Consider the following example of a matched pairs experiment. Given a sample of brave volounteers, we create a small cut on both hands and put ointment on one of the two cuts^[And most likely a bandage on both.]. This study design eliminates the variation in healing times for different people since both cuts are on the same person! For each individual, we observe a *difference*. That is, one observation per person!

|   | Subject 1    | S2   | S3   | S4  | S5  | S6   | S7   | S8   |
|---|--------------|------|------|-----|-----|------|------|------|
| With Ointment | 6.44 | 6.06 | 4.22 | 3.3 | 6.5 | 3.49 | 7.01 | 4.22 |
| Without   | 7.22 | 6.05 | 4.55 | 4   | 6.7 | 2.88 | 7.88 | 6.32 |
| Difference| -0.78 | 0.01|-0.33|-0.7| -0.2 | 0.61 | -0.87 | -2.1 |

*Note*: Differences were calculated as "With minus Without"! This will be important for setting up the alternative hypothesis later.

The important thing here is that last row of this table now represents our data - we can forget that the other two rows exist! In other words, we have *one* observation per person, rather than two sets of observations.

This is where the assumption that we know the population standard deviation is especially preposterous: we're looking just at the differences! Even if there's a true value of the sd for healing time for all people, the standard deviation of the difference between healing times isn't a reasonable quantity to speak of.

Since we're looking at the *difference*, we no longer have a hypothesized value of $\mu_0$. Instead, we hypothesize that the average pairwise difference is 0, i.e. $\mu_{with\; minus\; without} = \mu_{diff} = 0$^[In other words, the healing times are the same for each subject]. The alternative is "with" < "without", i.e. $\mu_{diff} < 0$.^[This is where it's important to know that we did "with minus without"; we could have done without minus with, but then our alternate hypotheses would need to be ">".]

```{r echo = TRUE}
x <- c(-0.78, 0.01, -0.33, -0.7, -0.2, 0.61, -0.87, -2.1)
xbar <- mean(x)
s <- sd(x)
n <- length(x)

t_obs <- (xbar - 0)/(s/sqrt(n)) # xbar is with - w/out
# Notice that we use pt() instead of pnorm()
pt(t_obs, df = n - 1) # Alternative is <
```


So our p-value is approximately 0.04. At the 5% level, the null hypothesis would be rejected and we would conclude that the ointment works^[A p-value says *nothing* about the effect size, so we can't say whether it's **practically significant**]. At the 1% level, we would conclude that it doesn't have a significant effect. This is why it's important to know the significance level before calculating the p-value - we shouldn't get to choose whether our results are statistically significant!

### t-tests in Practice

Do you think that researchers in the field are typing test statistics into their calculator? Of course not! We're finally at the point in this class where the methods are so commonly used that the built-in functions in R can calculate them.

```{r}
#| eval: true
#| echo: true
with_oint <- c(6.44, 6.06, 4.22, 3.3, 6.5, 3.49, 7.0, 4.22)
without <- c(7.22, 6.05, 4.55, 4  , 6.7, 2.88, 7.8, 6.32)
difference <- with_oint - without
t.test(difference, alternative = "less")
```

Notice that the output shows a **one-sided confidence interval**. This isn't a big leap from what you know: a confidence interval consists of all of the values that would *not* be rejected by a hypothesis test, and this works for one-sided as well as two-sided alternate hypotheses!

To get a two-sided confidence interval, we can either leave `alternative` at it's default value or set it to `"two.sided"`. We can also change the significance level with the `conf.level` argument. For an 89%CI:

```{r}
t.test(difference, alternative = "two.sided", conf.level = 0.89)
```

Notice that this calculated a two-sided p-value, which is twice what we saw before (and no longer significant at the 5% level!).


# Recap

## Hypothesis Tests in General

1. Decide on a hypothesis.
    - $H_0: \mu = \mu_0$ versus $H_a: \mu [\ne,>,\text{ or }<] \mu_0$\lspace
2. Choose a significance level $\alpha$.
    - Smaller leverl = require more evidence to reject the null.\lspace
3. Gather data
    - Independent observations from same population; random sample.\lspace
4. Calculate the test statistic based on $\bar x$, $s$, and $\mu_0$.
    - Sampling distribution is based on the *null* hypothesis.\lspace
5. Calculate the p-value according to the form of the *alternate* hypothesis.
    - If $<$, then `pnorm(z_obs)`; if $>$, then `1 - pnorm(z_obs)`; if two sided, double the correct one.

## Hypothesis Test Example

New York is sometimes called "the city that never sleeps". At the 5% level, do the following data provide evidence that the average New Yorker gets less than 8 hours of sleep per night?

| $\bar x$ | $s$ | $n$ |
|---|---|---|
|7.73 | 0.77 | 25 |

- Hypotheses: \pause $H_0: \mu = 8$, $H_a:\mu < 8$.\lspace
- $t_{obs}$ = \pause $\dfrac{7.73 - 8}{0.77/5} = -1.75$\lspace
- p-value = \pause `pt(-1.75, 24)` = `r round(pt(-1.75, 24), 4)`
- Conclude: \pause Since p < $\alpha$, we reject the null hypothesis.
    - We have found statistically significant evidence that New Yorkers sleep less than 8 hours per night on average. 

## Confidence Intervals

1. Choose a confidence level $\alpha$.
    - "100(1-$\alpha$)%CI\lspace
2. Collect data
    - Independent observations from same population; random sample.\lspace
3. Find the critical value $t^*_{n-1}$
    - We will not need $z^*$ again, except possibly as comparison.\lspace
4. Calculate $\bar x \pm t^*s/\sqrt{n}$\lspace
5. Conclude: 95% of the intervals constructed this way will contain the true population mean.

## Confidence Interval Example

Construct a 95% CI for the New York sleep example.

| $\bar x$ | $s$ | $n$ |
|---|---|---|
|7.73 | 0.77 | 25 |

- $\alpha$ = \pause 0.05\lspace
- $t^*_{n-1}$ = \pause `qt(0.025, 24)` = `r round(qt(0.025, 24), 4)`.\lspace
- $\bar x \pm t^*_{n-1}s/\sqrt{n} = 7.73 \pm 2.0639*0.77/5 = (7.41, 8.05)$\lspace
- Conclude: we are 95% confident that the true average nights sleep in New York is between 7.41 and 8.05.
    - This interval includes 8, so 8 would *not* be rejected by a hypothesis test?!?!


## Participation Questions

### Q1

What is the standard error?

\pspace

1. $\sigma/\sqrt{n}$
2. $\sqrt{\frac{p(1-p)}{n}}$
3. $\sqrt{s_1^2/n_1 + s_2^2/n}$
4. The standard deviation of the sampling distribution.

### Q2

What is the standard deviation of the sampling distribution?

\pspace

1. The standard deviation of the population divided by the square root of the sample size ($\sigma/\sqrt{n}$).
2. The standard deviation of the value of a sample statistic across all possible samples from the population.
3. The same as the standard deviation of the population.
4. The average distance to the mean of the population.

### Q3

Why does the sampling distribution have a lower variance than the population?

\pspace

1. Because the standard deviation is smaller than the variance.
2. Because the population has a larger number of possible, so the variance is smaller.
3. Because outliers are not as likely in a sample.
4. Because we are summarising many observations from a sample into a single value.

### Q4

After conducting a study, we found a p-value of 0.04. Did we find a statistically significant result?

\pspace

1. Yes, since the p-value is less than 0.05
2. No, since the p-value is less than 0.05
3. We failed to set the significance lavel ahead of time, so we have to be very careful about concluding significance.

### Q5

After conducting a study, we found a 95% confidence interval for $\mu$ from -0.1 to 1.9. What can we conclude?

\pspace

1. Since 0 is in the interval, a hypothesis test for $\mu = 0$ versus $\mu \ne 0$ would not be significant at the 5% level. 
2. Since 0 is in the interval, a hypothesis test for $\mu = 0$ versus $\mu > 0$ would not be significant at the 5% level. 
3. Since 0 is in the interval, a hypothesis test for $\mu = 0$ versus $\mu \ne 0$ would not be significant at the 2.5% level. 
4. Since 0 is in the interval, a hypothesis test for $\mu = 0$ versus $\mu > 0$ would not be significant at the 2.5% level. 

### Q6

Under which condition does the CLT *not* apply?

\pspace

1. For $\bar x$, the sample size is between 3 and 60 but a histogram of the sample appears normal.
2. For $\bar x$, the sample size is much larger than 60.
3. For $\hat p$, the sample size is much larger than 60.
4. For $\hat p$, we have checked $np>10$ and $n(1-p)>10$


<details>
    <summary>**Solution**</summary>
41431
</details>



## Self-Study Questions


1. Explain why: If the $z_{obs}$ test statistic is the same as the $t_{obs}$ test statistic, then the p-value for $t_{obs}$ will be wider. 
2. If a test is statistically signficant, does that mean there's a large effect size? That is, does a hypothesis test tell you anything about the size of the effect?
    - Compare this to confidence intervals.
3. Can we interpret a $t$ confidence interval as "all null hypothesis values that would not be rejected"?
4. Re-do the ointment example, but using without - with.
    a. Draw a t distribution and mark the two test statistics, then fill in the area that corresponds to the p-value. 












