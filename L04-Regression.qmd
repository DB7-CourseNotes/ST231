---
title: "Regression"
date: "last-modified"
---

```{r}
#| echo: false
#| eval: true
set.seed(2112)
```

## Introduction

These notes are based on Chapter 6.1 to 6.3 in OpenIntro Biostats.

In linear modelling, we have a collection of pairs $x_i$ and $y_i$. We think that there's some sort of relationship between $x$ and $y$, and we think that a line is an adequate way to characterize that relationship^[Very few things are actually linear, but lines are fantastic approximations to many things.].

Just like we assume that there's a "true" population mean, there is also a "true" slope and intercept for the line that characterizes the relationship between $x$ and $y$. In the plot below, the green line represents the "true" relationship between $x$ and $y$, and the data are random values above and below that line^[We assume that $x$ is fixed, but $y$ has random noise. In other words, $x$ is not a random variable but $y$ is.].

```{r fig.height=2.75, echo=FALSE}
par(mar = c(2,2,0.5,0.5))
x <- runif(30, 0, 10)
y <- 2 + 3*x + rnorm(length(x), 0, 4)
plot(x,y)
abline(a = 2, b = 3, col = "green", lwd = 2)
legend("topleft", legend = c("True Line", "Observations with noise"), 
    lty = c(1,0), pch = c(NA,1), col = c("green", 1))
```


In high school, you may have learned a line as $y = mx + b$. In statistics, we often use latin letters (a, b, c, d, ...) for estimates and greek letters ($\alpha$, $\beta$, $\gamma$, and other lower case versions of letters you've seen on frat/sorority houses) for population parameters^[Because we think it makes us sound smarter.]. The population line is labelled:

$$
y_i = \alpha + \beta x_i + \epsilon_i
$$

- $\alpha$ is the **intercept**.
- $\beta$ is the **slope**.
    - A 1 unit increase in $x$ corresponds to a $\beta$ increase in $y$.
- $\epsilon_i$ is random noise ($N(0,\sigma)$, although you're not expected to understand this notation yet).
    - Again, we think of $x$ as being fixed. The random noise is above and below the line, not side to side.
- The formula implies that $y_i$ is centered at $\alpha + \beta x_i$ but randomly varies above and below the line with variance $\sigma^2$.

The word "regression" means to go backward. I like to think that we are "going backward" to the population numbers from the sample values^[Actually, the word comes from "regressing to the mean", which comes from how children are closer to average height than their parents - they go back toward the mean. This is not important.]. Any situation where you are estimating a population parameter is technically a **regression**, but this terminology is not useful for this class.^[I get very annoyed by the term "Regression Analysis" because there are practically infinite different types of regression. Linear regression is just one of them, albeit a a very very very popular one.]

To regress, we **estimate** the parameters using sample statistics. $a$ is the estimate for $\alpha$, $b$ for $\beta$, and $e$ for $\epsilon$. In order to do find these sample statistics, we minimize the squared error between the line and the data:

$$e_i^2 = (y_i - a - b x_i)^2$$

In other words, we find $a$ (for $\alpha$) and $b$ (for $\beta$) that make the sum of the squared errors $e_i$ as small as possible. We use the squared errors for the same reason we use squared deviations in the forumla for the variance: so that positive and negative values do not cancel out^[Also, because the calculus works out so much better.].


The estimates $a$ and $b$ are as follows: 

\begin{align*}
b &= rs_y/s_x\\
a &= \bar y - b\bar x
\end{align*}

These are called the **least squares** estimates^[There are other ways to estimate these parameters, but they're outside the scope of this course. For example, we could use the absolute value instead of the squared value. This works just as well for the calculation, but there are a lot of mathematical reasons why the square is nice. Mainly, it has a nice derivative and allows for easy equation manipulation.]. The equation for $b$ is especially important!

In R, these can be calculated as follows. The `mtcars` data set is a collection of measurements made on various cars. In this example, we'll regress the fuel efficiency (in miles per gallon, or mpg) against the weight of the car.

```{r}
#| echo: true
## Load a built-in data set
data(mtcars) 

## Define which variables are x and y.
## This isn't necessary, but helps with teaching
x <- mtcars$wt
y <- mtcars$mpg

## Calculate the estimates by hand
b <- cor(x, y) * sd(y) / sd(x)
a <- mean(y) - b * mean(x)

## Print the estimates 
c(a, b)

## Use the built-in functions
summary(lm(y ~ x))
```

From this line, we can make **predictions** about new points by simply plugging in the $x$ value. For example, let's say we wanted to guess the mpg of a car that weighs 3,000 lbs. In the data, the units for weight are 1000 lbs, so this means plugging a value of `wt=3` into the data.

```{r}
#| echo: true
a + b * 3
```

So we would guess that a 3 ton car would have a fuel efficiency of `r round(a + b*3, 2)` miles per gallon. Let's look at this on a plot:

```{r}
#| echo: true
plot(y ~ x)
points(3, a + b * 3, col = "red", pch = 16)
```

It looks like this is somewhere around where we would expect.

If we repeat this for every possible $x$ value, we get the regression line below:

```{r}
#| echo: true
plot(y ~ x)
points(3, a + b*3, col = "red", pch = 16)
## abline adds a line with slope b and intercept a to a plot.
abline(a = a, b = b, col = "red")
```

We cal also see the values of $e$, the residuals.

```{r}
#| echo: true
e <- y - (a + b*x) ## Observed minus expected
plot(e ~ x, main = "Plot of the Residuals")
## abline can also draw a line with slope 0 (horizontal)
abline(h = 0, col = "grey")
```


## Regression Facts

Here are some facts about the least squares regression line:

- The point $(\bar x, \bar y)$ is always on the line.
    - Least squares regression can be seen as putting a line through $(\bar x, \bar y)$ and rotating it until the squared error is the smallest.
- $s_y\ge 0$ and $s_x\ge 0$, so whenever $r > 0$, we know that $b > 0$.
    - The slope has the same sign as the correlation. Otherwise, the slope could be pretty much any number, regardless of the correlation.
    - If $r = 0$, then $b = 0$, and vice versa.
    - Other than the sign and the special case of $r=0$, there is no way to tell the value of $r$ if all you know is $b$.
- For $r$, the distinction between $y$ and $x$ doesn't matter.
    - For the regression line, it *absolutely* matters!
- The sum of the errors is 0.

```{r}
#| echo: true
## The prediction at mean(x) is equal to mean(y)
## In other words, (mean(x), mean(y)) is a point on the line
a + b * mean(x)
mean(y)

## Correlation doesn't care about order
cor(x, y)
cor(y, x)

## Theoretically 0, but computers aren't perfectly precise
## Note: e-14 refers to 10^-14, or 14 zeroes before the first digit
    # So, pretty close to 0.
sum(e) 
```

## Percent of Variation Explained

Because of some mathematical magic, $r^2$, the squared value of $r$, can be interpreted as:

> **The percent of variation in $y$ that can be "explained" by the linear model**.^[Usually $r^2$ is labelled $R^2$ for historical reasons. Capitalization matters in math; it's just coincidence that both lower case and upper case mean the same thing here.]

The value of $r^2$ can be calculated as:
$$
R^2 = r^2 \approx \frac{\text{Variance of the predicted }y\text{-values}}{\text{Variance of the observed }y\text{-values}}
$$

I'll explain this in steps. The first plot below shows just the values in $y$. This collection of values has a own mean and variance.

The second plot shows the change in variance that the line "explains". Instead of deviations above and below the mean, the variance can now be characterized as the deviations above and below the regression line. This variance will always be lower than the variance of $y$ without incorporating $x$^[Except when $r=0$, can you explain why?]. 

The third plot shows where this variance went. *The line itself has variance*; there is deviation in the line above and below the mean of $y$. This is the variance that gets explained by incorporating $x$! If you consider one of the points in $y$, say $y_1$, the distance between $y_1$ and $\bar y$ can be split up into the difference between $\bar y$ and the regression line plus the distance between the regression line and $y_1$. 

```{r fig.height=4}
#| echo: false
#| eval: true
layout(mat = matrix(c(1,2,3), nrow = 1), widths = c(0.5,1,1))
set.seed(18)
x <- runif(25, 0, 10)
y <- rnorm(25, 2 + 5*x, 6)

plot(rep(1, 25), y, xlab = "y", ylab = "The observed y-values have variance", xaxt = "n")
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

plot(x, y, ylab = "The residuals have variance")
abline(lm(y~x))
abline(h = mean(y))
axis(2, mean(y), bquote(bar(y)), las = 1)

mids <- predict(lm(y~x))
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(y[i], mids[i]), col = 1)
}

mids <- predict(lm(y~x))
plot(mids ~ x, type = "n", ylab = "The predicted y-values have variance, too!")
for(i in seq_along(mids)){
    lines(x = rep(x[i], 2), y = c(mean(y), mids[i]))
}
axis(2, at = mean(y), labels = bquote(bar(y)), las = 1)
abline(h = mean(y))
```

The rest of the variance is left **unexplianed**. No regression will ever be perfect unless we are studing a very very simple .

To see this a different way, consider what happens when $r = 0$^[Therefore the slope will also be 0.]. This will just be a horizontal line, and none of the variance is explained. On the other had, if $r = 1$ then all of the points will be exactly on the line. All of the variance in $y$ has been explained by the regression against $x$ - there's no variance left to be explained!^[Statistics is *still* just the study of variance.]

Notice how the R output includes 


## Extensions and Cautions

### Prediction

For a new $x$ value,
$$y = a + bx$$
is the **predicted** value of $y$. That is, if we have an $x$ value, we can plug it into the equation and find out what value of $y$ we would expect.


Note: There is still variance around this prediction! Our "expected" value will never be exactly equal to the truth - The value of $y$ at a given value of $x$ follows a normal distribution^[Our **prediction** is just us guessing the mean value of $y$ at different values of $x$.], and the probability of a single point is 0!


### Extrapolation

**Extrapolation** is what happens when prediction goes wrong. In particular, it's what happens when we try to make a prediction at an $x$ value where we don't have any data. Usually this means we're predicting an $x$ value far above or far below the range of our data, but it can also happen if there's a gap in the middle of our data.

In the plot below, the black dots are the original data, and we're trying to predict a new value at $x = 25$. The red line is the true model that I generated the data from. The black line represents a linear model. This model fits the original data quite well^[Even though it's not the true relationship, it's a reasonable approximation.], but predictions are completely inappropriate for values outside the data.

```{r echo = FALSE}
set.seed(2112)
x <- c(runif(29, 0, 10), 25)
y <- 1 + 0.25*x^2 + rnorm(length(x), 0, 5)
plot(x, y)
points(x[30], y[30], col = 2, pch = 16)
points(x[30], y[30], col = 1, pch = 1)
abline(lm(y ~ x, data = data.frame(y=y[1:29], x = x[1:29])), lwd = 2, col = 3)
#abline(lm(y ~ x, data = data.frame(y=y, x = x)), col = 0)
text(x = x[30], y = y[30], labels = "New Obs ->", pos = 2)

newx <- x[30]
newy <- c(1,x[30])%*%coef(lm(y ~ x, data = data.frame(y=y[1:29], x = x[1:29])))
text(x = newx, y = newy, labels = "Prediction ->", pos = 2)
points(x = newx, y = newy, col = 3, pch = 16)
points(x = newx, y = newy, col = 1, pch = 1)
xseq <- seq(0, 25, 0.01)
lines(xseq, 1 + 0.25*xseq^2, col = 2, lwd = 2)
legend("topleft", legend = c("Linear Model", "True Model"), col = c(3,2), lwd = 1)
```




### Lurking Variables

The black line in the plot below represents a regression where all of the data was lumped together. As we can see, this line does not seem to fit the data well. There is a hidden relationship in the the data - the green points and the red points should be considered separately^[Possibly as a blocking variable.].

```{r echo=FALSE}
set.seed(42)
x1 <- runif(30, 0, 10)
x2 <- runif(42, 0, 10)
y1 <- 3*x1 + rnorm(length(x1), 0, 7)
y2 <- 30 - 3*x2 + rnorm(length(x2), 0, 7)
plot(x1, y1, col = 2)
points(x2, y2, col = 3)
abline(lm(y ~ x, data = data.frame(x = c(x1, x2), y = c(y1, y2))), lwd = 2)
abline(lm(y1 ~ x1), col = 2, lwd = 2)
abline(lm(y2 ~ x2), col = 3, lwd = 2)
legend("top", legend = c("No grouping", "Group 1", "Group 2"), col = 1:3, lty = 1, lwd = 2)
```

A more serious consequence of a lurking variable has shown up before in the Palmer penguins data. In that example, the lurking variable actually **reversed** the correlation - if we lumped the groups together we got a negative correlation (and therefore negative slope), but if we looked at the groups individually we got positive associations in all of the groups! This is called **Simpson's Paradox**, and basically means that we have to be very careful about interpreting correlations!


```{r}
#| echo: false
library(palmerpenguins)
library(ggplot2)
theme_set(theme_bw())
library(patchwork)

gall <- ggplot(penguins) + 
    aes(x = bill_depth_mm, y = body_mass_g) +
    geom_point(mapping = aes(colour = species)) +
    scale_colour_manual(values = c("darkorchid", "forestgreen", "orange3")) +
    geom_smooth(method = "lm", formula = y~x, se = FALSE) +
    labs(x = "Bill Depth (mm)",
        y = "Body Mass (g)",
        title = paste0("Correlation (not using Species): ",
            round(cor(penguins$bill_depth_mm, 
                penguins$body_mass_g, use = "complete"),
                4))) +
    theme(legend.position = "none")

gadelie <- with(subset(penguins, species == "Adelie"),
    ggplot() + aes(x = bill_depth_mm, y = body_mass_g) +
        geom_point(colour = "darkorchid") +
        geom_smooth(method = "lm", formula = y~x, se = FALSE) +
        labs(x = "Bill Depth (mm)",
            y = "Body Mass (g)",
            title = paste0("Correlation for Adelie Penguins: ",
                round(cor(bill_depth_mm, body_mass_g, 
                    use = "complete"), 
                4)))
)
ginstrap <- with(subset(penguins, species == "Chinstrap"),
    ggplot() + aes(x = bill_depth_mm, y = body_mass_g) +
        geom_point(colour = "forestgreen") +
        geom_smooth(method = "lm", formula = y~x, se = FALSE) +
        labs(x = "Bill Depth (mm)",
            y = "Body Mass (g)",
            title = paste0("Correlation for Chinstrap Penguins: ",
                round(cor(bill_depth_mm, body_mass_g, 
                    use = "complete"), 
                4)))
)
gentoo <- with(subset(penguins, species == "Gentoo"),
    ggplot() + aes(x = bill_depth_mm, y = body_mass_g) +
        geom_point(colour = "orange3") +
        geom_smooth(method = "lm", formula = y~x, se = FALSE) +
        labs(x = "Bill Depth (mm)",
            y = "Body Mass (g)",
            title = paste0("Correlation for Gentoo Penguins: ",
                round(cor(bill_depth_mm, body_mass_g, 
                    use = "complete"), 
                4)))
)

gall + (gadelie / ginstrap / gentoo)
```

# Exercises

1 Suppose $r = 0.3$. What are the possible values for the slope?
    a. $b > 0$
    b. $b \ge 0$
    c. The slope could still be any value.

<details>
    <summary>**Solution** (really think it through before revealing answer!)</summary>
The slope and the correlation coefficient have the same sign (if the correlation is positive, then the slope must be positive). From the formula, $b=0$ only if $r = 0$ (and vice versa). Therefore the answer is a.
</details>

2. Suppose the coefficient of variation is 0.3. What are the possible values of the slope?
    a. $b > 0$
    b. $b \ge 0$
    c. The slope could still be any value.
    d. The slope could still be any value (except 0).

<details>
    <summary>**Solution** (really think it through before revealing answer)</summary>
Since $R^2$ is a squared value, it's positive whether the slope is positive or negative. However, the slope is still 0 whenever $r$ is 0, so the answer is d.
</details>

3. Which of the following might be modelled well by a linear relationship?
    a. Career points in a particular sport versus number of injuries.
    b. Hours of work required for a project and labour costs.
    c. The height of a ball over time after being thrown in the air.
    d. Hours spent studying and final grade.

<details>
    <summary>**Solution**</summary>
a, b, and d could all be argued to be linear, assuming that the data were collected well and as categorical variables. c is clearly not linear - a ball goes up and then goes down, which cannot be modelled by a line. 
</details>

## Crowdsourced Questions

The following questions are added from the Winter 2024 section of ST231 at Wilfrid Laurier University. The students submitted questions for bonus marks, and I have included them here (with permission, and possibly minor modifications).

1. Suppose you have collected data on the number of hours studied and the corresponding exam scores for 10 students. using simple linear regression, determine the relationship between the number of hours studied and exam scores.

| hours studied (x) | exam Score (y) |
|-------------------|----------------|
| 3 | 60 |
| 4 | 65 |
| 6 | 75 |
| 5 | 70 |
| 7 | 85 |
| 8 | 80 |
| 9 | 90 |
| 10 | 95 |
| 12 | 100 |
| 11 | 98 |

a. What are the means of x and y?
b. What are the standard deviations of x and y?
c. What is the correlation between x and y?
d. Use your answers above to calculate a regression line. Validate with R.

```{r}
hours_studied <- c(3, 4, 6, 5, 7, 8, 9, 10, 12, 11)
exam_score <- c(60, 65, 75, 70, 85, 80, 90, 95, 100, 98)
```

<details>
<summary>**Solution**</summary>

Try the calculations by hand!

```{r}
# a)
mean(hours_studied)
mean(exam_score)

# b)
sd(hours_studied)
sd(exam_score)

# c)
cor(hours_studied, exam_score)

# d)
# Regression slope b = r * s_y / s_x
b <- cor(hours_studied, exam_score) * sd(exam_score) / sd(hours_studied)
b
# Regression intercept a = ybar - xbar * b
a <- mean(exam_score) - mean(hours_studied) * b
a

# Check with R
lm(exam_score ~ hours_studied)
```

*****
</details>
